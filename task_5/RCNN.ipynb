{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from math import sqrt\n",
    "\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置参数\n",
    "\n",
    "class TrainingConfig(object):\n",
    "    epoches = 10\n",
    "    evaluateEvery = 100\n",
    "    checkpointEvery = 100\n",
    "    learningRate = 0.001\n",
    "    \n",
    "class ModelConfig(object):\n",
    "    embeddingSize = 200\n",
    "    \n",
    "    hiddenSizes = [128]  # LSTM结构的神经元个数\n",
    "    \n",
    "    dropoutKeepProb = 0.5\n",
    "    l2RegLambda = 0.0\n",
    "    \n",
    "    outputSize = 128  # 从高维映射到低维的神经元个数\n",
    "    \n",
    "class Config(object):\n",
    "    sequenceLength = 200  # 取了所有序列长度的均值\n",
    "    batchSize = 128\n",
    "    \n",
    "    dataSource = \"data/labeledTrain.csv\"\n",
    "    \n",
    "    stopWordSource = \"data/english\"\n",
    "    \n",
    "    numClasses = 1  # 二分类设置为1，多分类设置为类别的数目\n",
    "    \n",
    "    rate = 0.8  # 训练集的比例\n",
    "    \n",
    "    training = TrainingConfig()\n",
    "    \n",
    "    model = ModelConfig()\n",
    "\n",
    "    \n",
    "# 实例化配置参数对象\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理的类，生成训练集和测试集\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self._dataSource = config.dataSource\n",
    "        self._stopWordSource = config.stopWordSource  \n",
    "        \n",
    "        self._sequenceLength = config.sequenceLength  # 每条输入的序列处理为定长\n",
    "        self._embeddingSize = config.model.embeddingSize\n",
    "        self._batchSize = config.batchSize\n",
    "        self._rate = config.rate\n",
    "        \n",
    "        self._stopWordDict = {}\n",
    "        \n",
    "        self.trainReviews = []\n",
    "        self.trainLabels = []\n",
    "        \n",
    "        self.evalReviews = []\n",
    "        self.evalLabels = []\n",
    "        \n",
    "        self.wordEmbedding =None\n",
    "        \n",
    "        self.labelList = []\n",
    "        \n",
    "    def _readData(self, filePath):\n",
    "        \"\"\"\n",
    "        从csv文件中读取数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_csv(filePath)\n",
    "        \n",
    "        if self.config.numClasses == 1:\n",
    "            labels = df[\"sentiment\"].tolist()\n",
    "        elif self.config.numClasses > 1:\n",
    "            labels = df[\"rate\"].tolist()\n",
    "            \n",
    "        review = df[\"review\"].tolist()\n",
    "        reviews = [line.strip().split() for line in review]\n",
    "\n",
    "        return reviews, labels\n",
    "    \n",
    "    def _labelToIndex(self, labels, label2idx):\n",
    "        \"\"\"\n",
    "        将标签转换成索引表示\n",
    "        \"\"\"\n",
    "        labelIds = [label2idx[label] for label in labels]\n",
    "        return labelIds\n",
    "    \n",
    "    def _wordToIndex(self, reviews, word2idx):\n",
    "        \"\"\"\n",
    "        将词转换成索引\n",
    "        \"\"\"\n",
    "        reviewIds = [[word2idx.get(item, word2idx[\"UNK\"]) for item in review] for review in reviews]\n",
    "        return reviewIds\n",
    "        \n",
    "    def _genTrainEvalData(self, x, y, word2idx, rate):\n",
    "        \"\"\"\n",
    "        生成训练集和验证集\n",
    "        \"\"\"\n",
    "        reviews = []\n",
    "        for review in x:\n",
    "            if len(review) >= self._sequenceLength:\n",
    "                reviews.append(review[:self._sequenceLength])\n",
    "            else:\n",
    "                reviews.append(review + [word2idx[\"PAD\"]] * (self._sequenceLength - len(review)))\n",
    "            \n",
    "        trainIndex = int(len(x) * rate)\n",
    "        \n",
    "        trainReviews = np.asarray(reviews[:trainIndex], dtype=\"int64\")\n",
    "        trainLabels = np.array(y[:trainIndex], dtype=\"float32\")\n",
    "        \n",
    "        evalReviews = np.asarray(reviews[trainIndex:], dtype=\"int64\")\n",
    "        evalLabels = np.array(y[trainIndex:], dtype=\"float32\")\n",
    "\n",
    "        return trainReviews, trainLabels, evalReviews, evalLabels\n",
    "        \n",
    "    def _genVocabulary(self, reviews, labels):\n",
    "        \"\"\"\n",
    "        生成词向量和词汇-索引映射字典，可以用全数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        allWords = [word for review in reviews for word in review]\n",
    "        \n",
    "        # 去掉停用词\n",
    "        subWords = [word for word in allWords if word not in self.stopWordDict]\n",
    "        \n",
    "        wordCount = Counter(subWords)  # 统计词频\n",
    "        sortWordCount = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 去除低频词\n",
    "        words = [item[0] for item in sortWordCount if item[1] >= 5]\n",
    "        \n",
    "        vocab, wordEmbedding = self._getWordEmbedding(words)\n",
    "        self.wordEmbedding = wordEmbedding\n",
    "        \n",
    "        word2idx = dict(zip(vocab, list(range(len(vocab)))))\n",
    "        \n",
    "        uniqueLabel = list(set(labels))\n",
    "        label2idx = dict(zip(uniqueLabel, list(range(len(uniqueLabel)))))\n",
    "        self.labelList = list(range(len(uniqueLabel)))\n",
    "        \n",
    "        # 将词汇-索引映射表保存为json数据，之后做inference时直接加载来处理数据\n",
    "        with open(\"../data/wordJson/word2idx.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(word2idx, f)\n",
    "        \n",
    "        with open(\"../data/wordJson/label2idx.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(label2idx, f)\n",
    "        \n",
    "        return word2idx, label2idx\n",
    "            \n",
    "    def _getWordEmbedding(self, words):\n",
    "        \"\"\"\n",
    "        按照我们的数据集中的单词取出预训练好的word2vec中的词向量\n",
    "        \"\"\"\n",
    "        \n",
    "        wordVec = gensim.models.KeyedVectors.load_word2vec_format(\"data/word2Vec.bin\", binary=True)\n",
    "        vocab = []\n",
    "        wordEmbedding = []\n",
    "        \n",
    "        # 添加 \"pad\" 和 \"UNK\", \n",
    "        vocab.append(\"PAD\")\n",
    "        vocab.append(\"UNK\")\n",
    "        wordEmbedding.append(np.zeros(self._embeddingSize))\n",
    "        wordEmbedding.append(np.random.randn(self._embeddingSize))\n",
    "        \n",
    "        for word in words:\n",
    "            try:\n",
    "                vector = wordVec.wv[word]\n",
    "                vocab.append(word)\n",
    "                wordEmbedding.append(vector)\n",
    "            except:\n",
    "                print(word + \"不存在于词向量中\")\n",
    "                \n",
    "        return vocab, np.array(wordEmbedding)\n",
    "    \n",
    "    def _readStopWord(self, stopWordPath):\n",
    "        \"\"\"\n",
    "        读取停用词\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(stopWordPath, \"r\") as f:\n",
    "            stopWords = f.read()\n",
    "            stopWordList = stopWords.splitlines()\n",
    "            # 将停用词用列表的形式生成，之后查找停用词时会比较快\n",
    "            self.stopWordDict = dict(zip(stopWordList, list(range(len(stopWordList)))))\n",
    "            \n",
    "    def dataGen(self):\n",
    "        \"\"\"\n",
    "        初始化训练集和验证集\n",
    "        \"\"\"\n",
    "        \n",
    "        # 初始化停用词\n",
    "        self._readStopWord(self._stopWordSource)\n",
    "        \n",
    "        # 初始化数据集\n",
    "        reviews, labels = self._readData(self._dataSource)\n",
    "        \n",
    "        # 初始化词汇-索引映射表和词向量矩阵\n",
    "        word2idx, label2idx = self._genVocabulary(reviews, labels)\n",
    "        \n",
    "        # 将标签和句子数值化\n",
    "        labelIds = self._labelToIndex(labels, label2idx)\n",
    "        reviewIds = self._wordToIndex(reviews, word2idx)\n",
    "        \n",
    "        # 初始化训练集和测试集\n",
    "        trainReviews, trainLabels, evalReviews, evalLabels = self._genTrainEvalData(reviewIds, labelIds, word2idx, self._rate)\n",
    "        self.trainReviews = trainReviews\n",
    "        self.trainLabels = trainLabels\n",
    "        \n",
    "        self.evalReviews = evalReviews\n",
    "        self.evalLabels = evalLabels\n",
    "        \n",
    "        \n",
    "data = Dataset(config)\n",
    "data.dataGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (20000, 200)\n",
      "train label shape: (20000,)\n",
      "eval data shape: (5000, 200)\n"
     ]
    }
   ],
   "source": [
    "print(\"train data shape: {}\".format(data.trainReviews.shape))\n",
    "print(\"train label shape: {}\".format(data.trainLabels.shape))\n",
    "print(\"eval data shape: {}\".format(data.evalReviews.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出batch数据集\n",
    "\n",
    "def nextBatch(x, y, batchSize):\n",
    "        \"\"\"\n",
    "        生成batch数据集，用生成器的方式输出\n",
    "        \"\"\"\n",
    "    \n",
    "        perm = np.arange(len(x))\n",
    "        np.random.shuffle(perm)\n",
    "        x = x[perm]\n",
    "        y = y[perm]\n",
    "        \n",
    "        numBatches = len(x) // batchSize\n",
    "\n",
    "        for i in range(numBatches):\n",
    "            start = i * batchSize\n",
    "            end = start + batchSize\n",
    "            batchX = np.array(x[start: end], dtype=\"int64\")\n",
    "            batchY = np.array(y[start: end], dtype=\"float32\")\n",
    "            \n",
    "            yield batchX, batchY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "构建模型，模型的架构如下：\n",
    "1，利用Bi-LSTM获得上下文的信息\n",
    "2，将Bi-LSTM获得的隐层输出和词向量拼接[fwOutput;wordEmbedding;bwOutput]\n",
    "3，将2所得的词表示映射到低维\n",
    "4，hidden_size上每个位置的值都取时间步上最大的值，类似于max-pool\n",
    "5，softmax分类\n",
    "\"\"\"\n",
    "\n",
    "class RCNN(object):\n",
    "    \"\"\"\n",
    "    RCNN 用于文本分类\n",
    "    \"\"\"\n",
    "    def __init__(self, config, wordEmbedding):\n",
    "\n",
    "        # 定义模型的输入\n",
    "        self.inputX = tf.placeholder(tf.int32, [None, config.sequenceLength], name=\"inputX\")\n",
    "        self.inputY = tf.placeholder(tf.int32, [None], name=\"inputY\")\n",
    "        \n",
    "        self.dropoutKeepProb = tf.placeholder(tf.float32, name=\"dropoutKeepProb\")\n",
    "        \n",
    "        # 定义l2损失\n",
    "        l2Loss = tf.constant(0.0)\n",
    "        \n",
    "        # 词嵌入层\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "\n",
    "            # 利用预训练的词向量初始化词嵌入矩阵\n",
    "            self.W = tf.Variable(tf.cast(wordEmbedding, dtype=tf.float32, name=\"word2vec\") ,name=\"W\")\n",
    "            # 利用词嵌入矩阵将输入的数据中的词转换成词向量，维度[batch_size, sequence_length, embedding_size]\n",
    "            self.embeddedWords = tf.nn.embedding_lookup(self.W, self.inputX)\n",
    "            # 复制一份embedding input\n",
    "            self.embeddedWords_ = self.embeddedWords\n",
    "            \n",
    "        # 定义两层双向LSTM的模型结构\n",
    "\n",
    "#         with tf.name_scope(\"Bi-LSTM\"):\n",
    "#             fwHiddenLayers = []\n",
    "#             bwHiddenLayers = []\n",
    "#             for idx, hiddenSize in enumerate(config.model.hiddenSizes):\n",
    "\n",
    "#                 with tf.name_scope(\"Bi-LSTM-\" + str(idx)):\n",
    "#                     # 定义前向LSTM结构\n",
    "#                     lstmFwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n",
    "#                                                                  output_keep_prob=self.dropoutKeepProb)\n",
    "#                     # 定义反向LSTM结构\n",
    "#                     lstmBwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n",
    "#                                                                  output_keep_prob=self.dropoutKeepProb)\n",
    "\n",
    "#                 fwHiddenLayers.append(lstmFwCell)\n",
    "#                 bwHiddenLayers.append(lstmBwCell)\n",
    "\n",
    "#             # 实现多层的LSTM结构， state_is_tuple=True，则状态会以元祖的形式组合(h, c)，否则列向拼接\n",
    "#             fwMultiLstm = tf.nn.rnn_cell.MultiRNNCell(cells=fwHiddenLayers, state_is_tuple=True)\n",
    "#             bwMultiLstm = tf.nn.rnn_cell.MultiRNNCell(cells=bwHiddenLayers, state_is_tuple=True)\n",
    "\n",
    "#             # 采用动态rnn，可以动态的输入序列的长度，若没有输入，则取序列的全长\n",
    "#             # outputs是一个元祖(output_fw, output_bw)，其中两个元素的维度都是[batch_size, max_time, hidden_size],fw和bw的hidden_size一样\n",
    "#             # self.current_state 是最终的状态，二元组(state_fw, state_bw)，state_fw=[batch_size, s]，s是一个元祖(h, c)\n",
    "#             outputs, self.current_state = tf.nn.bidirectional_dynamic_rnn(fwMultiLstm, bwMultiLstm, self.embeddedWords, dtype=tf.float32)\n",
    "#             fwOutput, bwOutput = outputs\n",
    "\n",
    "        with tf.name_scope(\"Bi-LSTM\"):\n",
    "            for idx, hiddenSize in enumerate(config.model.hiddenSizes):\n",
    "                with tf.name_scope(\"Bi-LSTM\" + str(idx)):\n",
    "                    # 定义前向LSTM结构\n",
    "                    lstmFwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n",
    "                                                                 output_keep_prob=self.dropoutKeepProb)\n",
    "                    # 定义反向LSTM结构\n",
    "                    lstmBwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n",
    "                                                                 output_keep_prob=self.dropoutKeepProb)\n",
    "\n",
    "\n",
    "                    # 采用动态rnn，可以动态的输入序列的长度，若没有输入，则取序列的全长\n",
    "                    # outputs是一个元祖(output_fw, output_bw)，其中两个元素的维度都是[batch_size, max_time, hidden_size],fw和bw的hidden_size一样\n",
    "                    # self.current_state 是最终的状态，二元组(state_fw, state_bw)，state_fw=[batch_size, s]，s是一个元祖(h, c)\n",
    "                    outputs_, self.current_state = tf.nn.bidirectional_dynamic_rnn(lstmFwCell, lstmBwCell, \n",
    "                                                                                  self.embeddedWords_, dtype=tf.float32,\n",
    "                                                                                  scope=\"bi-lstm\" + str(idx))\n",
    "        \n",
    "                    # 对outputs中的fw和bw的结果拼接 [batch_size, time_step, hidden_size * 2], 传入到下一层Bi-LSTM中\n",
    "                    self.embeddedWords_ = tf.concat(outputs_, 2)\n",
    "                \n",
    "        # 将最后一层Bi-LSTM输出的结果分割成前向和后向的输出\n",
    "        fwOutput, bwOutput = tf.split(self.embeddedWords_, 2, -1)\n",
    "            \n",
    "        with tf.name_scope(\"context\"):\n",
    "            shape = [tf.shape(fwOutput)[0], 1, tf.shape(fwOutput)[2]]\n",
    "            self.contextLeft = tf.concat([tf.zeros(shape), fwOutput[:, :-1]], axis=1, name=\"contextLeft\")\n",
    "            self.contextRight = tf.concat([bwOutput[:, 1:], tf.zeros(shape)], axis=1, name=\"contextRight\")\n",
    "            \n",
    "        # 将前向，后向的输出和最早的词向量拼接在一起得到最终的词表征\n",
    "        with tf.name_scope(\"wordRepresentation\"):\n",
    "            self.wordRepre = tf.concat([self.contextLeft, self.embeddedWords, self.contextRight], axis=2)\n",
    "            wordSize = config.model.hiddenSizes[-1] * 2 + config.model.embeddingSize \n",
    "        \n",
    "        with tf.name_scope(\"textRepresentation\"):\n",
    "            outputSize = config.model.outputSize\n",
    "            textW = tf.Variable(tf.random_uniform([wordSize, outputSize], -1.0, 1.0), name=\"W2\")\n",
    "            textB = tf.Variable(tf.constant(0.1, shape=[outputSize]), name=\"b2\")\n",
    "            \n",
    "            # tf.einsum可以指定维度的消除运算\n",
    "            self.textRepre = tf.tanh(tf.einsum('aij,jk->aik', self.wordRepre, textW) + textB)\n",
    "            \n",
    "        # 做max-pool的操作，将时间步的维度消失\n",
    "        output = tf.reduce_max(self.textRepre, axis=1)\n",
    "        \n",
    "        # 全连接层的输出\n",
    "        with tf.name_scope(\"output\"):\n",
    "            outputW = tf.get_variable(\n",
    "                \"outputW\",\n",
    "                shape=[outputSize, config.numClasses],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            outputB= tf.Variable(tf.constant(0.1, shape=[config.numClasses]), name=\"outputB\")\n",
    "            l2Loss += tf.nn.l2_loss(outputW)\n",
    "            l2Loss += tf.nn.l2_loss(outputB)\n",
    "            self.logits = tf.nn.xw_plus_b(output, outputW, outputB, name=\"logits\")\n",
    "            \n",
    "            if config.numClasses == 1:\n",
    "                self.predictions = tf.cast(tf.greater_equal(self.logits, 0.0), tf.float32, name=\"predictions\")\n",
    "            elif config.numClasses > 1:\n",
    "                self.predictions = tf.argmax(self.logits, axis=-1, name=\"predictions\")\n",
    "        \n",
    "        # 计算二元交叉熵损失\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            \n",
    "            if config.numClasses == 1:\n",
    "                losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.logits, labels=tf.cast(tf.reshape(self.inputY, [-1, 1]), \n",
    "                                                                                                    dtype=tf.float32))\n",
    "            elif config.numClasses > 1:\n",
    "                losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.inputY)\n",
    "                \n",
    "            self.loss = tf.reduce_mean(losses) + config.model.l2RegLambda * l2Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "定义各类性能指标\n",
    "\"\"\"\n",
    "\n",
    "def mean(item: list) -> float:\n",
    "    \"\"\"\n",
    "    计算列表中元素的平均值\n",
    "    :param item: 列表对象\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    res = sum(item) / len(item) if len(item) > 0 else 0\n",
    "    return res\n",
    "\n",
    "\n",
    "def accuracy(pred_y, true_y):\n",
    "    \"\"\"\n",
    "    计算二类和多类的准确率\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if isinstance(pred_y[0], list):\n",
    "        pred_y = [item[0] for item in pred_y]\n",
    "    corr = 0\n",
    "    for i in range(len(pred_y)):\n",
    "        if pred_y[i] == true_y[i]:\n",
    "            corr += 1\n",
    "    acc = corr / len(pred_y) if len(pred_y) > 0 else 0\n",
    "    return acc\n",
    "\n",
    "\n",
    "def binary_precision(pred_y, true_y, positive=1):\n",
    "    \"\"\"\n",
    "    二类的精确率计算\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param positive: 正例的索引表示\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    corr = 0\n",
    "    pred_corr = 0\n",
    "    for i in range(len(pred_y)):\n",
    "        if pred_y[i] == positive:\n",
    "            pred_corr += 1\n",
    "            if pred_y[i] == true_y[i]:\n",
    "                corr += 1\n",
    "\n",
    "    prec = corr / pred_corr if pred_corr > 0 else 0\n",
    "    return prec\n",
    "\n",
    "\n",
    "def binary_recall(pred_y, true_y, positive=1):\n",
    "    \"\"\"\n",
    "    二类的召回率\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param positive: 正例的索引表示\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    corr = 0\n",
    "    true_corr = 0\n",
    "    for i in range(len(pred_y)):\n",
    "        if true_y[i] == positive:\n",
    "            true_corr += 1\n",
    "            if pred_y[i] == true_y[i]:\n",
    "                corr += 1\n",
    "\n",
    "    rec = corr / true_corr if true_corr > 0 else 0\n",
    "    return rec\n",
    "\n",
    "\n",
    "def binary_f_beta(pred_y, true_y, beta=1.0, positive=1):\n",
    "    \"\"\"\n",
    "    二类的f beta值\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param beta: beta值\n",
    "    :param positive: 正例的索引表示\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    precision = binary_precision(pred_y, true_y, positive)\n",
    "    recall = binary_recall(pred_y, true_y, positive)\n",
    "    try:\n",
    "        f_b = (1 + beta * beta) * precision * recall / (beta * beta * precision + recall)\n",
    "    except:\n",
    "        f_b = 0\n",
    "    return f_b\n",
    "\n",
    "\n",
    "def multi_precision(pred_y, true_y, labels):\n",
    "    \"\"\"\n",
    "    多类的精确率\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param labels: 标签列表\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if isinstance(pred_y[0], list):\n",
    "        pred_y = [item[0] for item in pred_y]\n",
    "\n",
    "    precisions = [binary_precision(pred_y, true_y, label) for label in labels]\n",
    "    prec = mean(precisions)\n",
    "    return prec\n",
    "\n",
    "\n",
    "def multi_recall(pred_y, true_y, labels):\n",
    "    \"\"\"\n",
    "    多类的召回率\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param labels: 标签列表\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if isinstance(pred_y[0], list):\n",
    "        pred_y = [item[0] for item in pred_y]\n",
    "\n",
    "    recalls = [binary_recall(pred_y, true_y, label) for label in labels]\n",
    "    rec = mean(recalls)\n",
    "    return rec\n",
    "\n",
    "\n",
    "def multi_f_beta(pred_y, true_y, labels, beta=1.0):\n",
    "    \"\"\"\n",
    "    多类的f beta值\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param labels: 标签列表\n",
    "    :param beta: beta值\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if isinstance(pred_y[0], list):\n",
    "        pred_y = [item[0] for item in pred_y]\n",
    "\n",
    "    f_betas = [binary_f_beta(pred_y, true_y, beta, label) for label in labels]\n",
    "    f_beta = mean(f_betas)\n",
    "    return f_beta\n",
    "\n",
    "\n",
    "def get_binary_metrics(pred_y, true_y, f_beta=1.0):\n",
    "    \"\"\"\n",
    "    得到二分类的性能指标\n",
    "    :param pred_y:\n",
    "    :param true_y:\n",
    "    :param f_beta:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    acc = accuracy(pred_y, true_y)\n",
    "    recall = binary_recall(pred_y, true_y)\n",
    "    precision = binary_precision(pred_y, true_y)\n",
    "    f_beta = binary_f_beta(pred_y, true_y, f_beta)\n",
    "    return acc, recall, precision, f_beta\n",
    "\n",
    "\n",
    "def get_multi_metrics(pred_y, true_y, labels, f_beta=1.0):\n",
    "    \"\"\"\n",
    "    得到多分类的性能指标\n",
    "    :param pred_y:\n",
    "    :param true_y:\n",
    "    :param labels:\n",
    "    :param f_beta:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    acc = accuracy(pred_y, true_y)\n",
    "    recall = multi_recall(pred_y, true_y, labels)\n",
    "    precision = multi_precision(pred_y, true_y, labels)\n",
    "    f_beta = multi_f_beta(pred_y, true_y, labels, f_beta)\n",
    "    return acc, recall, precision, f_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/fw/lstm_cell/kernel:0/grad/hist is illegal; using bi-lstm0/fw/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/fw/lstm_cell/kernel:0/grad/sparsity is illegal; using bi-lstm0/fw/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/fw/lstm_cell/bias:0/grad/hist is illegal; using bi-lstm0/fw/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/fw/lstm_cell/bias:0/grad/sparsity is illegal; using bi-lstm0/fw/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/bw/lstm_cell/kernel:0/grad/hist is illegal; using bi-lstm0/bw/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/bw/lstm_cell/kernel:0/grad/sparsity is illegal; using bi-lstm0/bw/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/bw/lstm_cell/bias:0/grad/hist is illegal; using bi-lstm0/bw/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/bw/lstm_cell/bias:0/grad/sparsity is illegal; using bi-lstm0/bw/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name textRepresentation/W2:0/grad/hist is illegal; using textRepresentation/W2_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name textRepresentation/W2:0/grad/sparsity is illegal; using textRepresentation/W2_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name textRepresentation/b2:0/grad/hist is illegal; using textRepresentation/b2_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name textRepresentation/b2:0/grad/sparsity is illegal; using textRepresentation/b2_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name outputW:0/grad/hist is illegal; using outputW_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name outputW:0/grad/sparsity is illegal; using outputW_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/outputB:0/grad/hist is illegal; using output/outputB_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/outputB:0/grad/sparsity is illegal; using output/outputB_0/grad/sparsity instead.\n",
      "Writing to E:\\githubProject\\textClassifier\\RCNN\\summarys\n",
      "\n",
      "start training model\n",
      "train: step: 1, loss: 0.7346760034561157, acc: 0.5625, recall: 1.0, precision: 0.5625, f_beta: 0.72\n",
      "train: step: 2, loss: 0.679059624671936, acc: 0.5859375, recall: 1.0, precision: 0.5859375, f_beta: 0.7389162561576355\n",
      "train: step: 3, loss: 0.6897246837615967, acc: 0.546875, recall: 1.0, precision: 0.546875, f_beta: 0.7070707070707071\n",
      "train: step: 4, loss: 0.6907001733779907, acc: 0.5703125, recall: 0.8360655737704918, precision: 0.53125, f_beta: 0.6496815286624203\n",
      "train: step: 5, loss: 0.6908186674118042, acc: 0.5390625, recall: 0.6557377049180327, precision: 0.5128205128205128, f_beta: 0.5755395683453238\n",
      "train: step: 6, loss: 0.691948652267456, acc: 0.5234375, recall: 0.515625, precision: 0.5238095238095238, f_beta: 0.5196850393700788\n",
      "train: step: 7, loss: 0.7026311755180359, acc: 0.453125, recall: 0.015151515151515152, precision: 0.16666666666666666, f_beta: 0.027777777777777776\n",
      "train: step: 8, loss: 0.7161846160888672, acc: 0.40625, recall: 0.025974025974025976, precision: 0.6666666666666666, f_beta: 0.05\n",
      "train: step: 9, loss: 0.682144284248352, acc: 0.609375, recall: 0.1509433962264151, precision: 0.6153846153846154, f_beta: 0.24242424242424243\n",
      "train: step: 10, loss: 0.6948176622390747, acc: 0.5078125, recall: 0.06779661016949153, precision: 0.3333333333333333, f_beta: 0.11267605633802817\n",
      "train: step: 11, loss: 0.6862763166427612, acc: 0.5546875, recall: 0.125, precision: 0.4666666666666667, f_beta: 0.19718309859154928\n",
      "train: step: 12, loss: 0.6975080966949463, acc: 0.4609375, recall: 0.0, precision: 0.0, f_beta: 0\n",
      "train: step: 13, loss: 0.690747857093811, acc: 0.515625, recall: 0.046153846153846156, precision: 1.0, f_beta: 0.08823529411764706\n",
      "train: step: 14, loss: 0.6915587186813354, acc: 0.5234375, recall: 0.01639344262295082, precision: 0.5, f_beta: 0.031746031746031744\n",
      "train: step: 15, loss: 0.6864244937896729, acc: 0.5390625, recall: 0.04838709677419355, precision: 1.0, f_beta: 0.09230769230769231\n",
      "train: step: 16, loss: 0.6996261477470398, acc: 0.4375, recall: 0.19736842105263158, precision: 0.5769230769230769, f_beta: 0.2941176470588235\n",
      "train: step: 17, loss: 0.6923176050186157, acc: 0.53125, recall: 0.5161290322580645, precision: 0.5161290322580645, f_beta: 0.5161290322580645\n",
      "train: step: 18, loss: 0.6940932273864746, acc: 0.4921875, recall: 0.48214285714285715, precision: 0.42857142857142855, f_beta: 0.45378151260504196\n",
      "train: step: 19, loss: 0.6909087300300598, acc: 0.5078125, recall: 0.5362318840579711, precision: 0.5441176470588235, f_beta: 0.5401459854014599\n",
      "train: step: 20, loss: 0.6932612657546997, acc: 0.5390625, recall: 0.6065573770491803, precision: 0.5138888888888888, f_beta: 0.556390977443609\n",
      "train: step: 21, loss: 0.6905772686004639, acc: 0.5859375, recall: 0.6111111111111112, precision: 0.5076923076923077, f_beta: 0.5546218487394958\n",
      "train: step: 22, loss: 0.6912973523139954, acc: 0.546875, recall: 0.6666666666666666, precision: 0.5128205128205128, f_beta: 0.5797101449275363\n",
      "train: step: 23, loss: 0.6915012001991272, acc: 0.53125, recall: 0.5, precision: 0.6166666666666667, f_beta: 0.5522388059701493\n",
      "train: step: 24, loss: 0.6926321983337402, acc: 0.515625, recall: 0.5857142857142857, precision: 0.5540540540540541, f_beta: 0.5694444444444445\n",
      "train: step: 25, loss: 0.6959488391876221, acc: 0.4921875, recall: 0.5178571428571429, precision: 0.43283582089552236, f_beta: 0.47154471544715443\n",
      "train: step: 26, loss: 0.6955162286758423, acc: 0.4765625, recall: 0.5454545454545454, precision: 0.4931506849315068, f_beta: 0.5179856115107914\n",
      "train: step: 27, loss: 0.6960852742195129, acc: 0.4765625, recall: 0.46153846153846156, precision: 0.4838709677419355, f_beta: 0.4724409448818898\n",
      "train: step: 28, loss: 0.6946159601211548, acc: 0.484375, recall: 0.46153846153846156, precision: 0.4918032786885246, f_beta: 0.4761904761904762\n",
      "train: step: 29, loss: 0.6993265151977539, acc: 0.453125, recall: 0.4166666666666667, precision: 0.5172413793103449, f_beta: 0.46153846153846156\n",
      "train: step: 30, loss: 0.6922563910484314, acc: 0.5078125, recall: 0.4714285714285714, precision: 0.559322033898305, f_beta: 0.5116279069767442\n",
      "train: step: 31, loss: 0.6876670718193054, acc: 0.546875, recall: 0.5074626865671642, precision: 0.576271186440678, f_beta: 0.5396825396825398\n",
      "train: step: 32, loss: 0.6946085095405579, acc: 0.515625, recall: 0.5789473684210527, precision: 0.4647887323943662, f_beta: 0.515625\n",
      "train: step: 33, loss: 0.6939333081245422, acc: 0.4765625, recall: 0.47619047619047616, precision: 0.46875, f_beta: 0.47244094488188976\n",
      "train: step: 34, loss: 0.6924515962600708, acc: 0.5, recall: 0.5588235294117647, precision: 0.5277777777777778, f_beta: 0.5428571428571428\n",
      "train: step: 35, loss: 0.6960850954055786, acc: 0.484375, recall: 0.5079365079365079, precision: 0.47761194029850745, f_beta: 0.4923076923076923\n",
      "train: step: 36, loss: 0.6909953355789185, acc: 0.5390625, recall: 0.6031746031746031, precision: 0.5277777777777778, f_beta: 0.562962962962963\n",
      "train: step: 37, loss: 0.6938278079032898, acc: 0.5, recall: 0.45901639344262296, precision: 0.4745762711864407, f_beta: 0.4666666666666667\n",
      "train: step: 38, loss: 0.6913243532180786, acc: 0.53125, recall: 0.6060606060606061, precision: 0.5405405405405406, f_beta: 0.5714285714285714\n",
      "train: step: 39, loss: 0.6906590461730957, acc: 0.5, recall: 0.5, precision: 0.4375, f_beta: 0.4666666666666667\n",
      "train: step: 40, loss: 0.6894395351409912, acc: 0.5703125, recall: 0.15517241379310345, precision: 0.6, f_beta: 0.24657534246575347\n",
      "train: step: 41, loss: 0.6929864287376404, acc: 0.5078125, recall: 0.046153846153846156, precision: 0.75, f_beta: 0.08695652173913043\n",
      "train: step: 42, loss: 0.694938600063324, acc: 0.4765625, recall: 0.015151515151515152, precision: 0.3333333333333333, f_beta: 0.028985507246376812\n",
      "train: step: 43, loss: 0.6917272806167603, acc: 0.5234375, recall: 0.03225806451612903, precision: 0.6666666666666666, f_beta: 0.06153846153846154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 44, loss: 0.688346266746521, acc: 0.546875, recall: 0.08064516129032258, precision: 0.8333333333333334, f_beta: 0.14705882352941174\n",
      "train: step: 45, loss: 0.6939014196395874, acc: 0.5, recall: 0.17391304347826086, precision: 0.631578947368421, f_beta: 0.2727272727272727\n",
      "train: step: 46, loss: 0.6967169046401978, acc: 0.484375, recall: 0.43548387096774194, precision: 0.46551724137931033, f_beta: 0.45\n",
      "train: step: 47, loss: 0.7011480927467346, acc: 0.4609375, recall: 0.6607142857142857, precision: 0.42528735632183906, f_beta: 0.5174825174825175\n",
      "train: step: 48, loss: 0.6913569569587708, acc: 0.5, recall: 0.463768115942029, precision: 0.5423728813559322, f_beta: 0.5\n",
      "train: step: 49, loss: 0.6868341565132141, acc: 0.578125, recall: 0.28846153846153844, precision: 0.46875, f_beta: 0.35714285714285715\n",
      "train: step: 50, loss: 0.6869939565658569, acc: 0.5390625, recall: 0.06557377049180328, precision: 0.6666666666666666, f_beta: 0.11940298507462688\n",
      "train: step: 51, loss: 0.6736899614334106, acc: 0.625, recall: 0.0, precision: 0.0, f_beta: 0\n",
      "train: step: 52, loss: 0.6848160028457642, acc: 0.5546875, recall: 0.017543859649122806, precision: 0.5, f_beta: 0.033898305084745756\n",
      "train: step: 53, loss: 0.7068216800689697, acc: 0.4765625, recall: 0.0, precision: 0, f_beta: 0\n",
      "train: step: 54, loss: 0.7006950974464417, acc: 0.5078125, recall: 0.0, precision: 0, f_beta: 0\n",
      "train: step: 55, loss: 0.7011702060699463, acc: 0.4765625, recall: 0.0, precision: 0, f_beta: 0\n",
      "train: step: 56, loss: 0.6804870367050171, acc: 0.578125, recall: 0.0, precision: 0, f_beta: 0\n",
      "train: step: 57, loss: 0.6947338581085205, acc: 0.53125, recall: 0.0, precision: 0, f_beta: 0\n",
      "train: step: 58, loss: 0.6853592395782471, acc: 0.5703125, recall: 0.0, precision: 0, f_beta: 0\n",
      "train: step: 59, loss: 0.699118435382843, acc: 0.453125, recall: 0.014084507042253521, precision: 1.0, f_beta: 0.02777777777777778\n",
      "train: step: 60, loss: 0.6873965859413147, acc: 0.5390625, recall: 0.0, precision: 0, f_beta: 0\n",
      "train: step: 61, loss: 0.6963534355163574, acc: 0.4609375, recall: 0.0, precision: 0.0, f_beta: 0\n",
      "train: step: 62, loss: 0.6905718445777893, acc: 0.5390625, recall: 0.31666666666666665, precision: 0.5135135135135135, f_beta: 0.39175257731958757\n",
      "train: step: 63, loss: 0.6938714385032654, acc: 0.484375, recall: 0.4666666666666667, precision: 0.45161290322580644, f_beta: 0.45901639344262296\n",
      "train: step: 64, loss: 0.693777322769165, acc: 0.4765625, recall: 0.6896551724137931, precision: 0.449438202247191, f_beta: 0.54421768707483\n",
      "train: step: 65, loss: 0.6914819478988647, acc: 0.4921875, recall: 0.5573770491803278, precision: 0.4722222222222222, f_beta: 0.5112781954887219\n",
      "train: step: 66, loss: 0.6941757202148438, acc: 0.4921875, recall: 0.42424242424242425, precision: 0.509090909090909, f_beta: 0.46280991735537186\n",
      "train: step: 67, loss: 0.6947354078292847, acc: 0.4609375, recall: 0.4375, precision: 0.45901639344262296, f_beta: 0.44799999999999995\n",
      "train: step: 68, loss: 0.6948255300521851, acc: 0.4609375, recall: 0.4375, precision: 0.45901639344262296, f_beta: 0.44799999999999995\n",
      "train: step: 69, loss: 0.6952228546142578, acc: 0.453125, recall: 0.352112676056338, precision: 0.5102040816326531, f_beta: 0.41666666666666663\n",
      "train: step: 70, loss: 0.692889928817749, acc: 0.5234375, recall: 0.676923076923077, precision: 0.5238095238095238, f_beta: 0.5906040268456375\n",
      "train: step: 71, loss: 0.6912383437156677, acc: 0.53125, recall: 0.8309859154929577, precision: 0.5514018691588785, f_beta: 0.6629213483146067\n",
      "train: step: 72, loss: 0.6928795576095581, acc: 0.5078125, recall: 0.9137931034482759, precision: 0.4774774774774775, f_beta: 0.6272189349112427\n",
      "train: step: 73, loss: 0.691646933555603, acc: 0.53125, recall: 0.9090909090909091, precision: 0.5263157894736842, f_beta: 0.6666666666666666\n",
      "train: step: 74, loss: 0.6948779821395874, acc: 0.4921875, recall: 0.9833333333333333, precision: 0.4796747967479675, f_beta: 0.644808743169399\n",
      "train: step: 75, loss: 0.6976579427719116, acc: 0.4296875, recall: 0.9622641509433962, precision: 0.4180327868852459, f_beta: 0.5828571428571427\n",
      "train: step: 76, loss: 0.6929301023483276, acc: 0.5, recall: 0.8983050847457628, precision: 0.4774774774774775, f_beta: 0.6235294117647059\n",
      "train: step: 77, loss: 0.693822979927063, acc: 0.546875, recall: 0.48484848484848486, precision: 0.5714285714285714, f_beta: 0.5245901639344263\n",
      "train: step: 78, loss: 0.6945757865905762, acc: 0.453125, recall: 0.014492753623188406, precision: 0.3333333333333333, f_beta: 0.027777777777777776\n",
      "train: step: 79, loss: 0.6917030215263367, acc: 0.5078125, recall: 0.0, precision: 0, f_beta: 0\n",
      "train: step: 80, loss: 0.6976689696311951, acc: 0.453125, recall: 0.014084507042253521, precision: 1.0, f_beta: 0.02777777777777778\n",
      "train: step: 81, loss: 0.6991390585899353, acc: 0.4296875, recall: 0.0, precision: 0.0, f_beta: 0\n",
      "train: step: 82, loss: 0.6899694204330444, acc: 0.5390625, recall: 0.18333333333333332, precision: 0.5238095238095238, f_beta: 0.2716049382716049\n",
      "train: step: 83, loss: 0.6940069198608398, acc: 0.515625, recall: 0.4032258064516129, precision: 0.5, f_beta: 0.4464285714285714\n",
      "train: step: 84, loss: 0.6915156245231628, acc: 0.5703125, recall: 0.6521739130434783, precision: 0.5921052631578947, f_beta: 0.6206896551724137\n",
      "train: step: 85, loss: 0.6909335255622864, acc: 0.5234375, recall: 0.7076923076923077, precision: 0.5227272727272727, f_beta: 0.6013071895424837\n",
      "train: step: 86, loss: 0.6952077746391296, acc: 0.484375, recall: 0.8103448275862069, precision: 0.46078431372549017, f_beta: 0.5875\n",
      "train: step: 87, loss: 0.6891430020332336, acc: 0.546875, recall: 0.8529411764705882, precision: 0.5471698113207547, f_beta: 0.6666666666666666\n",
      "train: step: 88, loss: 0.6883218884468079, acc: 0.5703125, recall: 0.7945205479452054, precision: 0.5918367346938775, f_beta: 0.6783625730994152\n",
      "train: step: 89, loss: 0.6880894303321838, acc: 0.5625, recall: 0.9305555555555556, precision: 0.5677966101694916, f_beta: 0.7052631578947368\n",
      "train: step: 90, loss: 0.6927188038825989, acc: 0.53125, recall: 0.9852941176470589, precision: 0.5317460317460317, f_beta: 0.6907216494845362\n",
      "train: step: 91, loss: 0.7015012502670288, acc: 0.4296875, recall: 1.0, precision: 0.4296875, f_beta: 0.6010928961748634\n",
      "train: step: 92, loss: 0.7004985809326172, acc: 0.4375, recall: 1.0, precision: 0.4375, f_beta: 0.6086956521739131\n",
      "train: step: 93, loss: 0.6964253783226013, acc: 0.453125, recall: 0.9830508474576272, precision: 0.4566929133858268, f_beta: 0.6236559139784946\n",
      "train: step: 94, loss: 0.6905171871185303, acc: 0.515625, recall: 0.8636363636363636, precision: 0.5181818181818182, f_beta: 0.6477272727272728\n",
      "train: step: 95, loss: 0.6971182823181152, acc: 0.421875, recall: 0.2328767123287671, precision: 0.4857142857142857, f_beta: 0.3148148148148148\n",
      "train: step: 96, loss: 0.6915158033370972, acc: 0.5, recall: 0.2191780821917808, precision: 0.6956521739130435, f_beta: 0.3333333333333333\n",
      "train: step: 97, loss: 0.6919435858726501, acc: 0.5859375, recall: 0.24074074074074073, precision: 0.52, f_beta: 0.3291139240506329\n",
      "train: step: 98, loss: 0.6923626661300659, acc: 0.53125, recall: 0.26229508196721313, precision: 0.5161290322580645, f_beta: 0.34782608695652173\n",
      "train: step: 99, loss: 0.6906499266624451, acc: 0.5390625, recall: 0.1774193548387097, precision: 0.5789473684210527, f_beta: 0.2716049382716049\n",
      "train: step: 100, loss: 0.6932904124259949, acc: 0.53125, recall: 0.171875, precision: 0.6111111111111112, f_beta: 0.2682926829268293\n",
      "\n",
      "Evaluation:\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "\n",
    "# 生成训练集和验证集\n",
    "trainReviews = data.trainReviews\n",
    "trainLabels = data.trainLabels\n",
    "evalReviews = data.evalReviews\n",
    "evalLabels = data.evalLabels\n",
    "\n",
    "wordEmbedding = data.wordEmbedding\n",
    "labelList = data.labelList\n",
    "\n",
    "# 定义计算图\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "    session_conf.gpu_options.allow_growth=True\n",
    "    session_conf.gpu_options.per_process_gpu_memory_fraction = 0.9  # 配置gpu占用率  \n",
    "\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    \n",
    "    # 定义会话\n",
    "    with sess.as_default():\n",
    "        lstm = RCNN(config, wordEmbedding)\n",
    "        \n",
    "        globalStep = tf.Variable(0, name=\"globalStep\", trainable=False)\n",
    "        # 定义优化函数，传入学习速率参数\n",
    "        optimizer = tf.train.AdamOptimizer(config.training.learningRate)\n",
    "        # 计算梯度,得到梯度和变量\n",
    "        gradsAndVars = optimizer.compute_gradients(lstm.loss)\n",
    "        # 将梯度应用到变量下，生成训练器\n",
    "        trainOp = optimizer.apply_gradients(gradsAndVars, global_step=globalStep)\n",
    "        \n",
    "        # 用summary绘制tensorBoard\n",
    "        gradSummaries = []\n",
    "        for g, v in gradsAndVars:\n",
    "            if g is not None:\n",
    "                tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "        \n",
    "        outDir = os.path.abspath(os.path.join(os.path.curdir, \"summarys\"))\n",
    "        print(\"Writing to {}\\n\".format(outDir))\n",
    "        \n",
    "        lossSummary = tf.summary.scalar(\"loss\", lstm.loss)\n",
    "        summaryOp = tf.summary.merge_all()\n",
    "        \n",
    "        trainSummaryDir = os.path.join(outDir, \"train\")\n",
    "        trainSummaryWriter = tf.summary.FileWriter(trainSummaryDir, sess.graph)\n",
    "        \n",
    "        evalSummaryDir = os.path.join(outDir, \"eval\")\n",
    "        evalSummaryWriter = tf.summary.FileWriter(evalSummaryDir, sess.graph)\n",
    "        \n",
    "        \n",
    "        # 初始化所有变量\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)\n",
    "        \n",
    "       # 保存模型的一种方式，保存为pb文件\n",
    "        savedModelPath = \"model/RCNN/savedModel\"\n",
    "        if os.path.exists(savedModelPath):\n",
    "            os.rmdir(savedModelPath)\n",
    "        builder = tf.saved_model.builder.SavedModelBuilder(savedModelPath)\n",
    "            \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def trainStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            训练函数\n",
    "            \"\"\"   \n",
    "            feed_dict = {\n",
    "              lstm.inputX: batchX,\n",
    "              lstm.inputY: batchY,\n",
    "              lstm.dropoutKeepProb: config.model.dropoutKeepProb\n",
    "            }\n",
    "            _, summary, step, loss, predictions = sess.run(\n",
    "                [trainOp, summaryOp, globalStep, lstm.loss, lstm.predictions],\n",
    "                feed_dict)\n",
    "            \n",
    "            if config.numClasses == 1:\n",
    "                acc, recall, prec, f_beta = get_binary_metrics(pred_y=predictions, true_y=batchY)\n",
    "\n",
    "                \n",
    "            elif config.numClasses > 1:\n",
    "                acc, recall, prec, f_beta = get_multi_metrics(pred_y=predictions, true_y=batchY,\n",
    "                                                              labels=labelList)\n",
    "                \n",
    "            trainSummaryWriter.add_summary(summary, step)\n",
    "            \n",
    "            return loss, acc, prec, recall, f_beta\n",
    "\n",
    "        def devStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            验证函数\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              lstm.inputX: batchX,\n",
    "              lstm.inputY: batchY,\n",
    "              lstm.dropoutKeepProb: 1.0\n",
    "            }\n",
    "            summary, step, loss, predictions = sess.run(\n",
    "                [summaryOp, globalStep, lstm.loss, lstm.predictions],\n",
    "                feed_dict)\n",
    "            \n",
    "            if config.numClasses == 1:\n",
    "            \n",
    "                acc, precision, recall, f_beta = get_binary_metrics(pred_y=predictions, true_y=batchY)\n",
    "            elif config.numClasses > 1:\n",
    "                acc, precision, recall, f_beta = get_multi_metrics(pred_y=predictions, true_y=batchY, labels=labelList)\n",
    "            \n",
    "            evalSummaryWriter.add_summary(summary, step)\n",
    "            \n",
    "            return loss, acc, precision, recall, f_beta\n",
    "        \n",
    "        for i in range(config.training.epoches):\n",
    "            # 训练模型\n",
    "            print(\"start training model\")\n",
    "            for batchTrain in nextBatch(trainReviews, trainLabels, config.batchSize):\n",
    "                loss, acc, prec, recall, f_beta = trainStep(batchTrain[0], batchTrain[1])\n",
    "                \n",
    "                currentStep = tf.train.global_step(sess, globalStep) \n",
    "                print(\"train: step: {}, loss: {}, acc: {}, recall: {}, precision: {}, f_beta: {}\".format(\n",
    "                    currentStep, loss, acc, recall, prec, f_beta))\n",
    "                if currentStep % config.training.evaluateEvery == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    \n",
    "                    losses = []\n",
    "                    accs = []\n",
    "                    f_betas = []\n",
    "                    precisions = []\n",
    "                    recalls = []\n",
    "                    \n",
    "                    for batchEval in nextBatch(evalReviews, evalLabels, config.batchSize):\n",
    "                        loss, acc, precision, recall, f_beta = devStep(batchEval[0], batchEval[1])\n",
    "                        losses.append(loss)\n",
    "                        accs.append(acc)\n",
    "                        f_betas.append(f_beta)\n",
    "                        precisions.append(precision)\n",
    "                        recalls.append(recall)\n",
    "                        \n",
    "                    time_str = datetime.datetime.now().isoformat()\n",
    "                    print(\"{}, step: {}, loss: {}, acc: {},precision: {}, recall: {}, f_beta: {}\".format(time_str, currentStep, mean(losses), \n",
    "                                                                                                       mean(accs), mean(precisions),\n",
    "                                                                                                       mean(recalls), mean(f_betas)))\n",
    "                    \n",
    "                if currentStep % config.training.checkpointEvery == 0:\n",
    "                    # 保存模型的另一种方法，保存checkpoint文件\n",
    "                    path = saver.save(sess, \"model/RCNN/model/my-model\", global_step=currentStep)\n",
    "                    print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "                    \n",
    "        inputs = {\"inputX\": tf.saved_model.utils.build_tensor_info(lstm.inputX),\n",
    "                  \"keepProb\": tf.saved_model.utils.build_tensor_info(lstm.dropoutKeepProb)}\n",
    "\n",
    "        outputs = {\"predictions\": tf.saved_model.utils.build_tensor_info(lstm.predictions)}\n",
    "\n",
    "        prediction_signature = tf.saved_model.signature_def_utils.build_signature_def(inputs=inputs, outputs=outputs,\n",
    "                                                                                      method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)\n",
    "        legacy_init_op = tf.group(tf.tables_initializer(), name=\"legacy_init_op\")\n",
    "        builder.add_meta_graph_and_variables(sess, [tf.saved_model.tag_constants.SERVING],\n",
    "                                            signature_def_map={\"predict\": prediction_signature}, legacy_init_op=legacy_init_op)\n",
    "\n",
    "        builder.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"this movie is full of references like mad max ii the wild one and many others the ladybug´s face it´s a clear reference or tribute to peter lorre this movie is a masterpiece we´ll talk much more about in the future\"\n",
    "\n",
    "# 注：下面两个词典要保证和当前加载的模型对应的词典是一致的\n",
    "with open(\"data/wordJson/word2idx.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    word2idx = json.load(f)\n",
    "        \n",
    "with open(\"../data/wordJson/label2idx.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    label2idx = json.load(f)\n",
    "idx2label = {value: key for key, value in label2idx.items()}\n",
    "    \n",
    "xIds = [word2idx.get(item, word2idx[\"UNK\"]) for item in x.split(\" \")]\n",
    "if len(xIds) >= config.sequenceLength:\n",
    "    xIds = xIds[:config.sequenceLength]\n",
    "else:\n",
    "    xIds = xIds + [word2idx[\"PAD\"]] * (config.sequenceLength - len(xIds))\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False, gpu_options=gpu_options)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "\n",
    "    with sess.as_default():\n",
    "        checkpoint_file = tf.train.latest_checkpoint(\"../model/RCNN/model/\")\n",
    "        saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
    "        saver.restore(sess, checkpoint_file)\n",
    "\n",
    "        # 获得需要喂给模型的参数，输出的结果依赖的输入值\n",
    "        inputX = graph.get_operation_by_name(\"inputX\").outputs[0]\n",
    "        dropoutKeepProb = graph.get_operation_by_name(\"dropoutKeepProb\").outputs[0]\n",
    "\n",
    "        # 获得输出的结果\n",
    "        predictions = graph.get_tensor_by_name(\"output/predictions:0\")\n",
    "\n",
    "        pred = sess.run(predictions, feed_dict={inputX: [xIds], dropoutKeepProb: 1.0})[0]\n",
    "        \n",
    "pred = [idx2label[item] for item in pred]     \n",
    "print(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}