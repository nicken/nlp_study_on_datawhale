{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import json\n",
    "\n",
    "from collections import Counter\n",
    "from math import sqrt\n",
    "\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置参数\n",
    "\n",
    "class TrainingConfig(object):\n",
    "    epoches = 5\n",
    "    evaluateEvery = 100\n",
    "    checkpointEvery = 100\n",
    "    learningRate = 0.001\n",
    "    \n",
    "class ModelConfig(object):\n",
    "    embeddingSize = 200\n",
    "    numFilters = 128\n",
    "\n",
    "    filterSizes = [2, 3, 4, 5]\n",
    "    dropoutKeepProb = 0.5\n",
    "    l2RegLambda = 0.0\n",
    "    \n",
    "class Config(object):\n",
    "    sequenceLength = 200  # 取了所有序列长度的均值\n",
    "    batchSize = 128\n",
    "    \n",
    "    dataSource = \"data/labeledTrain.csv\"\n",
    "    \n",
    "    stopWordSource = \"data/english\"\n",
    "    \n",
    "    numClasses = 1  # 二分类设置为1，多分类设置为类别的数目\n",
    "    \n",
    "    rate = 0.8  # 训练集的比例\n",
    "    \n",
    "    training = TrainingConfig()\n",
    "    \n",
    "    model = ModelConfig()\n",
    "\n",
    "    \n",
    "# 实例化配置参数对象\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicken/NLP_study/nlp_env/lib/python3.6/site-packages/ipykernel_launcher.py:129: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    }
   ],
   "source": [
    "# 数据预处理的类，生成训练集和测试集\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self._dataSource = config.dataSource\n",
    "        self._stopWordSource = config.stopWordSource  \n",
    "        \n",
    "        self._sequenceLength = config.sequenceLength  # 每条输入的序列处理为定长\n",
    "        self._embeddingSize = config.model.embeddingSize\n",
    "        self._batchSize = config.batchSize\n",
    "        self._rate = config.rate\n",
    "        \n",
    "        self._stopWordDict = {}\n",
    "        \n",
    "        self.trainReviews = []\n",
    "        self.trainLabels = []\n",
    "        \n",
    "        self.evalReviews = []\n",
    "        self.evalLabels = []\n",
    "        \n",
    "        self.wordEmbedding =None\n",
    "        \n",
    "        self.labelList = []\n",
    "        \n",
    "    def _readData(self, filePath):\n",
    "        \"\"\"\n",
    "        从csv文件中读取数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_csv(filePath)\n",
    "        \n",
    "        if self.config.numClasses == 1:\n",
    "            labels = df[\"sentiment\"].tolist()\n",
    "        elif self.config.numClasses > 1:\n",
    "            labels = df[\"rate\"].tolist()\n",
    "            \n",
    "        review = df[\"review\"].tolist()\n",
    "        reviews = [line.strip().split() for line in review]\n",
    "\n",
    "        return reviews, labels\n",
    "    \n",
    "    def _labelToIndex(self, labels, label2idx):\n",
    "        \"\"\"\n",
    "        将标签转换成索引表示\n",
    "        \"\"\"\n",
    "        labelIds = [label2idx[label] for label in labels]\n",
    "        return labelIds\n",
    "    \n",
    "    def _wordToIndex(self, reviews, word2idx):\n",
    "        \"\"\"\n",
    "        将词转换成索引\n",
    "        \"\"\"\n",
    "        reviewIds = [[word2idx.get(item, word2idx[\"UNK\"]) for item in review] for review in reviews]\n",
    "        return reviewIds\n",
    "        \n",
    "    def _genTrainEvalData(self, x, y, word2idx, rate):\n",
    "        \"\"\"\n",
    "        生成训练集和验证集\n",
    "        \"\"\"\n",
    "        reviews = []\n",
    "        for review in x:\n",
    "            if len(review) >= self._sequenceLength:\n",
    "                reviews.append(review[:self._sequenceLength])\n",
    "            else:\n",
    "                reviews.append(review + [word2idx[\"PAD\"]] * (self._sequenceLength - len(review)))\n",
    "            \n",
    "        trainIndex = int(len(x) * rate)\n",
    "        \n",
    "        trainReviews = np.asarray(reviews[:trainIndex], dtype=\"int64\")\n",
    "        trainLabels = np.array(y[:trainIndex], dtype=\"float32\")\n",
    "        \n",
    "        evalReviews = np.asarray(reviews[trainIndex:], dtype=\"int64\")\n",
    "        evalLabels = np.array(y[trainIndex:], dtype=\"float32\")\n",
    "\n",
    "        return trainReviews, trainLabels, evalReviews, evalLabels\n",
    "        \n",
    "    def _genVocabulary(self, reviews, labels):\n",
    "        \"\"\"\n",
    "        生成词向量和词汇-索引映射字典，可以用全数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        allWords = [word for review in reviews for word in review]\n",
    "        \n",
    "        # 去掉停用词\n",
    "        subWords = [word for word in allWords if word not in self.stopWordDict]\n",
    "        \n",
    "        wordCount = Counter(subWords)  # 统计词频\n",
    "        sortWordCount = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 去除低频词\n",
    "        words = [item[0] for item in sortWordCount if item[1] >= 5]\n",
    "        \n",
    "        vocab, wordEmbedding = self._getWordEmbedding(words)\n",
    "        self.wordEmbedding = wordEmbedding\n",
    "        \n",
    "        word2idx = dict(zip(vocab, list(range(len(vocab)))))\n",
    "        \n",
    "        uniqueLabel = list(set(labels))\n",
    "        label2idx = dict(zip(uniqueLabel, list(range(len(uniqueLabel)))))\n",
    "        self.labelList = list(range(len(uniqueLabel)))\n",
    "        \n",
    "        # 将词汇-索引映射表保存为json数据，之后做inference时直接加载来处理数据\n",
    "        with open(\"data/wordJson/word2idx.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(word2idx, f)\n",
    "        \n",
    "        with open(\"data/wordJson/label2idx.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(label2idx, f)\n",
    "        \n",
    "        return word2idx, label2idx\n",
    "            \n",
    "    def _getWordEmbedding(self, words):\n",
    "        \"\"\"\n",
    "        按照我们的数据集中的单词取出预训练好的word2vec中的词向量\n",
    "        \"\"\"\n",
    "        \n",
    "        wordVec = gensim.models.KeyedVectors.load_word2vec_format(\"data/word2Vec.bin\", binary=True)\n",
    "        vocab = []\n",
    "        wordEmbedding = []\n",
    "        \n",
    "        # 添加 \"pad\" 和 \"UNK\", \n",
    "        vocab.append(\"PAD\")\n",
    "        vocab.append(\"UNK\")\n",
    "        wordEmbedding.append(np.zeros(self._embeddingSize))\n",
    "        wordEmbedding.append(np.random.randn(self._embeddingSize))\n",
    "        \n",
    "        for word in words:\n",
    "            try:\n",
    "                vector = wordVec.wv[word]\n",
    "                vocab.append(word)\n",
    "                wordEmbedding.append(vector)\n",
    "            except:\n",
    "                print(word + \"不存在于词向量中\")\n",
    "                \n",
    "        return vocab, np.array(wordEmbedding)\n",
    "    \n",
    "    def _readStopWord(self, stopWordPath):\n",
    "        \"\"\"\n",
    "        读取停用词\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(stopWordPath, \"r\") as f:\n",
    "            stopWords = f.read()\n",
    "            stopWordList = stopWords.splitlines()\n",
    "            # 将停用词用列表的形式生成，之后查找停用词时会比较快\n",
    "            self.stopWordDict = dict(zip(stopWordList, list(range(len(stopWordList)))))\n",
    "            \n",
    "    def dataGen(self):\n",
    "        \"\"\"\n",
    "        初始化训练集和验证集\n",
    "        \"\"\"\n",
    "        \n",
    "        # 初始化停用词\n",
    "        self._readStopWord(self._stopWordSource)\n",
    "        \n",
    "        # 初始化数据集\n",
    "        reviews, labels = self._readData(self._dataSource)\n",
    "        \n",
    "        # 初始化词汇-索引映射表和词向量矩阵\n",
    "        word2idx, label2idx = self._genVocabulary(reviews, labels)\n",
    "        \n",
    "        # 将标签和句子数值化\n",
    "        labelIds = self._labelToIndex(labels, label2idx)\n",
    "        reviewIds = self._wordToIndex(reviews, word2idx)\n",
    "        \n",
    "        # 初始化训练集和测试集\n",
    "        trainReviews, trainLabels, evalReviews, evalLabels = self._genTrainEvalData(reviewIds, labelIds, word2idx, self._rate)\n",
    "        self.trainReviews = trainReviews\n",
    "        self.trainLabels = trainLabels\n",
    "        \n",
    "        self.evalReviews = evalReviews\n",
    "        self.evalLabels = evalLabels\n",
    "        \n",
    "        \n",
    "data = Dataset(config)\n",
    "data.dataGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (20000, 200)\n",
      "train label shape: (20000,)\n",
      "eval data shape: (5000, 200)\n"
     ]
    }
   ],
   "source": [
    "print(\"train data shape: {}\".format(data.trainReviews.shape))\n",
    "print(\"train label shape: {}\".format(data.trainLabels.shape))\n",
    "print(\"eval data shape: {}\".format(data.evalReviews.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出batch数据集\n",
    "\n",
    "def nextBatch(x, y, batchSize):\n",
    "        \"\"\"\n",
    "        生成batch数据集，用生成器的方式输出\n",
    "        \"\"\"\n",
    "    \n",
    "        perm = np.arange(len(x))\n",
    "        np.random.shuffle(perm)\n",
    "        x = x[perm]\n",
    "        y = y[perm]\n",
    "        \n",
    "        numBatches = len(x) // batchSize\n",
    "\n",
    "        for i in range(numBatches):\n",
    "            start = i * batchSize\n",
    "            end = start + batchSize\n",
    "            batchX = np.array(x[start: end], dtype=\"int64\")\n",
    "            batchY = np.array(y[start: end], dtype=\"float32\")\n",
    "            \n",
    "            yield batchX, batchY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    Text CNN 用于文本分类\n",
    "    \"\"\"\n",
    "    def __init__(self, config, wordEmbedding):\n",
    "\n",
    "        # 定义模型的输入\n",
    "        self.inputX = tf.placeholder(tf.int32, [None, config.sequenceLength], name=\"inputX\")\n",
    "        self.inputY = tf.placeholder(tf.int32, [None], name=\"inputY\")\n",
    "        \n",
    "        self.dropoutKeepProb = tf.placeholder(tf.float32, name=\"dropoutKeepProb\")\n",
    "        \n",
    "        # 定义l2损失\n",
    "        l2Loss = tf.constant(0.0)\n",
    "        \n",
    "        # 词嵌入层\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "\n",
    "            # 利用预训练的词向量初始化词嵌入矩阵\n",
    "            self.W = tf.Variable(tf.cast(wordEmbedding, dtype=tf.float32, name=\"word2vec\") ,name=\"W\")\n",
    "            # 利用词嵌入矩阵将输入的数据中的词转换成词向量，维度[batch_size, sequence_length, embedding_size]\n",
    "            self.embeddedWords = tf.nn.embedding_lookup(self.W, self.inputX)\n",
    "            # 卷积的输入是思维[batch_size, width, height, channel]，因此需要增加维度，用tf.expand_dims来增大维度\n",
    "            self.embeddedWordsExpanded = tf.expand_dims(self.embeddedWords, -1)\n",
    "\n",
    "        # 创建卷积和池化层\n",
    "        pooledOutputs = []\n",
    "        # 有三种size的filter，3， 4， 5，textCNN是个多通道单层卷积的模型，可以看作三个单层的卷积模型的融合\n",
    "        for i, filterSize in enumerate(config.model.filterSizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filterSize):\n",
    "                # 卷积层，卷积核尺寸为filterSize * embeddingSize，卷积核的个数为numFilters\n",
    "                # 初始化权重矩阵和偏置\n",
    "                filterShape = [filterSize, config.model.embeddingSize, 1, config.model.numFilters]\n",
    "                W = tf.Variable(tf.truncated_normal(filterShape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[config.model.numFilters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embeddedWordsExpanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                \n",
    "                # relu函数的非线性映射\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                \n",
    "                # 池化层，最大池化，池化是对卷积后的序列取一个最大值\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, config.sequenceLength - filterSize + 1, 1, 1],  # ksize shape: [batch, height, width, channels]\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooledOutputs.append(pooled)  # 将三种size的filter的输出一起加入到列表中\n",
    "\n",
    "        # 得到CNN网络的输出长度\n",
    "        numFiltersTotal = config.model.numFilters * len(config.model.filterSizes)\n",
    "        \n",
    "        # 池化后的维度不变，按照最后的维度channel来concat\n",
    "        self.hPool = tf.concat(pooledOutputs, 3)\n",
    "        \n",
    "        # 摊平成二维的数据输入到全连接层\n",
    "        self.hPoolFlat = tf.reshape(self.hPool, [-1, numFiltersTotal])\n",
    "\n",
    "        # dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.hDrop = tf.nn.dropout(self.hPoolFlat, self.dropoutKeepProb)\n",
    "       \n",
    "        # 全连接层的输出\n",
    "        with tf.name_scope(\"output\"):\n",
    "            outputW = tf.get_variable(\n",
    "                \"outputW\",\n",
    "                shape=[numFiltersTotal, config.numClasses],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            outputB= tf.Variable(tf.constant(0.1, shape=[config.numClasses]), name=\"outputB\")\n",
    "            l2Loss += tf.nn.l2_loss(outputW)\n",
    "            l2Loss += tf.nn.l2_loss(outputB)\n",
    "            self.logits = tf.nn.xw_plus_b(self.hDrop, outputW, outputB, name=\"logits\")\n",
    "            if config.numClasses == 1:\n",
    "                self.predictions = tf.cast(tf.greater_equal(self.logits, 0.0), tf.int32, name=\"predictions\")\n",
    "            elif config.numClasses > 1:\n",
    "                self.predictions = tf.argmax(self.logits, axis=-1, name=\"predictions\")\n",
    "            \n",
    "            print(self.predictions)\n",
    "        \n",
    "        # 计算二元交叉熵损失\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            \n",
    "            if config.numClasses == 1:\n",
    "                losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.logits, labels=tf.cast(tf.reshape(self.inputY, [-1, 1]), \n",
    "                                                                                                    dtype=tf.float32))\n",
    "            elif config.numClasses > 1:\n",
    "                losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.inputY)\n",
    "                \n",
    "            self.loss = tf.reduce_mean(losses) + config.model.l2RegLambda * l2Loss\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "定义各类性能指标\n",
    "\"\"\"\n",
    "\n",
    "def mean(item: list) -> float:\n",
    "    \"\"\"\n",
    "    计算列表中元素的平均值\n",
    "    :param item: 列表对象\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    res = sum(item) / len(item) if len(item) > 0 else 0\n",
    "    return res\n",
    "\n",
    "\n",
    "def accuracy(pred_y, true_y):\n",
    "    \"\"\"\n",
    "    计算二类和多类的准确率\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if isinstance(pred_y[0], list):\n",
    "        pred_y = [item[0] for item in pred_y]\n",
    "    corr = 0\n",
    "    for i in range(len(pred_y)):\n",
    "        if pred_y[i] == true_y[i]:\n",
    "            corr += 1\n",
    "    acc = corr / len(pred_y) if len(pred_y) > 0 else 0\n",
    "    return acc\n",
    "\n",
    "\n",
    "def binary_precision(pred_y, true_y, positive=1):\n",
    "    \"\"\"\n",
    "    二类的精确率计算\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param positive: 正例的索引表示\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    corr = 0\n",
    "    pred_corr = 0\n",
    "    for i in range(len(pred_y)):\n",
    "        if pred_y[i] == positive:\n",
    "            pred_corr += 1\n",
    "            if pred_y[i] == true_y[i]:\n",
    "                corr += 1\n",
    "\n",
    "    prec = corr / pred_corr if pred_corr > 0 else 0\n",
    "    return prec\n",
    "\n",
    "\n",
    "def binary_recall(pred_y, true_y, positive=1):\n",
    "    \"\"\"\n",
    "    二类的召回率\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param positive: 正例的索引表示\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    corr = 0\n",
    "    true_corr = 0\n",
    "    for i in range(len(pred_y)):\n",
    "        if true_y[i] == positive:\n",
    "            true_corr += 1\n",
    "            if pred_y[i] == true_y[i]:\n",
    "                corr += 1\n",
    "\n",
    "    rec = corr / true_corr if true_corr > 0 else 0\n",
    "    return rec\n",
    "\n",
    "\n",
    "def binary_f_beta(pred_y, true_y, beta=1.0, positive=1):\n",
    "    \"\"\"\n",
    "    二类的f beta值\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param beta: beta值\n",
    "    :param positive: 正例的索引表示\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    precision = binary_precision(pred_y, true_y, positive)\n",
    "    recall = binary_recall(pred_y, true_y, positive)\n",
    "    try:\n",
    "        f_b = (1 + beta * beta) * precision * recall / (beta * beta * precision + recall)\n",
    "    except:\n",
    "        f_b = 0\n",
    "    return f_b\n",
    "\n",
    "\n",
    "def multi_precision(pred_y, true_y, labels):\n",
    "    \"\"\"\n",
    "    多类的精确率\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param labels: 标签列表\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if isinstance(pred_y[0], list):\n",
    "        pred_y = [item[0] for item in pred_y]\n",
    "\n",
    "    precisions = [binary_precision(pred_y, true_y, label) for label in labels]\n",
    "    prec = mean(precisions)\n",
    "    return prec\n",
    "\n",
    "\n",
    "def multi_recall(pred_y, true_y, labels):\n",
    "    \"\"\"\n",
    "    多类的召回率\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param labels: 标签列表\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if isinstance(pred_y[0], list):\n",
    "        pred_y = [item[0] for item in pred_y]\n",
    "\n",
    "    recalls = [binary_recall(pred_y, true_y, label) for label in labels]\n",
    "    rec = mean(recalls)\n",
    "    return rec\n",
    "\n",
    "\n",
    "def multi_f_beta(pred_y, true_y, labels, beta=1.0):\n",
    "    \"\"\"\n",
    "    多类的f beta值\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param labels: 标签列表\n",
    "    :param beta: beta值\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if isinstance(pred_y[0], list):\n",
    "        pred_y = [item[0] for item in pred_y]\n",
    "\n",
    "    f_betas = [binary_f_beta(pred_y, true_y, beta, label) for label in labels]\n",
    "    f_beta = mean(f_betas)\n",
    "    return f_beta\n",
    "\n",
    "\n",
    "def get_binary_metrics(pred_y, true_y, f_beta=1.0):\n",
    "    \"\"\"\n",
    "    得到二分类的性能指标\n",
    "    :param pred_y:\n",
    "    :param true_y:\n",
    "    :param f_beta:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    acc = accuracy(pred_y, true_y)\n",
    "    recall = binary_recall(pred_y, true_y)\n",
    "    precision = binary_precision(pred_y, true_y)\n",
    "    f_beta = binary_f_beta(pred_y, true_y, f_beta)\n",
    "    return acc, recall, precision, f_beta\n",
    "\n",
    "\n",
    "def get_multi_metrics(pred_y, true_y, labels, f_beta=1.0):\n",
    "    \"\"\"\n",
    "    得到多分类的性能指标\n",
    "    :param pred_y:\n",
    "    :param true_y:\n",
    "    :param labels:\n",
    "    :param f_beta:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    acc = accuracy(pred_y, true_y)\n",
    "    recall = multi_recall(pred_y, true_y, labels)\n",
    "    precision = multi_precision(pred_y, true_y, labels)\n",
    "    f_beta = multi_f_beta(pred_y, true_y, labels, f_beta)\n",
    "    return acc, recall, precision, f_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"output/predictions:0\", shape=(?, 1), dtype=int32)\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/W:0/grad/hist is illegal; using conv-maxpool-2/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/W:0/grad/sparsity is illegal; using conv-maxpool-2/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/b:0/grad/hist is illegal; using conv-maxpool-2/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/b:0/grad/sparsity is illegal; using conv-maxpool-2/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name outputW:0/grad/hist is illegal; using outputW_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name outputW:0/grad/sparsity is illegal; using outputW_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/outputB:0/grad/hist is illegal; using output/outputB_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/outputB:0/grad/sparsity is illegal; using output/outputB_0/grad/sparsity instead.\n",
      "Writing to /home/nicken/NLP_study/nlp_task/task_4/summarys\n",
      "\n",
      "start training model\n",
      "train: step: 1, loss: 2.002155065536499, acc: 0.40625, recall: 0.46774193548387094, precision: 0.4027777777777778, f_beta: 0.4328358208955224\n",
      "train: step: 2, loss: 3.2052407264709473, acc: 0.5078125, recall: 0.0, precision: 0.0, f_beta: 0\n",
      "train: step: 3, loss: 1.5410375595092773, acc: 0.5703125, recall: 0.423728813559322, precision: 0.5434782608695652, f_beta: 0.47619047619047616\n",
      "train: step: 4, loss: 2.416452646255493, acc: 0.46875, recall: 0.8181818181818182, precision: 0.4909090909090909, f_beta: 0.6136363636363636\n",
      "train: step: 5, loss: 2.4520444869995117, acc: 0.5, recall: 0.8805970149253731, precision: 0.5130434782608696, f_beta: 0.6483516483516484\n",
      "train: step: 6, loss: 2.1274404525756836, acc: 0.5, recall: 0.8, precision: 0.48, f_beta: 0.6\n",
      "train: step: 7, loss: 1.504202127456665, acc: 0.4921875, recall: 0.47761194029850745, precision: 0.5161290322580645, f_beta: 0.49612403100775193\n",
      "train: step: 8, loss: 1.9252952337265015, acc: 0.4921875, recall: 0.23214285714285715, precision: 0.37142857142857144, f_beta: 0.2857142857142857\n",
      "train: step: 9, loss: 2.2781076431274414, acc: 0.4921875, recall: 0.11666666666666667, precision: 0.3684210526315789, f_beta: 0.17721518987341772\n",
      "train: step: 10, loss: 2.2779722213745117, acc: 0.4140625, recall: 0.11428571428571428, precision: 0.38095238095238093, f_beta: 0.17582417582417584\n",
      "train: step: 11, loss: 1.6188273429870605, acc: 0.4296875, recall: 0.23880597014925373, precision: 0.42105263157894735, f_beta: 0.30476190476190473\n",
      "train: step: 12, loss: 1.6142394542694092, acc: 0.5546875, recall: 0.6666666666666666, precision: 0.5569620253164557, f_beta: 0.6068965517241379\n",
      "train: step: 13, loss: 2.0871734619140625, acc: 0.4765625, recall: 0.6774193548387096, precision: 0.47191011235955055, f_beta: 0.5562913907284769\n",
      "train: step: 14, loss: 1.989884853363037, acc: 0.515625, recall: 0.890625, precision: 0.5089285714285714, f_beta: 0.6477272727272727\n",
      "train: step: 15, loss: 1.9228734970092773, acc: 0.515625, recall: 0.8909090909090909, precision: 0.4666666666666667, f_beta: 0.6125\n",
      "train: step: 16, loss: 1.2643327713012695, acc: 0.5390625, recall: 0.6666666666666666, precision: 0.5432098765432098, f_beta: 0.5986394557823128\n",
      "train: step: 17, loss: 1.6663098335266113, acc: 0.4921875, recall: 0.4153846153846154, precision: 0.5, f_beta: 0.453781512605042\n",
      "train: step: 18, loss: 1.9937620162963867, acc: 0.484375, recall: 0.2465753424657534, precision: 0.6206896551724138, f_beta: 0.35294117647058826\n",
      "train: step: 19, loss: 1.6888949871063232, acc: 0.4765625, recall: 0.20689655172413793, precision: 0.36363636363636365, f_beta: 0.26373626373626374\n",
      "train: step: 20, loss: 1.43984854221344, acc: 0.5390625, recall: 0.28125, precision: 0.5806451612903226, f_beta: 0.37894736842105264\n",
      "train: step: 21, loss: 1.3653122186660767, acc: 0.5234375, recall: 0.4375, precision: 0.5283018867924528, f_beta: 0.47863247863247865\n",
      "train: step: 22, loss: 1.449161171913147, acc: 0.5, recall: 0.589041095890411, precision: 0.5584415584415584, f_beta: 0.5733333333333333\n",
      "train: step: 23, loss: 1.9170022010803223, acc: 0.46875, recall: 0.8103448275862069, precision: 0.4519230769230769, f_beta: 0.5802469135802469\n",
      "train: step: 24, loss: 1.5631914138793945, acc: 0.578125, recall: 0.7777777777777778, precision: 0.550561797752809, f_beta: 0.644736842105263\n",
      "train: step: 25, loss: 1.3280470371246338, acc: 0.6015625, recall: 0.7076923076923077, precision: 0.5897435897435898, f_beta: 0.6433566433566433\n",
      "train: step: 26, loss: 1.3932571411132812, acc: 0.484375, recall: 0.5576923076923077, precision: 0.4027777777777778, f_beta: 0.46774193548387094\n",
      "train: step: 27, loss: 1.5958712100982666, acc: 0.5078125, recall: 0.4262295081967213, precision: 0.48148148148148145, f_beta: 0.45217391304347826\n",
      "train: step: 28, loss: 1.4793977737426758, acc: 0.53125, recall: 0.38461538461538464, precision: 0.7142857142857143, f_beta: 0.5\n",
      "train: step: 29, loss: 1.4520255327224731, acc: 0.5390625, recall: 0.31666666666666665, precision: 0.5135135135135135, f_beta: 0.39175257731958757\n",
      "train: step: 30, loss: 1.0169507265090942, acc: 0.5859375, recall: 0.37735849056603776, precision: 0.5, f_beta: 0.43010752688172044\n",
      "train: step: 31, loss: 1.4091931581497192, acc: 0.4453125, recall: 0.4126984126984127, precision: 0.43333333333333335, f_beta: 0.4227642276422764\n",
      "train: step: 32, loss: 1.3018410205841064, acc: 0.46875, recall: 0.5362318840579711, precision: 0.5068493150684932, f_beta: 0.5211267605633804\n",
      "train: step: 33, loss: 1.3713417053222656, acc: 0.5078125, recall: 0.5671641791044776, precision: 0.5277777777777778, f_beta: 0.5467625899280575\n",
      "train: step: 34, loss: 1.5781859159469604, acc: 0.5234375, recall: 0.6567164179104478, precision: 0.5365853658536586, f_beta: 0.5906040268456376\n",
      "train: step: 35, loss: 1.2104980945587158, acc: 0.6015625, recall: 0.7692307692307693, precision: 0.5813953488372093, f_beta: 0.6622516556291391\n",
      "train: step: 36, loss: 1.4394490718841553, acc: 0.3984375, recall: 0.6226415094339622, precision: 0.36666666666666664, f_beta: 0.46153846153846156\n",
      "train: step: 37, loss: 1.1883211135864258, acc: 0.578125, recall: 0.6567164179104478, precision: 0.5866666666666667, f_beta: 0.619718309859155\n",
      "train: step: 38, loss: 1.2344770431518555, acc: 0.5625, recall: 0.546875, precision: 0.5645161290322581, f_beta: 0.5555555555555557\n",
      "train: step: 39, loss: 1.377733588218689, acc: 0.4921875, recall: 0.2537313432835821, precision: 0.53125, f_beta: 0.3434343434343434\n",
      "train: step: 40, loss: 1.162766695022583, acc: 0.546875, recall: 0.3283582089552239, precision: 0.6285714285714286, f_beta: 0.4313725490196078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 41, loss: 0.8055738806724548, acc: 0.6484375, recall: 0.46296296296296297, precision: 0.6097560975609756, f_beta: 0.5263157894736843\n",
      "train: step: 42, loss: 1.083658218383789, acc: 0.515625, recall: 0.4, precision: 0.43137254901960786, f_beta: 0.41509433962264153\n",
      "train: step: 43, loss: 1.043562889099121, acc: 0.6015625, recall: 0.6825396825396826, precision: 0.581081081081081, f_beta: 0.6277372262773723\n",
      "train: step: 44, loss: 0.8973970413208008, acc: 0.609375, recall: 0.7377049180327869, precision: 0.569620253164557, f_beta: 0.6428571428571429\n",
      "train: step: 45, loss: 0.9642280340194702, acc: 0.5546875, recall: 0.6619718309859155, precision: 0.5875, f_beta: 0.6225165562913908\n",
      "train: step: 46, loss: 0.9768763184547424, acc: 0.5703125, recall: 0.6901408450704225, precision: 0.5975609756097561, f_beta: 0.6405228758169934\n",
      "train: step: 47, loss: 1.266152262687683, acc: 0.515625, recall: 0.6825396825396826, precision: 0.5058823529411764, f_beta: 0.581081081081081\n",
      "train: step: 48, loss: 0.7165749669075012, acc: 0.671875, recall: 0.6527777777777778, precision: 0.734375, f_beta: 0.6911764705882354\n",
      "train: step: 49, loss: 0.9416438937187195, acc: 0.578125, recall: 0.5892857142857143, precision: 0.515625, f_beta: 0.5499999999999999\n",
      "train: step: 50, loss: 1.0246834754943848, acc: 0.6015625, recall: 0.5166666666666667, precision: 0.5849056603773585, f_beta: 0.5486725663716815\n",
      "train: step: 51, loss: 1.010420560836792, acc: 0.6171875, recall: 0.6031746031746031, precision: 0.6129032258064516, f_beta: 0.608\n",
      "train: step: 52, loss: 1.1288533210754395, acc: 0.4921875, recall: 0.2647058823529412, precision: 0.5454545454545454, f_beta: 0.3564356435643565\n",
      "train: step: 53, loss: 0.8339502215385437, acc: 0.640625, recall: 0.5254237288135594, precision: 0.6326530612244898, f_beta: 0.5740740740740742\n",
      "train: step: 54, loss: 1.013318657875061, acc: 0.5390625, recall: 0.5737704918032787, precision: 0.5147058823529411, f_beta: 0.5426356589147286\n",
      "train: step: 55, loss: 0.7775667905807495, acc: 0.640625, recall: 0.625, precision: 0.5833333333333334, f_beta: 0.603448275862069\n",
      "train: step: 56, loss: 0.7030093669891357, acc: 0.6484375, recall: 0.6712328767123288, precision: 0.7, f_beta: 0.6853146853146853\n",
      "train: step: 57, loss: 0.8243668079376221, acc: 0.6640625, recall: 0.7543859649122807, precision: 0.5972222222222222, f_beta: 0.6666666666666666\n",
      "train: step: 58, loss: 0.9730273485183716, acc: 0.5625, recall: 0.6268656716417911, precision: 0.5753424657534246, f_beta: 0.6\n",
      "train: step: 59, loss: 0.9413933753967285, acc: 0.5625, recall: 0.6615384615384615, precision: 0.5584415584415584, f_beta: 0.6056338028169014\n",
      "train: step: 60, loss: 0.7564687132835388, acc: 0.6875, recall: 0.7746478873239436, precision: 0.6962025316455697, f_beta: 0.7333333333333333\n",
      "train: step: 61, loss: 0.8883780241012573, acc: 0.640625, recall: 0.5084745762711864, precision: 0.6382978723404256, f_beta: 0.5660377358490567\n",
      "train: step: 62, loss: 1.053214430809021, acc: 0.5390625, recall: 0.4375, precision: 0.5490196078431373, f_beta: 0.48695652173913045\n",
      "train: step: 63, loss: 0.8581460118293762, acc: 0.609375, recall: 0.46551724137931033, precision: 0.5869565217391305, f_beta: 0.5192307692307693\n",
      "train: step: 64, loss: 0.9382055401802063, acc: 0.5859375, recall: 0.5, precision: 0.660377358490566, f_beta: 0.5691056910569106\n",
      "train: step: 65, loss: 0.811175525188446, acc: 0.65625, recall: 0.6901408450704225, precision: 0.6901408450704225, f_beta: 0.6901408450704225\n",
      "train: step: 66, loss: 0.6404534578323364, acc: 0.6875, recall: 0.7857142857142857, precision: 0.6875, f_beta: 0.7333333333333334\n",
      "train: step: 67, loss: 0.8118000626564026, acc: 0.6796875, recall: 0.8620689655172413, precision: 0.6024096385542169, f_beta: 0.7092198581560283\n",
      "train: step: 68, loss: 0.9092258214950562, acc: 0.5703125, recall: 0.6779661016949152, precision: 0.5263157894736842, f_beta: 0.5925925925925927\n",
      "train: step: 69, loss: 0.8616164326667786, acc: 0.5859375, recall: 0.5909090909090909, precision: 0.6, f_beta: 0.5954198473282443\n",
      "train: step: 70, loss: 0.7068382501602173, acc: 0.671875, recall: 0.6065573770491803, precision: 0.6727272727272727, f_beta: 0.6379310344827586\n",
      "train: step: 71, loss: 0.8911167979240417, acc: 0.609375, recall: 0.44285714285714284, precision: 0.7380952380952381, f_beta: 0.5535714285714286\n",
      "train: step: 72, loss: 1.0408358573913574, acc: 0.4921875, recall: 0.375, precision: 0.574468085106383, f_beta: 0.453781512605042\n",
      "train: step: 73, loss: 0.694981575012207, acc: 0.65625, recall: 0.6545454545454545, precision: 0.5901639344262295, f_beta: 0.6206896551724138\n",
      "train: step: 74, loss: 0.726117730140686, acc: 0.6640625, recall: 0.7580645161290323, precision: 0.6266666666666667, f_beta: 0.6861313868613139\n",
      "train: step: 75, loss: 0.7388342618942261, acc: 0.6640625, recall: 0.7540983606557377, precision: 0.6216216216216216, f_beta: 0.6814814814814815\n",
      "train: step: 76, loss: 0.7383664846420288, acc: 0.6640625, recall: 0.75, precision: 0.64, f_beta: 0.6906474820143884\n",
      "train: step: 77, loss: 0.6297858953475952, acc: 0.671875, recall: 0.6833333333333333, precision: 0.640625, f_beta: 0.6612903225806451\n",
      "train: step: 78, loss: 0.778373658657074, acc: 0.625, recall: 0.5441176470588235, precision: 0.6851851851851852, f_beta: 0.6065573770491803\n",
      "train: step: 79, loss: 0.6047070026397705, acc: 0.75, recall: 0.6716417910447762, precision: 0.8181818181818182, f_beta: 0.7377049180327869\n",
      "train: step: 80, loss: 0.6464235782623291, acc: 0.671875, recall: 0.6909090909090909, precision: 0.6031746031746031, f_beta: 0.6440677966101694\n",
      "train: step: 81, loss: 0.76368248462677, acc: 0.625, recall: 0.47619047619047616, precision: 0.6666666666666666, f_beta: 0.5555555555555556\n",
      "train: step: 82, loss: 0.6601927280426025, acc: 0.65625, recall: 0.6031746031746031, precision: 0.6666666666666666, f_beta: 0.6333333333333333\n",
      "train: step: 83, loss: 0.6465041041374207, acc: 0.7109375, recall: 0.6818181818181818, precision: 0.7377049180327869, f_beta: 0.7086614173228346\n",
      "train: step: 84, loss: 0.8228901624679565, acc: 0.65625, recall: 0.7540983606557377, precision: 0.6133333333333333, f_beta: 0.676470588235294\n",
      "train: step: 85, loss: 0.6711478233337402, acc: 0.671875, recall: 0.7321428571428571, precision: 0.6029411764705882, f_beta: 0.6612903225806451\n",
      "train: step: 86, loss: 0.6668819785118103, acc: 0.6796875, recall: 0.75, precision: 0.6575342465753424, f_beta: 0.7007299270072993\n",
      "train: step: 87, loss: 0.7584413290023804, acc: 0.671875, recall: 0.64, precision: 0.7619047619047619, f_beta: 0.6956521739130435\n",
      "train: step: 88, loss: 0.6891998648643494, acc: 0.625, recall: 0.6440677966101694, precision: 0.5846153846153846, f_beta: 0.6129032258064516\n",
      "train: step: 89, loss: 0.7648154497146606, acc: 0.640625, recall: 0.5660377358490566, precision: 0.5660377358490566, f_beta: 0.5660377358490566\n",
      "train: step: 90, loss: 0.5973526835441589, acc: 0.7265625, recall: 0.578125, precision: 0.8222222222222222, f_beta: 0.6788990825688074\n",
      "train: step: 91, loss: 0.6148267388343811, acc: 0.7265625, recall: 0.6666666666666666, precision: 0.6792452830188679, f_beta: 0.6728971962616822\n",
      "train: step: 92, loss: 0.5704165697097778, acc: 0.734375, recall: 0.7358490566037735, precision: 0.6610169491525424, f_beta: 0.6964285714285713\n",
      "train: step: 93, loss: 0.6358044743537903, acc: 0.7265625, recall: 0.676056338028169, precision: 0.8, f_beta: 0.7328244274809161\n",
      "train: step: 94, loss: 0.5875977277755737, acc: 0.6484375, recall: 0.5471698113207547, precision: 0.58, f_beta: 0.5631067961165048\n",
      "train: step: 95, loss: 0.6238620281219482, acc: 0.6796875, recall: 0.75, precision: 0.6575342465753424, f_beta: 0.7007299270072993\n",
      "train: step: 96, loss: 0.6048458814620972, acc: 0.6484375, recall: 0.7213114754098361, precision: 0.6111111111111112, f_beta: 0.6616541353383459\n",
      "train: step: 97, loss: 0.5650681853294373, acc: 0.7109375, recall: 0.7428571428571429, precision: 0.7323943661971831, f_beta: 0.7375886524822696\n",
      "train: step: 98, loss: 0.6541634798049927, acc: 0.65625, recall: 0.6119402985074627, precision: 0.6949152542372882, f_beta: 0.6507936507936508\n",
      "train: step: 99, loss: 0.6750930547714233, acc: 0.7109375, recall: 0.6851851851851852, precision: 0.6491228070175439, f_beta: 0.6666666666666666\n",
      "train: step: 100, loss: 0.6302057504653931, acc: 0.71875, recall: 0.6865671641791045, precision: 0.7540983606557377, f_beta: 0.71875\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-14T10:52:00.920477, step: 100, loss: 0.4609536574437068, acc: 0.7940705128205128,precision: 0.7766715977164438, recall: 0.8074047240316222, f_beta: 0.790511623394431\n",
      "Saved model checkpoint to model/textCNN/model/my-model-100\n",
      "\n",
      "train: step: 101, loss: 0.6613962650299072, acc: 0.6640625, recall: 0.609375, precision: 0.6842105263157895, f_beta: 0.6446280991735538\n",
      "train: step: 102, loss: 0.632408082485199, acc: 0.7421875, recall: 0.734375, precision: 0.746031746031746, f_beta: 0.7401574803149606\n",
      "train: step: 103, loss: 0.6356856822967529, acc: 0.75, recall: 0.7142857142857143, precision: 0.8064516129032258, f_beta: 0.7575757575757576\n",
      "train: step: 104, loss: 0.7837862968444824, acc: 0.609375, recall: 0.6779661016949152, precision: 0.5633802816901409, f_beta: 0.6153846153846153\n",
      "train: step: 105, loss: 0.6533947587013245, acc: 0.6640625, recall: 0.6557377049180327, precision: 0.6451612903225806, f_beta: 0.6504065040650406\n",
      "train: step: 106, loss: 0.6028982996940613, acc: 0.7265625, recall: 0.782608695652174, precision: 0.7297297297297297, f_beta: 0.7552447552447553\n",
      "train: step: 107, loss: 0.5371299386024475, acc: 0.734375, recall: 0.84375, precision: 0.6923076923076923, f_beta: 0.7605633802816902\n",
      "train: step: 108, loss: 0.5596700310707092, acc: 0.7421875, recall: 0.7424242424242424, precision: 0.7538461538461538, f_beta: 0.7480916030534351\n",
      "train: step: 109, loss: 0.5882130265235901, acc: 0.6875, recall: 0.6507936507936508, precision: 0.6949152542372882, f_beta: 0.6721311475409837\n",
      "train: step: 110, loss: 0.6146458983421326, acc: 0.7109375, recall: 0.6567164179104478, precision: 0.7586206896551724, f_beta: 0.7040000000000001\n",
      "train: step: 111, loss: 0.5664000511169434, acc: 0.734375, recall: 0.6052631578947368, precision: 0.92, f_beta: 0.7301587301587301\n",
      "train: step: 112, loss: 0.64149010181427, acc: 0.671875, recall: 0.7, precision: 0.7, f_beta: 0.7\n",
      "train: step: 113, loss: 0.5815252065658569, acc: 0.7265625, recall: 0.8032786885245902, precision: 0.6805555555555556, f_beta: 0.7368421052631579\n",
      "train: step: 114, loss: 0.5700637102127075, acc: 0.703125, recall: 0.8, precision: 0.6486486486486487, f_beta: 0.7164179104477612\n",
      "train: step: 115, loss: 0.690992534160614, acc: 0.6484375, recall: 0.7313432835820896, precision: 0.6447368421052632, f_beta: 0.6853146853146853\n",
      "train: step: 116, loss: 0.5936284065246582, acc: 0.734375, recall: 0.8360655737704918, precision: 0.68, f_beta: 0.7500000000000001\n",
      "train: step: 117, loss: 0.6224513053894043, acc: 0.703125, recall: 0.7083333333333334, precision: 0.75, f_beta: 0.7285714285714285\n",
      "train: step: 118, loss: 0.5574092268943787, acc: 0.7265625, recall: 0.7301587301587301, precision: 0.71875, f_beta: 0.7244094488188977\n",
      "train: step: 119, loss: 0.5085364580154419, acc: 0.78125, recall: 0.7142857142857143, precision: 0.8181818181818182, f_beta: 0.7627118644067797\n",
      "train: step: 120, loss: 0.6788514852523804, acc: 0.6171875, recall: 0.5970149253731343, precision: 0.6451612903225806, f_beta: 0.6201550387596899\n",
      "train: step: 121, loss: 0.558396577835083, acc: 0.7109375, recall: 0.6451612903225806, precision: 0.7272727272727273, f_beta: 0.6837606837606838\n",
      "train: step: 122, loss: 0.48132970929145813, acc: 0.7421875, recall: 0.6615384615384615, precision: 0.7962962962962963, f_beta: 0.7226890756302522\n",
      "train: step: 123, loss: 0.5707992315292358, acc: 0.7109375, recall: 0.7419354838709677, precision: 0.6865671641791045, f_beta: 0.7131782945736433\n",
      "train: step: 124, loss: 0.5529077053070068, acc: 0.734375, recall: 0.7611940298507462, precision: 0.7391304347826086, f_beta: 0.75\n",
      "train: step: 125, loss: 0.5963782072067261, acc: 0.6796875, recall: 0.7796610169491526, precision: 0.6216216216216216, f_beta: 0.6917293233082706\n",
      "train: step: 126, loss: 0.58084636926651, acc: 0.6953125, recall: 0.7307692307692307, precision: 0.76, f_beta: 0.7450980392156863\n",
      "train: step: 127, loss: 0.5788443088531494, acc: 0.71875, recall: 0.75, precision: 0.7058823529411765, f_beta: 0.7272727272727272\n",
      "train: step: 128, loss: 0.5345313549041748, acc: 0.7265625, recall: 0.78125, precision: 0.704225352112676, f_beta: 0.7407407407407407\n",
      "train: step: 129, loss: 0.5678837299346924, acc: 0.6796875, recall: 0.7761194029850746, precision: 0.6666666666666666, f_beta: 0.7172413793103448\n",
      "train: step: 130, loss: 0.553813099861145, acc: 0.7578125, recall: 0.75, precision: 0.7377049180327869, f_beta: 0.743801652892562\n",
      "train: step: 131, loss: 0.5225623250007629, acc: 0.7578125, recall: 0.7857142857142857, precision: 0.6984126984126984, f_beta: 0.7394957983193275\n",
      "train: step: 132, loss: 0.5440207123756409, acc: 0.734375, recall: 0.6507936507936508, precision: 0.7735849056603774, f_beta: 0.706896551724138\n",
      "train: step: 133, loss: 0.6590564250946045, acc: 0.65625, recall: 0.5405405405405406, precision: 0.8, f_beta: 0.6451612903225806\n",
      "train: step: 134, loss: 0.47201278805732727, acc: 0.796875, recall: 0.7681159420289855, precision: 0.8412698412698413, f_beta: 0.8030303030303031\n",
      "train: step: 135, loss: 0.7293684482574463, acc: 0.65625, recall: 0.6515151515151515, precision: 0.671875, f_beta: 0.6615384615384616\n",
      "train: step: 136, loss: 0.42014414072036743, acc: 0.8125, recall: 0.8571428571428571, precision: 0.8354430379746836, f_beta: 0.846153846153846\n",
      "train: step: 137, loss: 0.49261707067489624, acc: 0.7734375, recall: 0.8970588235294118, precision: 0.7349397590361446, f_beta: 0.8079470198675497\n",
      "train: step: 138, loss: 0.5053001046180725, acc: 0.7578125, recall: 0.8484848484848485, precision: 0.7272727272727273, f_beta: 0.7832167832167832\n",
      "train: step: 139, loss: 0.6519384384155273, acc: 0.6796875, recall: 0.8225806451612904, precision: 0.6296296296296297, f_beta: 0.7132867132867133\n",
      "train: step: 140, loss: 0.6792461276054382, acc: 0.6875, recall: 0.8, precision: 0.6582278481012658, f_beta: 0.7222222222222222\n",
      "train: step: 141, loss: 0.4907708168029785, acc: 0.7109375, recall: 0.6710526315789473, precision: 0.8095238095238095, f_beta: 0.7338129496402879\n",
      "train: step: 142, loss: 0.5442070364952087, acc: 0.7578125, recall: 0.6716417910447762, precision: 0.8333333333333334, f_beta: 0.7438016528925621\n",
      "train: step: 143, loss: 0.5843772888183594, acc: 0.703125, recall: 0.6557377049180327, precision: 0.7017543859649122, f_beta: 0.6779661016949152\n",
      "train: step: 144, loss: 0.564102828502655, acc: 0.734375, recall: 0.6153846153846154, precision: 0.8163265306122449, f_beta: 0.7017543859649124\n",
      "train: step: 145, loss: 0.4974518418312073, acc: 0.7265625, recall: 0.6825396825396826, precision: 0.7413793103448276, f_beta: 0.7107438016528926\n",
      "train: step: 146, loss: 0.4086391031742096, acc: 0.8203125, recall: 0.8983050847457628, precision: 0.7571428571428571, f_beta: 0.8217054263565892\n",
      "train: step: 147, loss: 0.6952451467514038, acc: 0.6796875, recall: 0.7162162162162162, precision: 0.726027397260274, f_beta: 0.7210884353741497\n",
      "train: step: 148, loss: 0.5212681293487549, acc: 0.7265625, recall: 0.7611940298507462, precision: 0.7285714285714285, f_beta: 0.7445255474452556\n",
      "train: step: 149, loss: 0.42357495427131653, acc: 0.8046875, recall: 0.8529411764705882, precision: 0.7945205479452054, f_beta: 0.8226950354609929\n",
      "train: step: 150, loss: 0.47445112466812134, acc: 0.78125, recall: 0.873015873015873, precision: 0.7333333333333333, f_beta: 0.7971014492753622\n",
      "train: step: 151, loss: 0.46436309814453125, acc: 0.78125, recall: 0.8620689655172413, precision: 0.7142857142857143, f_beta: 0.7812500000000001\n",
      "train: step: 152, loss: 0.5792247653007507, acc: 0.7109375, recall: 0.673469387755102, precision: 0.6111111111111112, f_beta: 0.6407766990291262\n",
      "train: step: 153, loss: 0.490554541349411, acc: 0.734375, recall: 0.65, precision: 0.75, f_beta: 0.6964285714285715\n",
      "train: step: 154, loss: 0.45601221919059753, acc: 0.75, recall: 0.6415094339622641, precision: 0.723404255319149, f_beta: 0.68\n",
      "train: step: 155, loss: 0.507867157459259, acc: 0.7890625, recall: 0.7352941176470589, precision: 0.847457627118644, f_beta: 0.7874015748031497\n",
      "train: step: 156, loss: 0.5935356616973877, acc: 0.734375, recall: 0.6031746031746031, precision: 0.8085106382978723, f_beta: 0.6909090909090909\n",
      "start training model\n",
      "train: step: 157, loss: 0.535961389541626, acc: 0.734375, recall: 0.6666666666666666, precision: 0.7857142857142857, f_beta: 0.721311475409836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 158, loss: 0.4982028603553772, acc: 0.7421875, recall: 0.746031746031746, precision: 0.734375, f_beta: 0.7401574803149606\n",
      "train: step: 159, loss: 0.44284728169441223, acc: 0.796875, recall: 0.9491525423728814, precision: 0.7088607594936709, f_beta: 0.8115942028985508\n",
      "train: step: 160, loss: 0.41622939705848694, acc: 0.8125, recall: 0.8793103448275862, precision: 0.75, f_beta: 0.8095238095238094\n",
      "train: step: 161, loss: 0.4562831521034241, acc: 0.8125, recall: 0.9558823529411765, precision: 0.7558139534883721, f_beta: 0.8441558441558441\n",
      "train: step: 162, loss: 0.39028725028038025, acc: 0.8046875, recall: 0.8412698412698413, precision: 0.7794117647058824, f_beta: 0.8091603053435115\n",
      "train: step: 163, loss: 0.3867289125919342, acc: 0.8359375, recall: 0.8636363636363636, precision: 0.8260869565217391, f_beta: 0.8444444444444444\n",
      "train: step: 164, loss: 0.4626031517982483, acc: 0.75, recall: 0.6949152542372882, precision: 0.7454545454545455, f_beta: 0.7192982456140351\n",
      "train: step: 165, loss: 0.37305811047554016, acc: 0.8671875, recall: 0.7903225806451613, precision: 0.9245283018867925, f_beta: 0.8521739130434782\n",
      "train: step: 166, loss: 0.5046421885490417, acc: 0.75, recall: 0.6212121212121212, precision: 0.8541666666666666, f_beta: 0.7192982456140351\n",
      "train: step: 167, loss: 0.3171100616455078, acc: 0.8828125, recall: 0.7884615384615384, precision: 0.9111111111111111, f_beta: 0.845360824742268\n",
      "train: step: 168, loss: 0.46732693910598755, acc: 0.796875, recall: 0.7536231884057971, precision: 0.8524590163934426, f_beta: 0.8\n",
      "train: step: 169, loss: 0.4677916169166565, acc: 0.7734375, recall: 0.7972972972972973, precision: 0.8082191780821918, f_beta: 0.802721088435374\n",
      "train: step: 170, loss: 0.4874805212020874, acc: 0.765625, recall: 0.8260869565217391, precision: 0.76, f_beta: 0.7916666666666667\n",
      "train: step: 171, loss: 0.42948827147483826, acc: 0.78125, recall: 0.8923076923076924, precision: 0.7341772151898734, f_beta: 0.8055555555555556\n",
      "train: step: 172, loss: 0.42726796865463257, acc: 0.7890625, recall: 0.8714285714285714, precision: 0.7721518987341772, f_beta: 0.8187919463087248\n",
      "train: step: 173, loss: 0.3906741738319397, acc: 0.8203125, recall: 0.896551724137931, precision: 0.7536231884057971, f_beta: 0.8188976377952756\n",
      "train: step: 174, loss: 0.4497966766357422, acc: 0.7578125, recall: 0.8970588235294118, precision: 0.7176470588235294, f_beta: 0.7973856209150327\n",
      "train: step: 175, loss: 0.3711242079734802, acc: 0.859375, recall: 0.8666666666666667, precision: 0.8904109589041096, f_beta: 0.8783783783783784\n",
      "train: step: 176, loss: 0.4078499674797058, acc: 0.84375, recall: 0.8166666666666667, precision: 0.8448275862068966, f_beta: 0.8305084745762712\n",
      "train: step: 177, loss: 0.4270794093608856, acc: 0.8125, recall: 0.8032786885245902, precision: 0.8032786885245902, f_beta: 0.8032786885245902\n",
      "train: step: 178, loss: 0.4566846489906311, acc: 0.7421875, recall: 0.6666666666666666, precision: 0.68, f_beta: 0.6732673267326733\n",
      "train: step: 179, loss: 0.4486269950866699, acc: 0.75, recall: 0.6923076923076923, precision: 0.7894736842105263, f_beta: 0.7377049180327868\n",
      "train: step: 180, loss: 0.4708503484725952, acc: 0.7890625, recall: 0.782608695652174, precision: 0.8181818181818182, f_beta: 0.8\n",
      "train: step: 181, loss: 0.4487733244895935, acc: 0.8203125, recall: 0.78125, precision: 0.847457627118644, f_beta: 0.8130081300813008\n",
      "train: step: 182, loss: 0.3410775363445282, acc: 0.8828125, recall: 0.9365079365079365, precision: 0.8428571428571429, f_beta: 0.887218045112782\n",
      "train: step: 183, loss: 0.3491031527519226, acc: 0.8359375, recall: 0.8636363636363636, precision: 0.8260869565217391, f_beta: 0.8444444444444444\n",
      "train: step: 184, loss: 0.39913082122802734, acc: 0.8046875, recall: 0.8333333333333334, precision: 0.7971014492753623, f_beta: 0.8148148148148148\n",
      "train: step: 185, loss: 0.4146527349948883, acc: 0.796875, recall: 0.7910447761194029, precision: 0.8153846153846154, f_beta: 0.803030303030303\n",
      "train: step: 186, loss: 0.4525691866874695, acc: 0.7421875, recall: 0.8181818181818182, precision: 0.72, f_beta: 0.7659574468085107\n",
      "train: step: 187, loss: 0.39608481526374817, acc: 0.8046875, recall: 0.8059701492537313, precision: 0.8181818181818182, f_beta: 0.8120300751879699\n",
      "train: step: 188, loss: 0.36653009057044983, acc: 0.8515625, recall: 0.9444444444444444, precision: 0.7611940298507462, f_beta: 0.8429752066115703\n",
      "train: step: 189, loss: 0.3807448148727417, acc: 0.859375, recall: 0.8301886792452831, precision: 0.8301886792452831, f_beta: 0.8301886792452831\n",
      "train: step: 190, loss: 0.4634815752506256, acc: 0.7734375, recall: 0.7931034482758621, precision: 0.7301587301587301, f_beta: 0.7603305785123967\n",
      "train: step: 191, loss: 0.4266507625579834, acc: 0.7890625, recall: 0.7246376811594203, precision: 0.8620689655172413, f_beta: 0.7874015748031497\n",
      "train: step: 192, loss: 0.499081015586853, acc: 0.796875, recall: 0.7538461538461538, precision: 0.8305084745762712, f_beta: 0.7903225806451613\n",
      "train: step: 193, loss: 0.4582061171531677, acc: 0.7734375, recall: 0.7142857142857143, precision: 0.8035714285714286, f_beta: 0.7563025210084034\n",
      "train: step: 194, loss: 0.48728057742118835, acc: 0.765625, recall: 0.7090909090909091, precision: 0.7358490566037735, f_beta: 0.7222222222222221\n",
      "train: step: 195, loss: 0.36059772968292236, acc: 0.8515625, recall: 0.875, precision: 0.835820895522388, f_beta: 0.8549618320610687\n",
      "train: step: 196, loss: 0.40505343675613403, acc: 0.7890625, recall: 0.7704918032786885, precision: 0.7833333333333333, f_beta: 0.7768595041322314\n",
      "train: step: 197, loss: 0.34391844272613525, acc: 0.8125, recall: 0.8666666666666667, precision: 0.7647058823529411, f_beta: 0.8125\n",
      "train: step: 198, loss: 0.3475605249404907, acc: 0.859375, recall: 0.9090909090909091, precision: 0.8333333333333334, f_beta: 0.8695652173913043\n",
      "train: step: 199, loss: 0.38195520639419556, acc: 0.859375, recall: 0.8939393939393939, precision: 0.8428571428571429, f_beta: 0.8676470588235294\n",
      "train: step: 200, loss: 0.414842814207077, acc: 0.8203125, recall: 0.8524590163934426, precision: 0.7878787878787878, f_beta: 0.8188976377952757\n",
      "\n",
      "Evaluation:\n",
      "2019-08-14T10:52:24.178879, step: 200, loss: 0.3705869859609848, acc: 0.8391426282051282,precision: 0.8457459776118443, recall: 0.8382008134418168, f_beta: 0.8411278404769019\n",
      "Saved model checkpoint to model/textCNN/model/my-model-200\n",
      "\n",
      "train: step: 201, loss: 0.43449971079826355, acc: 0.734375, recall: 0.7619047619047619, precision: 0.7164179104477612, f_beta: 0.7384615384615385\n",
      "train: step: 202, loss: 0.3204951286315918, acc: 0.8828125, recall: 0.8939393939393939, precision: 0.8805970149253731, f_beta: 0.887218045112782\n",
      "train: step: 203, loss: 0.40327441692352295, acc: 0.828125, recall: 0.847457627118644, precision: 0.7936507936507936, f_beta: 0.819672131147541\n",
      "train: step: 204, loss: 0.3896651864051819, acc: 0.8125, recall: 0.7681159420289855, precision: 0.8688524590163934, f_beta: 0.8153846153846154\n",
      "train: step: 205, loss: 0.5274181365966797, acc: 0.71875, recall: 0.7142857142857143, precision: 0.7142857142857143, f_beta: 0.7142857142857143\n",
      "train: step: 206, loss: 0.5095341205596924, acc: 0.7578125, recall: 0.6428571428571429, precision: 0.7659574468085106, f_beta: 0.6990291262135921\n",
      "train: step: 207, loss: 0.3818455934524536, acc: 0.8125, recall: 0.7758620689655172, precision: 0.8035714285714286, f_beta: 0.7894736842105263\n",
      "train: step: 208, loss: 0.37994492053985596, acc: 0.8046875, recall: 0.8571428571428571, precision: 0.7714285714285715, f_beta: 0.81203007518797\n",
      "train: step: 209, loss: 0.38472211360931396, acc: 0.8359375, recall: 0.9047619047619048, precision: 0.7916666666666666, f_beta: 0.8444444444444444\n",
      "train: step: 210, loss: 0.4573505222797394, acc: 0.78125, recall: 0.7580645161290323, precision: 0.7833333333333333, f_beta: 0.7704918032786884\n",
      "train: step: 211, loss: 0.3286866247653961, acc: 0.859375, recall: 0.8108108108108109, precision: 0.9375, f_beta: 0.8695652173913043\n",
      "train: step: 212, loss: 0.3619672656059265, acc: 0.84375, recall: 0.8909090909090909, precision: 0.7777777777777778, f_beta: 0.8305084745762712\n",
      "train: step: 213, loss: 0.5072640776634216, acc: 0.7890625, recall: 0.828125, precision: 0.7681159420289855, f_beta: 0.7969924812030075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 214, loss: 0.39494991302490234, acc: 0.8359375, recall: 0.8805970149253731, precision: 0.8194444444444444, f_beta: 0.8489208633093526\n",
      "train: step: 215, loss: 0.5241665244102478, acc: 0.765625, recall: 0.7272727272727273, precision: 0.8, f_beta: 0.761904761904762\n",
      "train: step: 216, loss: 0.34553343057632446, acc: 0.8671875, recall: 0.8571428571428571, precision: 0.8709677419354839, f_beta: 0.864\n",
      "train: step: 217, loss: 0.33695217967033386, acc: 0.875, recall: 0.8529411764705882, precision: 0.90625, f_beta: 0.8787878787878787\n",
      "train: step: 218, loss: 0.35796067118644714, acc: 0.8515625, recall: 0.8787878787878788, precision: 0.8405797101449275, f_beta: 0.8592592592592593\n",
      "train: step: 219, loss: 0.4115264117717743, acc: 0.828125, recall: 0.7966101694915254, precision: 0.8245614035087719, f_beta: 0.8103448275862069\n",
      "train: step: 220, loss: 0.36995935440063477, acc: 0.890625, recall: 0.9322033898305084, precision: 0.8461538461538461, f_beta: 0.8870967741935484\n",
      "train: step: 221, loss: 0.45273011922836304, acc: 0.7890625, recall: 0.7794117647058824, precision: 0.8153846153846154, f_beta: 0.7969924812030074\n",
      "train: step: 222, loss: 0.4086570739746094, acc: 0.8125, recall: 0.873015873015873, precision: 0.7746478873239436, f_beta: 0.8208955223880596\n",
      "train: step: 223, loss: 0.3717418909072876, acc: 0.84375, recall: 0.8059701492537313, precision: 0.8852459016393442, f_beta: 0.84375\n",
      "train: step: 224, loss: 0.3555583357810974, acc: 0.8203125, recall: 0.8333333333333334, precision: 0.8208955223880597, f_beta: 0.8270676691729324\n",
      "train: step: 225, loss: 0.5306938290596008, acc: 0.78125, recall: 0.7704918032786885, precision: 0.7704918032786885, f_beta: 0.7704918032786885\n",
      "train: step: 226, loss: 0.41162076592445374, acc: 0.8125, recall: 0.8307692307692308, precision: 0.8059701492537313, f_beta: 0.8181818181818182\n",
      "train: step: 227, loss: 0.43712571263313293, acc: 0.796875, recall: 0.7857142857142857, precision: 0.7586206896551724, f_beta: 0.7719298245614034\n",
      "train: step: 228, loss: 0.3631928563117981, acc: 0.84375, recall: 0.8615384615384616, precision: 0.835820895522388, f_beta: 0.8484848484848485\n",
      "train: step: 229, loss: 0.4310707151889801, acc: 0.75, recall: 0.8070175438596491, precision: 0.6865671641791045, f_beta: 0.7419354838709676\n",
      "train: step: 230, loss: 0.39907267689704895, acc: 0.8203125, recall: 0.7592592592592593, precision: 0.803921568627451, f_beta: 0.780952380952381\n",
      "train: step: 231, loss: 0.377533495426178, acc: 0.875, recall: 0.8307692307692308, precision: 0.9152542372881356, f_beta: 0.870967741935484\n",
      "train: step: 232, loss: 0.29078221321105957, acc: 0.890625, recall: 0.9104477611940298, precision: 0.8840579710144928, f_beta: 0.8970588235294118\n",
      "train: step: 233, loss: 0.4572886824607849, acc: 0.7890625, recall: 0.7727272727272727, precision: 0.8095238095238095, f_beta: 0.7906976744186046\n",
      "train: step: 234, loss: 0.42994681000709534, acc: 0.78125, recall: 0.7285714285714285, precision: 0.85, f_beta: 0.7846153846153846\n",
      "train: step: 235, loss: 0.469675213098526, acc: 0.796875, recall: 0.819672131147541, precision: 0.7692307692307693, f_beta: 0.7936507936507937\n",
      "train: step: 236, loss: 0.3625941276550293, acc: 0.8515625, recall: 0.9333333333333333, precision: 0.7887323943661971, f_beta: 0.8549618320610686\n",
      "train: step: 237, loss: 0.34561577439308167, acc: 0.84375, recall: 0.8225806451612904, precision: 0.85, f_beta: 0.8360655737704918\n",
      "train: step: 238, loss: 0.325143039226532, acc: 0.8671875, recall: 0.8787878787878788, precision: 0.8656716417910447, f_beta: 0.8721804511278195\n",
      "train: step: 239, loss: 0.39492884278297424, acc: 0.8125, recall: 0.84375, precision: 0.7941176470588235, f_beta: 0.8181818181818182\n",
      "train: step: 240, loss: 0.38093358278274536, acc: 0.8515625, recall: 0.8382352941176471, precision: 0.8769230769230769, f_beta: 0.8571428571428571\n",
      "train: step: 241, loss: 0.3890395164489746, acc: 0.8359375, recall: 0.8441558441558441, precision: 0.8783783783783784, f_beta: 0.8609271523178808\n",
      "train: step: 242, loss: 0.38366734981536865, acc: 0.828125, recall: 0.8852459016393442, precision: 0.782608695652174, f_beta: 0.8307692307692308\n",
      "train: step: 243, loss: 0.3508535921573639, acc: 0.8359375, recall: 0.8870967741935484, precision: 0.7971014492753623, f_beta: 0.8396946564885496\n",
      "train: step: 244, loss: 0.30246052145957947, acc: 0.8828125, recall: 0.8305084745762712, precision: 0.9074074074074074, f_beta: 0.8672566371681415\n",
      "train: step: 245, loss: 0.3858380615711212, acc: 0.828125, recall: 0.8382352941176471, precision: 0.8382352941176471, f_beta: 0.8382352941176471\n",
      "train: step: 246, loss: 0.47596707940101624, acc: 0.7421875, recall: 0.8596491228070176, precision: 0.6621621621621622, f_beta: 0.7480916030534351\n",
      "train: step: 247, loss: 0.32035309076309204, acc: 0.8515625, recall: 0.8307692307692308, precision: 0.8709677419354839, f_beta: 0.8503937007874016\n",
      "train: step: 248, loss: 0.35897302627563477, acc: 0.84375, recall: 0.8103448275862069, precision: 0.8392857142857143, f_beta: 0.8245614035087718\n",
      "train: step: 249, loss: 0.32634997367858887, acc: 0.890625, recall: 0.8181818181818182, precision: 0.9183673469387755, f_beta: 0.8653846153846154\n",
      "train: step: 250, loss: 0.36204564571380615, acc: 0.8203125, recall: 0.7692307692307693, precision: 0.8620689655172413, f_beta: 0.8130081300813008\n",
      "train: step: 251, loss: 0.29958730936050415, acc: 0.890625, recall: 0.8387096774193549, precision: 0.9285714285714286, f_beta: 0.8813559322033899\n",
      "train: step: 252, loss: 0.3598024547100067, acc: 0.8203125, recall: 0.7903225806451613, precision: 0.8305084745762712, f_beta: 0.8099173553719008\n",
      "train: step: 253, loss: 0.296434223651886, acc: 0.8828125, recall: 0.8493150684931506, precision: 0.9393939393939394, f_beta: 0.8920863309352518\n",
      "train: step: 254, loss: 0.3814753293991089, acc: 0.8515625, recall: 0.8727272727272727, precision: 0.8, f_beta: 0.8347826086956521\n",
      "train: step: 255, loss: 0.4540943503379822, acc: 0.7734375, recall: 0.9259259259259259, precision: 0.6666666666666666, f_beta: 0.7751937984496123\n",
      "train: step: 256, loss: 0.3755154013633728, acc: 0.8125, recall: 0.8548387096774194, precision: 0.7794117647058824, f_beta: 0.8153846153846154\n",
      "train: step: 257, loss: 0.32298874855041504, acc: 0.875, recall: 0.90625, precision: 0.8529411764705882, f_beta: 0.8787878787878787\n",
      "train: step: 258, loss: 0.453592449426651, acc: 0.796875, recall: 0.7611940298507462, precision: 0.8360655737704918, f_beta: 0.796875\n",
      "train: step: 259, loss: 0.3221896290779114, acc: 0.8828125, recall: 0.9166666666666666, precision: 0.8461538461538461, f_beta: 0.8799999999999999\n",
      "train: step: 260, loss: 0.29871901869773865, acc: 0.875, recall: 0.8732394366197183, precision: 0.8985507246376812, f_beta: 0.8857142857142857\n",
      "train: step: 261, loss: 0.38224199414253235, acc: 0.828125, recall: 0.8115942028985508, precision: 0.8615384615384616, f_beta: 0.835820895522388\n",
      "train: step: 262, loss: 0.5065835118293762, acc: 0.828125, recall: 0.8103448275862069, precision: 0.8103448275862069, f_beta: 0.8103448275862069\n",
      "train: step: 263, loss: 0.25694674253463745, acc: 0.890625, recall: 0.9193548387096774, precision: 0.8636363636363636, f_beta: 0.890625\n",
      "train: step: 264, loss: 0.294033408164978, acc: 0.875, recall: 0.835820895522388, precision: 0.9180327868852459, f_beta: 0.875\n",
      "train: step: 265, loss: 0.3504965007305145, acc: 0.8671875, recall: 0.8928571428571429, precision: 0.819672131147541, f_beta: 0.8547008547008548\n",
      "train: step: 266, loss: 0.3725607395172119, acc: 0.8359375, recall: 0.8970588235294118, precision: 0.8133333333333334, f_beta: 0.8531468531468531\n",
      "train: step: 267, loss: 0.3951469957828522, acc: 0.875, recall: 0.9444444444444444, precision: 0.85, f_beta: 0.8947368421052632\n",
      "train: step: 268, loss: 0.359708696603775, acc: 0.84375, recall: 0.8271604938271605, precision: 0.9178082191780822, f_beta: 0.8701298701298701\n",
      "train: step: 269, loss: 0.31852608919143677, acc: 0.859375, recall: 0.9333333333333333, precision: 0.8, f_beta: 0.8615384615384616\n",
      "train: step: 270, loss: 0.39093148708343506, acc: 0.828125, recall: 0.875, precision: 0.8, f_beta: 0.8358208955223881\n",
      "train: step: 271, loss: 0.36230236291885376, acc: 0.8125, recall: 0.9387755102040817, precision: 0.6865671641791045, f_beta: 0.7931034482758622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 272, loss: 0.3945762813091278, acc: 0.796875, recall: 0.8181818181818182, precision: 0.7941176470588235, f_beta: 0.8059701492537314\n",
      "train: step: 273, loss: 0.42830923199653625, acc: 0.7578125, recall: 0.68, precision: 0.8793103448275862, f_beta: 0.7669172932330827\n",
      "train: step: 274, loss: 0.365809828042984, acc: 0.84375, recall: 0.7647058823529411, precision: 0.9285714285714286, f_beta: 0.8387096774193549\n",
      "train: step: 275, loss: 0.3939528167247772, acc: 0.828125, recall: 0.8285714285714286, precision: 0.8529411764705882, f_beta: 0.8405797101449276\n",
      "train: step: 276, loss: 0.36055445671081543, acc: 0.8515625, recall: 0.8840579710144928, precision: 0.8472222222222222, f_beta: 0.8652482269503546\n",
      "train: step: 277, loss: 0.34555575251579285, acc: 0.8671875, recall: 0.8939393939393939, precision: 0.855072463768116, f_beta: 0.8740740740740741\n",
      "train: step: 278, loss: 0.4833444654941559, acc: 0.8125, recall: 0.8571428571428571, precision: 0.75, f_beta: 0.7999999999999999\n",
      "train: step: 279, loss: 0.3753822147846222, acc: 0.8359375, recall: 0.9016393442622951, precision: 0.7857142857142857, f_beta: 0.8396946564885497\n",
      "train: step: 280, loss: 0.44222143292427063, acc: 0.7578125, recall: 0.8, precision: 0.7428571428571429, f_beta: 0.7703703703703704\n",
      "train: step: 281, loss: 0.43440648913383484, acc: 0.8203125, recall: 0.7941176470588235, precision: 0.8571428571428571, f_beta: 0.8244274809160305\n",
      "train: step: 282, loss: 0.3150792121887207, acc: 0.8828125, recall: 0.8888888888888888, precision: 0.9014084507042254, f_beta: 0.8951048951048951\n",
      "train: step: 283, loss: 0.36778926849365234, acc: 0.8359375, recall: 0.8548387096774194, precision: 0.8153846153846154, f_beta: 0.8346456692913387\n",
      "train: step: 284, loss: 0.4545414447784424, acc: 0.8046875, recall: 0.8, precision: 0.7586206896551724, f_beta: 0.7787610619469026\n",
      "train: step: 285, loss: 0.343107670545578, acc: 0.828125, recall: 0.8115942028985508, precision: 0.8615384615384616, f_beta: 0.835820895522388\n",
      "train: step: 286, loss: 0.3898128271102905, acc: 0.828125, recall: 0.8103448275862069, precision: 0.8103448275862069, f_beta: 0.8103448275862069\n",
      "train: step: 287, loss: 0.37472638487815857, acc: 0.8203125, recall: 0.7692307692307693, precision: 0.8620689655172413, f_beta: 0.8130081300813008\n",
      "train: step: 288, loss: 0.3448472321033478, acc: 0.8515625, recall: 0.8493150684931506, precision: 0.8857142857142857, f_beta: 0.8671328671328671\n",
      "train: step: 289, loss: 0.4237506687641144, acc: 0.8359375, recall: 0.8157894736842105, precision: 0.8985507246376812, f_beta: 0.8551724137931034\n",
      "train: step: 290, loss: 0.3158659040927887, acc: 0.859375, recall: 0.9253731343283582, precision: 0.8266666666666667, f_beta: 0.8732394366197183\n",
      "train: step: 291, loss: 0.38569799065589905, acc: 0.8203125, recall: 0.9264705882352942, precision: 0.7777777777777778, f_beta: 0.8456375838926173\n",
      "train: step: 292, loss: 0.3751155436038971, acc: 0.8359375, recall: 0.9833333333333333, precision: 0.7468354430379747, f_beta: 0.8489208633093525\n",
      "train: step: 293, loss: 0.33079200983047485, acc: 0.8515625, recall: 0.8787878787878788, precision: 0.8405797101449275, f_beta: 0.8592592592592593\n",
      "train: step: 294, loss: 0.3775911331176758, acc: 0.8203125, recall: 0.8615384615384616, precision: 0.8, f_beta: 0.8296296296296297\n",
      "train: step: 295, loss: 0.360666424036026, acc: 0.8515625, recall: 0.847457627118644, precision: 0.8333333333333334, f_beta: 0.8403361344537815\n",
      "train: step: 296, loss: 0.2937934398651123, acc: 0.9140625, recall: 0.8955223880597015, precision: 0.9375, f_beta: 0.9160305343511451\n",
      "train: step: 297, loss: 0.3413130044937134, acc: 0.8515625, recall: 0.7868852459016393, precision: 0.8888888888888888, f_beta: 0.8347826086956522\n",
      "train: step: 298, loss: 0.4182124137878418, acc: 0.84375, recall: 0.7777777777777778, precision: 0.9333333333333333, f_beta: 0.8484848484848485\n",
      "train: step: 299, loss: 0.43265238404273987, acc: 0.8046875, recall: 0.7580645161290323, precision: 0.8245614035087719, f_beta: 0.7899159663865545\n",
      "train: step: 300, loss: 0.325026273727417, acc: 0.8515625, recall: 0.8208955223880597, precision: 0.8870967741935484, f_beta: 0.8527131782945736\n",
      "\n",
      "Evaluation:\n",
      "2019-08-14T10:52:48.374351, step: 300, loss: 0.34482461366897976, acc: 0.8501602564102564,precision: 0.8921402638249486, recall: 0.8259881829381255, f_beta: 0.8569525098071458\n",
      "Saved model checkpoint to model/textCNN/model/my-model-300\n",
      "\n",
      "train: step: 301, loss: 0.3199250102043152, acc: 0.859375, recall: 0.8928571428571429, precision: 0.8064516129032258, f_beta: 0.8474576271186439\n",
      "train: step: 302, loss: 0.41115981340408325, acc: 0.7890625, recall: 0.847457627118644, precision: 0.7352941176470589, f_beta: 0.7874015748031497\n",
      "train: step: 303, loss: 0.27039092779159546, acc: 0.9140625, recall: 0.9850746268656716, precision: 0.868421052631579, f_beta: 0.923076923076923\n",
      "train: step: 304, loss: 0.5303385257720947, acc: 0.78125, recall: 0.8939393939393939, precision: 0.7375, f_beta: 0.8082191780821918\n",
      "train: step: 305, loss: 0.37795013189315796, acc: 0.8203125, recall: 0.8983050847457628, precision: 0.7571428571428571, f_beta: 0.8217054263565892\n",
      "train: step: 306, loss: 0.3683159053325653, acc: 0.859375, recall: 0.8548387096774194, precision: 0.8548387096774194, f_beta: 0.8548387096774194\n",
      "train: step: 307, loss: 0.3580837845802307, acc: 0.828125, recall: 0.7714285714285715, precision: 0.9, f_beta: 0.8307692307692307\n",
      "train: step: 308, loss: 0.4009215533733368, acc: 0.828125, recall: 0.8064516129032258, precision: 0.8333333333333334, f_beta: 0.819672131147541\n",
      "train: step: 309, loss: 0.37324392795562744, acc: 0.828125, recall: 0.75, precision: 0.8888888888888888, f_beta: 0.8135593220338982\n",
      "train: step: 310, loss: 0.3191368579864502, acc: 0.8671875, recall: 0.8153846153846154, precision: 0.9137931034482759, f_beta: 0.8617886178861788\n",
      "train: step: 311, loss: 0.38370633125305176, acc: 0.796875, recall: 0.8, precision: 0.8, f_beta: 0.8000000000000002\n",
      "train: step: 312, loss: 0.3860463500022888, acc: 0.84375, recall: 0.9047619047619048, precision: 0.8028169014084507, f_beta: 0.8507462686567164\n",
      "start training model\n",
      "train: step: 313, loss: 0.28175368905067444, acc: 0.8984375, recall: 0.9375, precision: 0.8695652173913043, f_beta: 0.9022556390977444\n",
      "train: step: 314, loss: 0.37559974193573, acc: 0.8125, recall: 0.8636363636363636, precision: 0.7916666666666666, f_beta: 0.8260869565217391\n",
      "train: step: 315, loss: 0.35359638929367065, acc: 0.8359375, recall: 0.8771929824561403, precision: 0.78125, f_beta: 0.8264462809917354\n",
      "train: step: 316, loss: 0.247310072183609, acc: 0.8984375, recall: 0.9178082191780822, precision: 0.9054054054054054, f_beta: 0.9115646258503401\n",
      "train: step: 317, loss: 0.31767737865448, acc: 0.8671875, recall: 0.8805970149253731, precision: 0.8676470588235294, f_beta: 0.874074074074074\n",
      "train: step: 318, loss: 0.3194940984249115, acc: 0.8671875, recall: 0.8615384615384616, precision: 0.875, f_beta: 0.8682170542635659\n",
      "train: step: 319, loss: 0.19016733765602112, acc: 0.9375, recall: 0.9230769230769231, precision: 0.9523809523809523, f_beta: 0.9375\n",
      "train: step: 320, loss: 0.3095622658729553, acc: 0.8671875, recall: 0.8727272727272727, precision: 0.8275862068965517, f_beta: 0.8495575221238938\n",
      "train: step: 321, loss: 0.23919814825057983, acc: 0.9296875, recall: 0.9298245614035088, precision: 0.9137931034482759, f_beta: 0.9217391304347825\n",
      "train: step: 322, loss: 0.4085085391998291, acc: 0.796875, recall: 0.7627118644067796, precision: 0.7894736842105263, f_beta: 0.7758620689655172\n",
      "train: step: 323, loss: 0.3285669982433319, acc: 0.859375, recall: 0.8235294117647058, precision: 0.9032258064516129, f_beta: 0.8615384615384616\n",
      "train: step: 324, loss: 0.2177121937274933, acc: 0.9140625, recall: 0.8939393939393939, precision: 0.9365079365079365, f_beta: 0.9147286821705426\n",
      "train: step: 325, loss: 0.3640088737010956, acc: 0.8515625, recall: 0.8064516129032258, precision: 0.8771929824561403, f_beta: 0.8403361344537815\n",
      "train: step: 326, loss: 0.38287073373794556, acc: 0.796875, recall: 0.7903225806451613, precision: 0.7903225806451613, f_beta: 0.7903225806451614\n",
      "train: step: 327, loss: 0.23822098970413208, acc: 0.90625, recall: 0.9054054054054054, precision: 0.9305555555555556, f_beta: 0.9178082191780821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 328, loss: 0.27552488446235657, acc: 0.875, recall: 0.8985507246376812, precision: 0.8732394366197183, f_beta: 0.8857142857142857\n",
      "train: step: 329, loss: 0.22405369579792023, acc: 0.90625, recall: 0.9333333333333333, precision: 0.9090909090909091, f_beta: 0.9210526315789475\n",
      "train: step: 330, loss: 0.2722238004207611, acc: 0.890625, recall: 0.9696969696969697, precision: 0.8421052631578947, f_beta: 0.9014084507042254\n",
      "train: step: 331, loss: 0.25730738043785095, acc: 0.8984375, recall: 0.9836065573770492, precision: 0.8333333333333334, f_beta: 0.9022556390977444\n",
      "train: step: 332, loss: 0.28255602717399597, acc: 0.890625, recall: 0.9411764705882353, precision: 0.8135593220338984, f_beta: 0.8727272727272728\n",
      "train: step: 333, loss: 0.33972930908203125, acc: 0.8359375, recall: 0.8428571428571429, precision: 0.855072463768116, f_beta: 0.8489208633093526\n",
      "train: step: 334, loss: 0.3796095848083496, acc: 0.859375, recall: 0.7704918032786885, precision: 0.9215686274509803, f_beta: 0.8392857142857142\n",
      "train: step: 335, loss: 0.3514230251312256, acc: 0.84375, recall: 0.7575757575757576, precision: 0.9259259259259259, f_beta: 0.8333333333333334\n",
      "train: step: 336, loss: 0.2843744158744812, acc: 0.8828125, recall: 0.8260869565217391, precision: 0.95, f_beta: 0.8837209302325583\n",
      "train: step: 337, loss: 0.3060033321380615, acc: 0.8828125, recall: 0.8648648648648649, precision: 0.927536231884058, f_beta: 0.8951048951048951\n",
      "train: step: 338, loss: 0.2802671492099762, acc: 0.8828125, recall: 0.8714285714285714, precision: 0.9104477611940298, f_beta: 0.8905109489051095\n",
      "train: step: 339, loss: 0.28698426485061646, acc: 0.890625, recall: 0.9152542372881356, precision: 0.8571428571428571, f_beta: 0.8852459016393444\n",
      "train: step: 340, loss: 0.3038356304168701, acc: 0.890625, recall: 0.9830508474576272, precision: 0.8169014084507042, f_beta: 0.8923076923076924\n",
      "train: step: 341, loss: 0.32293131947517395, acc: 0.8828125, recall: 0.9827586206896551, precision: 0.8028169014084507, f_beta: 0.8837209302325583\n",
      "train: step: 342, loss: 0.3044487237930298, acc: 0.859375, recall: 0.9365079365079365, precision: 0.8082191780821918, f_beta: 0.8676470588235294\n",
      "train: step: 343, loss: 0.2991286814212799, acc: 0.8671875, recall: 0.828125, precision: 0.8983050847457628, f_beta: 0.8617886178861789\n",
      "train: step: 344, loss: 0.2578502297401428, acc: 0.890625, recall: 0.8813559322033898, precision: 0.8813559322033898, f_beta: 0.8813559322033898\n",
      "train: step: 345, loss: 0.40331047773361206, acc: 0.828125, recall: 0.8, precision: 0.875, f_beta: 0.8358208955223881\n",
      "train: step: 346, loss: 0.25365912914276123, acc: 0.8828125, recall: 0.8548387096774194, precision: 0.8983050847457628, f_beta: 0.8760330578512397\n",
      "train: step: 347, loss: 0.290121853351593, acc: 0.8828125, recall: 0.8923076923076924, precision: 0.8787878787878788, f_beta: 0.8854961832061069\n",
      "train: step: 348, loss: 0.23704230785369873, acc: 0.9140625, recall: 0.8666666666666667, precision: 0.9454545454545454, f_beta: 0.9043478260869566\n",
      "train: step: 349, loss: 0.22699150443077087, acc: 0.9375, recall: 0.9074074074074074, precision: 0.9423076923076923, f_beta: 0.9245283018867925\n",
      "train: step: 350, loss: 0.2710202932357788, acc: 0.890625, recall: 0.8867924528301887, precision: 0.8545454545454545, f_beta: 0.8703703703703703\n",
      "train: step: 351, loss: 0.26345986127853394, acc: 0.890625, recall: 0.9285714285714286, precision: 0.8783783783783784, f_beta: 0.9027777777777779\n",
      "train: step: 352, loss: 0.2967317998409271, acc: 0.875, recall: 0.8947368421052632, precision: 0.8360655737704918, f_beta: 0.864406779661017\n",
      "train: step: 353, loss: 0.21473871171474457, acc: 0.9296875, recall: 0.90625, precision: 0.9508196721311475, f_beta: 0.9279999999999999\n",
      "train: step: 354, loss: 0.19396932423114777, acc: 0.921875, recall: 0.9166666666666666, precision: 0.9166666666666666, f_beta: 0.9166666666666666\n",
      "train: step: 355, loss: 0.30528175830841064, acc: 0.890625, recall: 0.875, precision: 0.9032258064516129, f_beta: 0.8888888888888888\n",
      "train: step: 356, loss: 0.2690946161746979, acc: 0.90625, recall: 0.9295774647887324, precision: 0.9041095890410958, f_beta: 0.9166666666666666\n",
      "train: step: 357, loss: 0.21408440172672272, acc: 0.90625, recall: 0.8888888888888888, precision: 0.9180327868852459, f_beta: 0.9032258064516128\n",
      "train: step: 358, loss: 0.3550856411457062, acc: 0.8125, recall: 0.8275862068965517, precision: 0.7741935483870968, f_beta: 0.7999999999999999\n",
      "train: step: 359, loss: 0.28188854455947876, acc: 0.8828125, recall: 0.9242424242424242, precision: 0.8591549295774648, f_beta: 0.8905109489051095\n",
      "train: step: 360, loss: 0.2704583406448364, acc: 0.90625, recall: 0.8666666666666667, precision: 0.9285714285714286, f_beta: 0.896551724137931\n",
      "train: step: 361, loss: 0.2570444941520691, acc: 0.9140625, recall: 0.9154929577464789, precision: 0.9285714285714286, f_beta: 0.921985815602837\n",
      "train: step: 362, loss: 0.31700173020362854, acc: 0.875, recall: 0.8412698412698413, precision: 0.8983050847457628, f_beta: 0.8688524590163935\n",
      "train: step: 363, loss: 0.29679906368255615, acc: 0.9140625, recall: 0.8695652173913043, precision: 0.967741935483871, f_beta: 0.9160305343511451\n",
      "train: step: 364, loss: 0.27625006437301636, acc: 0.90625, recall: 0.921875, precision: 0.8939393939393939, f_beta: 0.9076923076923077\n",
      "train: step: 365, loss: 0.4319104254245758, acc: 0.859375, recall: 0.8450704225352113, precision: 0.8955223880597015, f_beta: 0.8695652173913043\n",
      "train: step: 366, loss: 0.3263029158115387, acc: 0.8515625, recall: 0.9056603773584906, precision: 0.7741935483870968, f_beta: 0.8347826086956522\n",
      "train: step: 367, loss: 0.3341628909111023, acc: 0.8984375, recall: 0.9838709677419355, precision: 0.8356164383561644, f_beta: 0.9037037037037038\n",
      "train: step: 368, loss: 0.2244272083044052, acc: 0.921875, recall: 0.9354838709677419, precision: 0.90625, f_beta: 0.9206349206349206\n",
      "train: step: 369, loss: 0.26628556847572327, acc: 0.8984375, recall: 0.90625, precision: 0.8923076923076924, f_beta: 0.8992248062015504\n",
      "train: step: 370, loss: 0.28760263323783875, acc: 0.875, recall: 0.8636363636363636, precision: 0.890625, f_beta: 0.8769230769230768\n",
      "train: step: 371, loss: 0.24881179630756378, acc: 0.8828125, recall: 0.8448275862068966, precision: 0.8909090909090909, f_beta: 0.8672566371681416\n",
      "train: step: 372, loss: 0.2597725987434387, acc: 0.90625, recall: 0.9365079365079365, precision: 0.8805970149253731, f_beta: 0.9076923076923077\n",
      "train: step: 373, loss: 0.30062997341156006, acc: 0.890625, recall: 0.8571428571428571, precision: 0.8888888888888888, f_beta: 0.8727272727272727\n",
      "train: step: 374, loss: 0.16881558299064636, acc: 0.9609375, recall: 0.9672131147540983, precision: 0.9516129032258065, f_beta: 0.959349593495935\n",
      "train: step: 375, loss: 0.2513655126094818, acc: 0.890625, recall: 0.9193548387096774, precision: 0.8636363636363636, f_beta: 0.890625\n",
      "train: step: 376, loss: 0.27739518880844116, acc: 0.8828125, recall: 0.9122807017543859, precision: 0.8387096774193549, f_beta: 0.8739495798319329\n",
      "train: step: 377, loss: 0.2403489500284195, acc: 0.9296875, recall: 0.95, precision: 0.9047619047619048, f_beta: 0.9268292682926829\n",
      "train: step: 378, loss: 0.33058056235313416, acc: 0.84375, recall: 0.8571428571428571, precision: 0.8571428571428571, f_beta: 0.8571428571428571\n",
      "train: step: 379, loss: 0.3862651586532593, acc: 0.8359375, recall: 0.7419354838709677, precision: 0.9019607843137255, f_beta: 0.8141592920353982\n",
      "train: step: 380, loss: 0.23164474964141846, acc: 0.90625, recall: 0.8709677419354839, precision: 0.9310344827586207, f_beta: 0.9\n",
      "train: step: 381, loss: 0.3563639521598816, acc: 0.84375, recall: 0.8392857142857143, precision: 0.8103448275862069, f_beta: 0.8245614035087718\n",
      "train: step: 382, loss: 0.2723490595817566, acc: 0.890625, recall: 0.9230769230769231, precision: 0.8695652173913043, f_beta: 0.8955223880597014\n",
      "train: step: 383, loss: 0.3482637405395508, acc: 0.8359375, recall: 0.8695652173913043, precision: 0.8333333333333334, f_beta: 0.851063829787234\n",
      "train: step: 384, loss: 0.17173504829406738, acc: 0.96875, recall: 0.9846153846153847, precision: 0.9552238805970149, f_beta: 0.9696969696969696\n",
      "train: step: 385, loss: 0.2483559399843216, acc: 0.90625, recall: 0.9482758620689655, precision: 0.859375, f_beta: 0.9016393442622951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 386, loss: 0.26898425817489624, acc: 0.8671875, recall: 0.8571428571428571, precision: 0.8421052631578947, f_beta: 0.8495575221238938\n",
      "train: step: 387, loss: 0.28869572281837463, acc: 0.90625, recall: 0.8923076923076924, precision: 0.9206349206349206, f_beta: 0.90625\n",
      "train: step: 388, loss: 0.2387533038854599, acc: 0.8984375, recall: 0.8870967741935484, precision: 0.9016393442622951, f_beta: 0.8943089430894309\n",
      "train: step: 389, loss: 0.2142930030822754, acc: 0.921875, recall: 0.9166666666666666, precision: 0.9166666666666666, f_beta: 0.9166666666666666\n",
      "train: step: 390, loss: 0.2843407690525055, acc: 0.8671875, recall: 0.84, precision: 0.9264705882352942, f_beta: 0.881118881118881\n",
      "train: step: 391, loss: 0.29578834772109985, acc: 0.8984375, recall: 0.8923076923076924, precision: 0.90625, f_beta: 0.8992248062015504\n",
      "train: step: 392, loss: 0.2953954339027405, acc: 0.90625, recall: 0.9682539682539683, precision: 0.8591549295774648, f_beta: 0.9104477611940299\n",
      "train: step: 393, loss: 0.3096431791782379, acc: 0.890625, recall: 0.8923076923076924, precision: 0.8923076923076924, f_beta: 0.8923076923076924\n",
      "train: step: 394, loss: 0.27278515696525574, acc: 0.8671875, recall: 0.9245283018867925, precision: 0.7903225806451613, f_beta: 0.8521739130434782\n",
      "train: step: 395, loss: 0.3077760934829712, acc: 0.84375, recall: 0.9, precision: 0.8289473684210527, f_beta: 0.8630136986301371\n",
      "train: step: 396, loss: 0.24809715151786804, acc: 0.9453125, recall: 0.9672131147540983, precision: 0.921875, f_beta: 0.944\n",
      "train: step: 397, loss: 0.29218798875808716, acc: 0.875, recall: 0.8333333333333334, precision: 0.8928571428571429, f_beta: 0.8620689655172413\n",
      "train: step: 398, loss: 0.26766857504844666, acc: 0.890625, recall: 0.8970588235294118, precision: 0.8970588235294118, f_beta: 0.8970588235294118\n",
      "train: step: 399, loss: 0.3076942563056946, acc: 0.859375, recall: 0.8309859154929577, precision: 0.9076923076923077, f_beta: 0.8676470588235293\n",
      "train: step: 400, loss: 0.2566757798194885, acc: 0.90625, recall: 0.8970588235294118, precision: 0.9242424242424242, f_beta: 0.9104477611940298\n",
      "\n",
      "Evaluation:\n",
      "2019-08-14T10:53:11.200996, step: 400, loss: 0.3249099300457881, acc: 0.8623798076923077,precision: 0.8773024393125228, recall: 0.853888560034219, f_beta: 0.8648470308110876\n",
      "Saved model checkpoint to model/textCNN/model/my-model-400\n",
      "\n",
      "train: step: 401, loss: 0.2148062139749527, acc: 0.9296875, recall: 0.9253731343283582, precision: 0.9393939393939394, f_beta: 0.9323308270676692\n",
      "train: step: 402, loss: 0.31640011072158813, acc: 0.8515625, recall: 0.9322033898305084, precision: 0.7857142857142857, f_beta: 0.8527131782945736\n",
      "train: step: 403, loss: 0.30271798372268677, acc: 0.84375, recall: 0.9295774647887324, precision: 0.8148148148148148, f_beta: 0.8684210526315789\n",
      "train: step: 404, loss: 0.2582961916923523, acc: 0.875, recall: 0.8918918918918919, precision: 0.8918918918918919, f_beta: 0.8918918918918919\n",
      "train: step: 405, loss: 0.3322296738624573, acc: 0.8515625, recall: 0.8928571428571429, precision: 0.7936507936507936, f_beta: 0.8403361344537815\n",
      "train: step: 406, loss: 0.274269163608551, acc: 0.8671875, recall: 0.9041095890410958, precision: 0.868421052631579, f_beta: 0.8859060402684564\n",
      "train: step: 407, loss: 0.3303348422050476, acc: 0.84375, recall: 0.8153846153846154, precision: 0.8688524590163934, f_beta: 0.8412698412698412\n",
      "train: step: 408, loss: 0.2020845264196396, acc: 0.9296875, recall: 0.9054054054054054, precision: 0.9710144927536232, f_beta: 0.937062937062937\n",
      "train: step: 409, loss: 0.2963107228279114, acc: 0.8515625, recall: 0.8620689655172413, precision: 0.819672131147541, f_beta: 0.8403361344537814\n",
      "train: step: 410, loss: 0.28403177857398987, acc: 0.875, recall: 0.8833333333333333, precision: 0.8548387096774194, f_beta: 0.8688524590163934\n",
      "train: step: 411, loss: 0.2862907350063324, acc: 0.875, recall: 0.8513513513513513, precision: 0.9264705882352942, f_beta: 0.8873239436619719\n",
      "train: step: 412, loss: 0.2189846634864807, acc: 0.921875, recall: 0.9295774647887324, precision: 0.9295774647887324, f_beta: 0.9295774647887324\n",
      "train: step: 413, loss: 0.35147789120674133, acc: 0.8515625, recall: 0.8846153846153846, precision: 0.8734177215189873, f_beta: 0.8789808917197452\n",
      "train: step: 414, loss: 0.2912086844444275, acc: 0.8828125, recall: 0.9682539682539683, precision: 0.8243243243243243, f_beta: 0.8905109489051095\n",
      "train: step: 415, loss: 0.25408536195755005, acc: 0.8984375, recall: 0.96875, precision: 0.8493150684931506, f_beta: 0.9051094890510949\n",
      "train: step: 416, loss: 0.2946392595767975, acc: 0.8671875, recall: 0.9259259259259259, precision: 0.7936507936507936, f_beta: 0.8547008547008547\n",
      "train: step: 417, loss: 0.2537569999694824, acc: 0.890625, recall: 0.8985507246376812, precision: 0.8985507246376812, f_beta: 0.8985507246376812\n",
      "train: step: 418, loss: 0.2520401179790497, acc: 0.9140625, recall: 0.9253731343283582, precision: 0.9117647058823529, f_beta: 0.9185185185185185\n",
      "train: step: 419, loss: 0.3783249258995056, acc: 0.859375, recall: 0.78125, precision: 0.9259259259259259, f_beta: 0.847457627118644\n",
      "train: step: 420, loss: 0.2533505856990814, acc: 0.8984375, recall: 0.8461538461538461, precision: 0.9482758620689655, f_beta: 0.894308943089431\n",
      "train: step: 421, loss: 0.2667180895805359, acc: 0.875, recall: 0.835820895522388, precision: 0.9180327868852459, f_beta: 0.875\n",
      "train: step: 422, loss: 0.3079613149166107, acc: 0.8828125, recall: 0.8695652173913043, precision: 0.9090909090909091, f_beta: 0.888888888888889\n",
      "train: step: 423, loss: 0.2961733341217041, acc: 0.90625, recall: 0.9298245614035088, precision: 0.8688524590163934, f_beta: 0.8983050847457625\n",
      "train: step: 424, loss: 0.31278952956199646, acc: 0.8828125, recall: 0.9538461538461539, precision: 0.8378378378378378, f_beta: 0.8920863309352518\n",
      "train: step: 425, loss: 0.3042775094509125, acc: 0.8828125, recall: 0.9649122807017544, precision: 0.8088235294117647, f_beta: 0.88\n",
      "train: step: 426, loss: 0.26073071360588074, acc: 0.8984375, recall: 0.9642857142857143, precision: 0.8307692307692308, f_beta: 0.8925619834710744\n",
      "train: step: 427, loss: 0.3008575737476349, acc: 0.8828125, recall: 0.9047619047619048, precision: 0.8636363636363636, f_beta: 0.8837209302325582\n",
      "train: step: 428, loss: 0.27989062666893005, acc: 0.890625, recall: 0.8688524590163934, precision: 0.8983050847457628, f_beta: 0.8833333333333333\n",
      "train: step: 429, loss: 0.3029130697250366, acc: 0.8828125, recall: 0.803921568627451, precision: 0.8913043478260869, f_beta: 0.845360824742268\n",
      "train: step: 430, loss: 0.2399587333202362, acc: 0.8984375, recall: 0.8181818181818182, precision: 0.9818181818181818, f_beta: 0.8925619834710744\n",
      "train: step: 431, loss: 0.3088151812553406, acc: 0.8671875, recall: 0.7910447761194029, precision: 0.9464285714285714, f_beta: 0.8617886178861788\n",
      "train: step: 432, loss: 0.18967443704605103, acc: 0.9375, recall: 0.9333333333333333, precision: 0.9333333333333333, f_beta: 0.9333333333333333\n",
      "train: step: 433, loss: 0.31162816286087036, acc: 0.8828125, recall: 0.8970588235294118, precision: 0.8840579710144928, f_beta: 0.8905109489051095\n",
      "train: step: 434, loss: 0.2669399380683899, acc: 0.90625, recall: 0.896551724137931, precision: 0.896551724137931, f_beta: 0.896551724137931\n",
      "train: step: 435, loss: 0.2419058233499527, acc: 0.8828125, recall: 0.8939393939393939, precision: 0.8805970149253731, f_beta: 0.887218045112782\n",
      "train: step: 436, loss: 0.3037063479423523, acc: 0.859375, recall: 0.9047619047619048, precision: 0.8260869565217391, f_beta: 0.8636363636363636\n",
      "train: step: 437, loss: 0.2863011360168457, acc: 0.890625, recall: 0.9571428571428572, precision: 0.8589743589743589, f_beta: 0.9054054054054054\n",
      "train: step: 438, loss: 0.26320239901542664, acc: 0.890625, recall: 0.9206349206349206, precision: 0.8656716417910447, f_beta: 0.8923076923076922\n",
      "train: step: 439, loss: 0.28509387373924255, acc: 0.875, recall: 0.9354838709677419, precision: 0.8285714285714286, f_beta: 0.8787878787878788\n",
      "train: step: 440, loss: 0.2637816369533539, acc: 0.8671875, recall: 0.9090909090909091, precision: 0.8450704225352113, f_beta: 0.8759124087591241\n",
      "train: step: 441, loss: 0.27895933389663696, acc: 0.8828125, recall: 0.8947368421052632, precision: 0.85, f_beta: 0.8717948717948718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 442, loss: 0.27401793003082275, acc: 0.875, recall: 0.9076923076923077, precision: 0.855072463768116, f_beta: 0.8805970149253731\n",
      "train: step: 443, loss: 0.24764017760753632, acc: 0.8984375, recall: 0.8382352941176471, precision: 0.9661016949152542, f_beta: 0.8976377952755905\n",
      "train: step: 444, loss: 0.2978249788284302, acc: 0.890625, recall: 0.8933333333333333, precision: 0.9178082191780822, f_beta: 0.9054054054054055\n",
      "train: step: 445, loss: 0.3167170584201813, acc: 0.875, recall: 0.8387096774193549, precision: 0.896551724137931, f_beta: 0.8666666666666666\n",
      "train: step: 446, loss: 0.2641584575176239, acc: 0.8671875, recall: 0.8367346938775511, precision: 0.82, f_beta: 0.8282828282828283\n",
      "train: step: 447, loss: 0.294977605342865, acc: 0.8984375, recall: 0.8970588235294118, precision: 0.9104477611940298, f_beta: 0.9037037037037037\n",
      "train: step: 448, loss: 0.2406330555677414, acc: 0.90625, recall: 0.9253731343283582, precision: 0.8985507246376812, f_beta: 0.9117647058823529\n",
      "train: step: 449, loss: 0.24687866866588593, acc: 0.890625, recall: 0.92, precision: 0.8961038961038961, f_beta: 0.9078947368421053\n",
      "train: step: 450, loss: 0.2546091377735138, acc: 0.8984375, recall: 0.921875, precision: 0.8805970149253731, f_beta: 0.9007633587786259\n",
      "train: step: 451, loss: 0.25061148405075073, acc: 0.90625, recall: 0.9384615384615385, precision: 0.8840579710144928, f_beta: 0.9104477611940298\n",
      "train: step: 452, loss: 0.19455233216285706, acc: 0.9140625, recall: 0.8688524590163934, precision: 0.9464285714285714, f_beta: 0.9059829059829059\n",
      "train: step: 453, loss: 0.22596967220306396, acc: 0.8984375, recall: 0.9014084507042254, precision: 0.9142857142857143, f_beta: 0.9078014184397163\n",
      "train: step: 454, loss: 0.24172574281692505, acc: 0.90625, recall: 0.9295774647887324, precision: 0.9041095890410958, f_beta: 0.9166666666666666\n",
      "train: step: 455, loss: 0.18716168403625488, acc: 0.9296875, recall: 0.9466666666666667, precision: 0.9342105263157895, f_beta: 0.9403973509933775\n",
      "train: step: 456, loss: 0.32598772644996643, acc: 0.828125, recall: 0.8909090909090909, precision: 0.7538461538461538, f_beta: 0.8166666666666667\n",
      "train: step: 457, loss: 0.2541365623474121, acc: 0.890625, recall: 0.9117647058823529, precision: 0.8857142857142857, f_beta: 0.8985507246376812\n",
      "train: step: 458, loss: 0.2211703360080719, acc: 0.921875, recall: 0.8695652173913043, precision: 0.9090909090909091, f_beta: 0.888888888888889\n",
      "train: step: 459, loss: 0.25419652462005615, acc: 0.8828125, recall: 0.859375, precision: 0.9016393442622951, f_beta: 0.88\n",
      "train: step: 460, loss: 0.26837459206581116, acc: 0.8671875, recall: 0.8405797101449275, precision: 0.90625, f_beta: 0.8721804511278196\n",
      "train: step: 461, loss: 0.2539927661418915, acc: 0.8671875, recall: 0.8571428571428571, precision: 0.8709677419354839, f_beta: 0.864\n",
      "train: step: 462, loss: 0.24519787728786469, acc: 0.9140625, recall: 0.8928571428571429, precision: 0.9090909090909091, f_beta: 0.9009009009009009\n",
      "train: step: 463, loss: 0.25717052817344666, acc: 0.8984375, recall: 0.9193548387096774, precision: 0.8769230769230769, f_beta: 0.8976377952755904\n",
      "train: step: 464, loss: 0.2462024688720703, acc: 0.8828125, recall: 0.9642857142857143, precision: 0.8059701492537313, f_beta: 0.8780487804878049\n",
      "train: step: 465, loss: 0.32051095366477966, acc: 0.859375, recall: 0.9032258064516129, precision: 0.8235294117647058, f_beta: 0.8615384615384616\n",
      "train: step: 466, loss: 0.19708305597305298, acc: 0.9296875, recall: 0.9508196721311475, precision: 0.90625, f_beta: 0.9279999999999999\n",
      "train: step: 467, loss: 0.3257567286491394, acc: 0.890625, recall: 0.8787878787878788, precision: 0.90625, f_beta: 0.8923076923076922\n",
      "train: step: 468, loss: 0.28323787450790405, acc: 0.8515625, recall: 0.7857142857142857, precision: 0.9322033898305084, f_beta: 0.8527131782945736\n",
      "start training model\n",
      "train: step: 469, loss: 0.23976221680641174, acc: 0.90625, recall: 0.8985507246376812, precision: 0.9253731343283582, f_beta: 0.9117647058823529\n",
      "train: step: 470, loss: 0.19698233902454376, acc: 0.921875, recall: 0.9322033898305084, precision: 0.9016393442622951, f_beta: 0.9166666666666666\n",
      "train: step: 471, loss: 0.20661038160324097, acc: 0.9296875, recall: 0.9583333333333334, precision: 0.92, f_beta: 0.9387755102040817\n",
      "train: step: 472, loss: 0.19522172212600708, acc: 0.921875, recall: 0.9565217391304348, precision: 0.9041095890410958, f_beta: 0.9295774647887325\n",
      "train: step: 473, loss: 0.18971037864685059, acc: 0.9375, recall: 0.9032258064516129, precision: 0.9655172413793104, f_beta: 0.9333333333333333\n",
      "train: step: 474, loss: 0.16625462472438812, acc: 0.9453125, recall: 0.9482758620689655, precision: 0.9322033898305084, f_beta: 0.94017094017094\n",
      "train: step: 475, loss: 0.20825056731700897, acc: 0.9140625, recall: 0.9594594594594594, precision: 0.8987341772151899, f_beta: 0.9281045751633986\n",
      "train: step: 476, loss: 0.20072093605995178, acc: 0.921875, recall: 0.9701492537313433, precision: 0.8904109589041096, f_beta: 0.9285714285714287\n",
      "train: step: 477, loss: 0.18273073434829712, acc: 0.9140625, recall: 0.9193548387096774, precision: 0.9047619047619048, f_beta: 0.912\n",
      "train: step: 478, loss: 0.26082170009613037, acc: 0.8984375, recall: 0.9016393442622951, precision: 0.8870967741935484, f_beta: 0.8943089430894309\n",
      "train: step: 479, loss: 0.1390739381313324, acc: 0.9609375, recall: 0.95, precision: 0.9661016949152542, f_beta: 0.957983193277311\n",
      "train: step: 480, loss: 0.2526654601097107, acc: 0.890625, recall: 0.9, precision: 0.9, f_beta: 0.9\n",
      "train: step: 481, loss: 0.21227861940860748, acc: 0.9140625, recall: 0.8676470588235294, precision: 0.9672131147540983, f_beta: 0.9147286821705426\n",
      "train: step: 482, loss: 0.23810654878616333, acc: 0.9140625, recall: 0.8360655737704918, precision: 0.9807692307692307, f_beta: 0.9026548672566372\n",
      "train: step: 483, loss: 0.1988140344619751, acc: 0.9375, recall: 0.9076923076923077, precision: 0.9672131147540983, f_beta: 0.9365079365079365\n",
      "train: step: 484, loss: 0.24319887161254883, acc: 0.9296875, recall: 0.8985507246376812, precision: 0.96875, f_beta: 0.9323308270676692\n",
      "train: step: 485, loss: 0.21157750487327576, acc: 0.921875, recall: 0.9836065573770492, precision: 0.8695652173913043, f_beta: 0.923076923076923\n",
      "train: step: 486, loss: 0.2066052258014679, acc: 0.9453125, recall: 1.0, precision: 0.8985507246376812, f_beta: 0.9465648854961832\n",
      "train: step: 487, loss: 0.20719721913337708, acc: 0.9296875, recall: 0.9859154929577465, precision: 0.8974358974358975, f_beta: 0.9395973154362417\n",
      "train: step: 488, loss: 0.2276337444782257, acc: 0.90625, recall: 0.9655172413793104, precision: 0.8484848484848485, f_beta: 0.9032258064516129\n",
      "train: step: 489, loss: 0.23936143517494202, acc: 0.9140625, recall: 0.896551724137931, precision: 0.9122807017543859, f_beta: 0.9043478260869565\n",
      "train: step: 490, loss: 0.2439623922109604, acc: 0.921875, recall: 0.9384615384615385, precision: 0.9104477611940298, f_beta: 0.9242424242424243\n",
      "train: step: 491, loss: 0.1836710423231125, acc: 0.953125, recall: 0.9545454545454546, precision: 0.9545454545454546, f_beta: 0.9545454545454546\n",
      "train: step: 492, loss: 0.2072346955537796, acc: 0.9140625, recall: 0.8636363636363636, precision: 0.9661016949152542, f_beta: 0.912\n",
      "train: step: 493, loss: 0.15845456719398499, acc: 0.9453125, recall: 0.9166666666666666, precision: 0.9649122807017544, f_beta: 0.9401709401709402\n",
      "train: step: 494, loss: 0.20250873267650604, acc: 0.953125, recall: 0.9242424242424242, precision: 0.9838709677419355, f_beta: 0.9531249999999999\n",
      "train: step: 495, loss: 0.1740117073059082, acc: 0.9453125, recall: 0.9259259259259259, precision: 0.9433962264150944, f_beta: 0.9345794392523364\n",
      "train: step: 496, loss: 0.22810184955596924, acc: 0.9140625, recall: 0.9285714285714286, precision: 0.8813559322033898, f_beta: 0.9043478260869564\n",
      "train: step: 497, loss: 0.2799621820449829, acc: 0.8828125, recall: 0.9, precision: 0.8571428571428571, f_beta: 0.8780487804878048\n",
      "train: step: 498, loss: 0.2503048777580261, acc: 0.9140625, recall: 0.9137931034482759, precision: 0.8983050847457628, f_beta: 0.9059829059829059\n",
      "train: step: 499, loss: 0.16509473323822021, acc: 0.921875, recall: 0.9272727272727272, precision: 0.8947368421052632, f_beta: 0.9107142857142856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 500, loss: 0.13101021945476532, acc: 0.9609375, recall: 0.9846153846153847, precision: 0.9411764705882353, f_beta: 0.962406015037594\n",
      "\n",
      "Evaluation:\n",
      "2019-08-14T10:53:34.235877, step: 500, loss: 0.31753104352034056, acc: 0.8645833333333334,precision: 0.8675318073813642, recall: 0.8640987333863682, f_beta: 0.8651461554802722\n",
      "Saved model checkpoint to model/textCNN/model/my-model-500\n",
      "\n",
      "train: step: 501, loss: 0.2049303948879242, acc: 0.9140625, recall: 0.9464285714285714, precision: 0.8688524590163934, f_beta: 0.9059829059829059\n",
      "train: step: 502, loss: 0.21739059686660767, acc: 0.9140625, recall: 0.8688524590163934, precision: 0.9464285714285714, f_beta: 0.9059829059829059\n",
      "train: step: 503, loss: 0.17508545517921448, acc: 0.9375, recall: 0.9230769230769231, precision: 0.972972972972973, f_beta: 0.9473684210526315\n",
      "train: step: 504, loss: 0.2405908703804016, acc: 0.90625, recall: 0.875, precision: 0.9545454545454546, f_beta: 0.9130434782608695\n",
      "train: step: 505, loss: 0.2404070943593979, acc: 0.890625, recall: 0.8813559322033898, precision: 0.8813559322033898, f_beta: 0.8813559322033898\n",
      "train: step: 506, loss: 0.19245463609695435, acc: 0.90625, recall: 0.9180327868852459, precision: 0.8888888888888888, f_beta: 0.9032258064516128\n",
      "train: step: 507, loss: 0.16401250660419464, acc: 0.953125, recall: 0.9682539682539683, precision: 0.9384615384615385, f_beta: 0.953125\n",
      "train: step: 508, loss: 0.20576722919940948, acc: 0.9140625, recall: 0.9459459459459459, precision: 0.9090909090909091, f_beta: 0.9271523178807948\n",
      "train: step: 509, loss: 0.2135917842388153, acc: 0.9296875, recall: 0.9642857142857143, precision: 0.8852459016393442, f_beta: 0.923076923076923\n",
      "train: step: 510, loss: 0.23794768750667572, acc: 0.890625, recall: 0.8983050847457628, precision: 0.8688524590163934, f_beta: 0.8833333333333333\n",
      "train: step: 511, loss: 0.20314304530620575, acc: 0.921875, recall: 0.8823529411764706, precision: 0.967741935483871, f_beta: 0.923076923076923\n",
      "train: step: 512, loss: 0.18542265892028809, acc: 0.9296875, recall: 0.8888888888888888, precision: 0.9846153846153847, f_beta: 0.9343065693430657\n",
      "train: step: 513, loss: 0.22926290333271027, acc: 0.9140625, recall: 0.9047619047619048, precision: 0.9193548387096774, f_beta: 0.912\n",
      "train: step: 514, loss: 0.22989319264888763, acc: 0.8984375, recall: 0.8985507246376812, precision: 0.9117647058823529, f_beta: 0.9051094890510949\n",
      "train: step: 515, loss: 0.18576814234256744, acc: 0.953125, recall: 0.9852941176470589, precision: 0.9305555555555556, f_beta: 0.9571428571428572\n",
      "train: step: 516, loss: 0.1416650414466858, acc: 0.9765625, recall: 0.9701492537313433, precision: 0.9848484848484849, f_beta: 0.9774436090225564\n",
      "train: step: 517, loss: 0.20055976510047913, acc: 0.9140625, recall: 0.9444444444444444, precision: 0.9066666666666666, f_beta: 0.9251700680272109\n",
      "train: step: 518, loss: 0.20841579139232635, acc: 0.8984375, recall: 0.9814814814814815, precision: 0.8153846153846154, f_beta: 0.8907563025210083\n",
      "train: step: 519, loss: 0.23026910424232483, acc: 0.921875, recall: 0.9454545454545454, precision: 0.8813559322033898, f_beta: 0.9122807017543859\n",
      "train: step: 520, loss: 0.17321240901947021, acc: 0.9296875, recall: 0.9152542372881356, precision: 0.9310344827586207, f_beta: 0.923076923076923\n",
      "train: step: 521, loss: 0.17177924513816833, acc: 0.9375, recall: 0.8545454545454545, precision: 1.0, f_beta: 0.9215686274509803\n",
      "train: step: 522, loss: 0.2240249216556549, acc: 0.8984375, recall: 0.8591549295774648, precision: 0.953125, f_beta: 0.9037037037037037\n",
      "train: step: 523, loss: 0.17289772629737854, acc: 0.9453125, recall: 0.9230769230769231, precision: 0.967741935483871, f_beta: 0.9448818897637796\n",
      "train: step: 524, loss: 0.16797798871994019, acc: 0.953125, recall: 0.967741935483871, precision: 0.9375, f_beta: 0.9523809523809523\n",
      "train: step: 525, loss: 0.14883562922477722, acc: 0.953125, recall: 0.9384615384615385, precision: 0.9682539682539683, f_beta: 0.953125\n",
      "train: step: 526, loss: 0.19950294494628906, acc: 0.9140625, recall: 0.9298245614035088, precision: 0.8833333333333333, f_beta: 0.905982905982906\n",
      "train: step: 527, loss: 0.1898583024740219, acc: 0.921875, recall: 0.9523809523809523, precision: 0.8955223880597015, f_beta: 0.923076923076923\n",
      "train: step: 528, loss: 0.19965434074401855, acc: 0.9140625, recall: 0.921875, precision: 0.9076923076923077, f_beta: 0.9147286821705427\n",
      "train: step: 529, loss: 0.1689065843820572, acc: 0.9453125, recall: 0.967741935483871, precision: 0.9230769230769231, f_beta: 0.9448818897637796\n",
      "train: step: 530, loss: 0.2352789044380188, acc: 0.8984375, recall: 0.9655172413793104, precision: 0.835820895522388, f_beta: 0.896\n",
      "train: step: 531, loss: 0.1912672519683838, acc: 0.9140625, recall: 0.9516129032258065, precision: 0.8805970149253731, f_beta: 0.9147286821705426\n",
      "train: step: 532, loss: 0.22268924117088318, acc: 0.8828125, recall: 0.8484848484848485, precision: 0.9180327868852459, f_beta: 0.8818897637795275\n",
      "train: step: 533, loss: 0.2122153639793396, acc: 0.9453125, recall: 0.9016393442622951, precision: 0.9821428571428571, f_beta: 0.9401709401709402\n",
      "train: step: 534, loss: 0.18255828320980072, acc: 0.9375, recall: 0.8888888888888888, precision: 0.96, f_beta: 0.923076923076923\n",
      "train: step: 535, loss: 0.25650134682655334, acc: 0.8984375, recall: 0.8888888888888888, precision: 0.9032258064516129, f_beta: 0.8959999999999999\n",
      "train: step: 536, loss: 0.20518168807029724, acc: 0.90625, recall: 0.9444444444444444, precision: 0.85, f_beta: 0.8947368421052632\n",
      "train: step: 537, loss: 0.14333954453468323, acc: 0.9453125, recall: 0.9452054794520548, precision: 0.9583333333333334, f_beta: 0.9517241379310345\n",
      "train: step: 538, loss: 0.18926823139190674, acc: 0.9453125, recall: 0.9655172413793104, precision: 0.9180327868852459, f_beta: 0.9411764705882353\n",
      "train: step: 539, loss: 0.23097245395183563, acc: 0.9140625, recall: 0.9152542372881356, precision: 0.9, f_beta: 0.9075630252100839\n",
      "train: step: 540, loss: 0.13849283754825592, acc: 0.9765625, recall: 1.0, precision: 0.96, f_beta: 0.9795918367346939\n",
      "train: step: 541, loss: 0.1833200603723526, acc: 0.953125, recall: 0.9375, precision: 0.967741935483871, f_beta: 0.9523809523809523\n",
      "train: step: 542, loss: 0.14899079501628876, acc: 0.953125, recall: 0.9538461538461539, precision: 0.9538461538461539, f_beta: 0.9538461538461539\n",
      "train: step: 543, loss: 0.17688217759132385, acc: 0.9375, recall: 0.9850746268656716, precision: 0.9041095890410958, f_beta: 0.9428571428571428\n",
      "train: step: 544, loss: 0.22880974411964417, acc: 0.9296875, recall: 0.9375, precision: 0.9230769230769231, f_beta: 0.9302325581395349\n",
      "train: step: 545, loss: 0.2111828625202179, acc: 0.9453125, recall: 0.9242424242424242, precision: 0.9682539682539683, f_beta: 0.9457364341085271\n",
      "train: step: 546, loss: 0.15241727232933044, acc: 0.9453125, recall: 0.9558823529411765, precision: 0.9420289855072463, f_beta: 0.9489051094890512\n",
      "train: step: 547, loss: 0.30092114210128784, acc: 0.8984375, recall: 0.8636363636363636, precision: 0.9344262295081968, f_beta: 0.8976377952755905\n",
      "train: step: 548, loss: 0.21454091370105743, acc: 0.9140625, recall: 0.9491525423728814, precision: 0.875, f_beta: 0.9105691056910569\n",
      "train: step: 549, loss: 0.2169586420059204, acc: 0.921875, recall: 0.9206349206349206, precision: 0.9206349206349206, f_beta: 0.9206349206349206\n",
      "train: step: 550, loss: 0.19731949269771576, acc: 0.9140625, recall: 0.9420289855072463, precision: 0.9027777777777778, f_beta: 0.9219858156028369\n",
      "train: step: 551, loss: 0.22781401872634888, acc: 0.90625, recall: 0.9230769230769231, precision: 0.8955223880597015, f_beta: 0.9090909090909091\n",
      "train: step: 552, loss: 0.1587749570608139, acc: 0.9453125, recall: 0.9393939393939394, precision: 0.9538461538461539, f_beta: 0.9465648854961831\n",
      "train: step: 553, loss: 0.25880151987075806, acc: 0.890625, recall: 0.9, precision: 0.9230769230769231, f_beta: 0.9113924050632911\n",
      "train: step: 554, loss: 0.24410143494606018, acc: 0.875, recall: 0.864406779661017, precision: 0.864406779661017, f_beta: 0.864406779661017\n",
      "train: step: 555, loss: 0.2544713020324707, acc: 0.8984375, recall: 0.953125, precision: 0.8591549295774648, f_beta: 0.9037037037037037\n",
      "train: step: 556, loss: 0.16509178280830383, acc: 0.9375, recall: 0.927536231884058, precision: 0.9552238805970149, f_beta: 0.9411764705882353\n",
      "train: step: 557, loss: 0.2505797743797302, acc: 0.8984375, recall: 0.9090909090909091, precision: 0.8620689655172413, f_beta: 0.8849557522123893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 558, loss: 0.14913547039031982, acc: 0.9609375, recall: 0.953125, precision: 0.9682539682539683, f_beta: 0.9606299212598425\n",
      "train: step: 559, loss: 0.20865367352962494, acc: 0.9140625, recall: 0.9253731343283582, precision: 0.9117647058823529, f_beta: 0.9185185185185185\n",
      "train: step: 560, loss: 0.22396451234817505, acc: 0.9296875, recall: 0.9365079365079365, precision: 0.921875, f_beta: 0.9291338582677166\n",
      "train: step: 561, loss: 0.21570096909999847, acc: 0.9375, recall: 0.9090909090909091, precision: 0.9433962264150944, f_beta: 0.9259259259259259\n",
      "train: step: 562, loss: 0.19259566068649292, acc: 0.9296875, recall: 0.9104477611940298, precision: 0.953125, f_beta: 0.931297709923664\n",
      "train: step: 563, loss: 0.24397164583206177, acc: 0.8984375, recall: 0.8688524590163934, precision: 0.9137931034482759, f_beta: 0.8907563025210085\n",
      "train: step: 564, loss: 0.1454567313194275, acc: 0.9609375, recall: 0.9322033898305084, precision: 0.9821428571428571, f_beta: 0.9565217391304348\n",
      "train: step: 565, loss: 0.21230751276016235, acc: 0.921875, recall: 0.9452054794520548, precision: 0.92, f_beta: 0.9324324324324323\n",
      "train: step: 566, loss: 0.17465916275978088, acc: 0.96875, recall: 0.9838709677419355, precision: 0.953125, f_beta: 0.9682539682539683\n",
      "train: step: 567, loss: 0.14644110202789307, acc: 0.96875, recall: 0.9565217391304348, precision: 0.9850746268656716, f_beta: 0.9705882352941176\n",
      "train: step: 568, loss: 0.29299700260162354, acc: 0.8984375, recall: 0.9433962264150944, precision: 0.8333333333333334, f_beta: 0.8849557522123894\n",
      "train: step: 569, loss: 0.2808888554573059, acc: 0.859375, recall: 0.9019607843137255, precision: 0.7796610169491526, f_beta: 0.8363636363636364\n",
      "train: step: 570, loss: 0.15395618975162506, acc: 0.96875, recall: 0.9710144927536232, precision: 0.9710144927536232, f_beta: 0.9710144927536232\n",
      "train: step: 571, loss: 0.3116595149040222, acc: 0.875, recall: 0.8648648648648649, precision: 0.9142857142857143, f_beta: 0.888888888888889\n",
      "train: step: 572, loss: 0.20237836241722107, acc: 0.9296875, recall: 0.9, precision: 0.9473684210526315, f_beta: 0.9230769230769231\n",
      "train: step: 573, loss: 0.15639466047286987, acc: 0.9453125, recall: 0.9154929577464789, precision: 0.9848484848484849, f_beta: 0.948905109489051\n",
      "train: step: 574, loss: 0.22010526061058044, acc: 0.921875, recall: 0.8928571428571429, precision: 0.9259259259259259, f_beta: 0.9090909090909091\n",
      "train: step: 575, loss: 0.18188270926475525, acc: 0.9296875, recall: 0.9523809523809523, precision: 0.9090909090909091, f_beta: 0.9302325581395349\n",
      "train: step: 576, loss: 0.19539472460746765, acc: 0.921875, recall: 0.9117647058823529, precision: 0.9393939393939394, f_beta: 0.9253731343283583\n",
      "train: step: 577, loss: 0.19508476555347443, acc: 0.9296875, recall: 0.9583333333333334, precision: 0.92, f_beta: 0.9387755102040817\n",
      "train: step: 578, loss: 0.1557644158601761, acc: 0.9296875, recall: 0.9672131147540983, precision: 0.8939393939393939, f_beta: 0.9291338582677166\n",
      "train: step: 579, loss: 0.1904033124446869, acc: 0.9296875, recall: 0.9264705882352942, precision: 0.9402985074626866, f_beta: 0.9333333333333335\n",
      "train: step: 580, loss: 0.24503524601459503, acc: 0.90625, recall: 0.9516129032258065, precision: 0.8676470588235294, f_beta: 0.9076923076923077\n",
      "train: step: 581, loss: 0.223397359251976, acc: 0.9140625, recall: 0.9333333333333333, precision: 0.8888888888888888, f_beta: 0.9105691056910569\n",
      "train: step: 582, loss: 0.21589753031730652, acc: 0.8828125, recall: 0.9166666666666666, precision: 0.8461538461538461, f_beta: 0.8799999999999999\n",
      "train: step: 583, loss: 0.18051409721374512, acc: 0.953125, recall: 0.9298245614035088, precision: 0.9636363636363636, f_beta: 0.9464285714285715\n",
      "train: step: 584, loss: 0.33390185236930847, acc: 0.8203125, recall: 0.6842105263157895, precision: 0.8863636363636364, f_beta: 0.7722772277227723\n",
      "train: step: 585, loss: 0.22685782611370087, acc: 0.8828125, recall: 0.8333333333333334, precision: 0.9523809523809523, f_beta: 0.888888888888889\n",
      "train: step: 586, loss: 0.19798409938812256, acc: 0.9140625, recall: 0.8771929824561403, precision: 0.9259259259259259, f_beta: 0.9009009009009009\n",
      "train: step: 587, loss: 0.24514132738113403, acc: 0.90625, recall: 0.9166666666666666, precision: 0.9166666666666666, f_beta: 0.9166666666666666\n",
      "train: step: 588, loss: 0.19438536465168, acc: 0.9140625, recall: 0.9838709677419355, precision: 0.8591549295774648, f_beta: 0.9172932330827067\n",
      "train: step: 589, loss: 0.19775912165641785, acc: 0.9296875, recall: 0.9841269841269841, precision: 0.8857142857142857, f_beta: 0.9323308270676691\n",
      "train: step: 590, loss: 0.16270074248313904, acc: 0.9453125, recall: 1.0, precision: 0.9090909090909091, f_beta: 0.9523809523809523\n",
      "train: step: 591, loss: 0.21838578581809998, acc: 0.921875, recall: 0.875, precision: 0.9423076923076923, f_beta: 0.9074074074074073\n",
      "train: step: 592, loss: 0.2378104031085968, acc: 0.9296875, recall: 0.9253731343283582, precision: 0.9393939393939394, f_beta: 0.9323308270676692\n",
      "train: step: 593, loss: 0.1739462912082672, acc: 0.9453125, recall: 0.9655172413793104, precision: 0.9180327868852459, f_beta: 0.9411764705882353\n",
      "train: step: 594, loss: 0.16444280743598938, acc: 0.9375, recall: 0.927536231884058, precision: 0.9552238805970149, f_beta: 0.9411764705882353\n",
      "train: step: 595, loss: 0.25965529680252075, acc: 0.90625, recall: 0.8615384615384616, precision: 0.9491525423728814, f_beta: 0.903225806451613\n",
      "train: step: 596, loss: 0.16632607579231262, acc: 0.96875, recall: 0.9848484848484849, precision: 0.9558823529411765, f_beta: 0.9701492537313432\n",
      "train: step: 597, loss: 0.17836397886276245, acc: 0.921875, recall: 0.8769230769230769, precision: 0.9661016949152542, f_beta: 0.9193548387096773\n",
      "train: step: 598, loss: 0.20138247311115265, acc: 0.9296875, recall: 0.94, precision: 0.8867924528301887, f_beta: 0.912621359223301\n",
      "train: step: 599, loss: 0.2703271210193634, acc: 0.9140625, recall: 0.9166666666666666, precision: 0.9295774647887324, f_beta: 0.9230769230769231\n",
      "train: step: 600, loss: 0.22821484506130219, acc: 0.9375, recall: 0.9295774647887324, precision: 0.9565217391304348, f_beta: 0.9428571428571428\n",
      "\n",
      "Evaluation:\n",
      "2019-08-14T10:53:57.075090, step: 600, loss: 0.32227605734115994, acc: 0.8621794871794872,precision: 0.9035941636166328, recall: 0.8369776792401419, f_beta: 0.8683429081166852\n",
      "WARNING:tensorflow:From /home/nicken/NLP_study/nlp_env/lib/python3.6/site-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "Saved model checkpoint to model/textCNN/model/my-model-600\n",
      "\n",
      "train: step: 601, loss: 0.16536645591259003, acc: 0.9453125, recall: 0.9848484848484849, precision: 0.9154929577464789, f_beta: 0.948905109489051\n",
      "train: step: 602, loss: 0.16978737711906433, acc: 0.9296875, recall: 0.9545454545454546, precision: 0.9130434782608695, f_beta: 0.9333333333333332\n",
      "train: step: 603, loss: 0.24543535709381104, acc: 0.875, recall: 0.8813559322033898, precision: 0.8524590163934426, f_beta: 0.8666666666666666\n",
      "train: step: 604, loss: 0.1702745258808136, acc: 0.953125, recall: 0.9206349206349206, precision: 0.9830508474576272, f_beta: 0.9508196721311476\n",
      "train: step: 605, loss: 0.20564891397953033, acc: 0.921875, recall: 0.9, precision: 0.9545454545454546, f_beta: 0.9264705882352942\n",
      "train: step: 606, loss: 0.18879203498363495, acc: 0.9296875, recall: 0.95, precision: 0.9047619047619048, f_beta: 0.9268292682926829\n",
      "train: step: 607, loss: 0.22604084014892578, acc: 0.90625, recall: 0.875, precision: 0.9333333333333333, f_beta: 0.9032258064516129\n",
      "train: step: 608, loss: 0.1950448453426361, acc: 0.9296875, recall: 0.9420289855072463, precision: 0.9285714285714286, f_beta: 0.935251798561151\n",
      "train: step: 609, loss: 0.1539955884218216, acc: 0.9609375, recall: 0.9577464788732394, precision: 0.9714285714285714, f_beta: 0.9645390070921985\n",
      "train: step: 610, loss: 0.21858251094818115, acc: 0.8984375, recall: 0.881578947368421, precision: 0.9436619718309859, f_beta: 0.9115646258503401\n",
      "train: step: 611, loss: 0.2616099715232849, acc: 0.90625, recall: 0.9402985074626866, precision: 0.8873239436619719, f_beta: 0.9130434782608696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 612, loss: 0.24100646376609802, acc: 0.9140625, recall: 0.9655172413793104, precision: 0.8615384615384616, f_beta: 0.9105691056910569\n",
      "train: step: 613, loss: 0.23808790743350983, acc: 0.890625, recall: 0.9090909090909091, precision: 0.8823529411764706, f_beta: 0.8955223880597014\n",
      "train: step: 614, loss: 0.1677757352590561, acc: 0.9296875, recall: 0.8867924528301887, precision: 0.94, f_beta: 0.912621359223301\n",
      "train: step: 615, loss: 0.21066437661647797, acc: 0.953125, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001\n",
      "train: step: 616, loss: 0.24613270163536072, acc: 0.8984375, recall: 0.8412698412698413, precision: 0.9464285714285714, f_beta: 0.8907563025210083\n",
      "train: step: 617, loss: 0.16626036167144775, acc: 0.9375, recall: 0.9402985074626866, precision: 0.9402985074626866, f_beta: 0.9402985074626865\n",
      "train: step: 618, loss: 0.1821872591972351, acc: 0.9453125, recall: 0.9696969696969697, precision: 0.927536231884058, f_beta: 0.9481481481481481\n",
      "train: step: 619, loss: 0.18699520826339722, acc: 0.9375, recall: 0.95, precision: 0.9193548387096774, f_beta: 0.9344262295081968\n",
      "train: step: 620, loss: 0.2586625814437866, acc: 0.890625, recall: 0.984375, precision: 0.8289473684210527, f_beta: 0.9\n",
      "train: step: 621, loss: 0.16628703474998474, acc: 0.9453125, recall: 0.9411764705882353, precision: 0.9552238805970149, f_beta: 0.9481481481481482\n",
      "train: step: 622, loss: 0.22382809221744537, acc: 0.9296875, recall: 0.921875, precision: 0.9365079365079365, f_beta: 0.9291338582677166\n",
      "train: step: 623, loss: 0.19843710958957672, acc: 0.9296875, recall: 0.9104477611940298, precision: 0.953125, f_beta: 0.931297709923664\n",
      "train: step: 624, loss: 0.21304352581501007, acc: 0.9140625, recall: 0.8852459016393442, precision: 0.9310344827586207, f_beta: 0.9075630252100839\n",
      "start training model\n",
      "train: step: 625, loss: 0.1701582968235016, acc: 0.9296875, recall: 0.8846153846153846, precision: 0.9387755102040817, f_beta: 0.9108910891089108\n",
      "train: step: 626, loss: 0.1498735547065735, acc: 0.9609375, recall: 0.9242424242424242, precision: 1.0, f_beta: 0.9606299212598425\n",
      "train: step: 627, loss: 0.15907005965709686, acc: 0.953125, recall: 0.9393939393939394, precision: 0.96875, f_beta: 0.9538461538461539\n",
      "train: step: 628, loss: 0.13551421463489532, acc: 0.96875, recall: 0.9454545454545454, precision: 0.9811320754716981, f_beta: 0.9629629629629629\n",
      "train: step: 629, loss: 0.11556083709001541, acc: 0.953125, recall: 0.9594594594594594, precision: 0.9594594594594594, f_beta: 0.9594594594594594\n",
      "train: step: 630, loss: 0.20276591181755066, acc: 0.921875, recall: 0.9855072463768116, precision: 0.8831168831168831, f_beta: 0.9315068493150684\n",
      "train: step: 631, loss: 0.11656966805458069, acc: 0.953125, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001\n",
      "train: step: 632, loss: 0.17670980095863342, acc: 0.953125, recall: 0.9454545454545454, precision: 0.9454545454545454, f_beta: 0.9454545454545454\n",
      "train: step: 633, loss: 0.14819416403770447, acc: 0.9609375, recall: 0.95, precision: 0.9661016949152542, f_beta: 0.957983193277311\n",
      "train: step: 634, loss: 0.14047613739967346, acc: 0.953125, recall: 0.9516129032258065, precision: 0.9516129032258065, f_beta: 0.9516129032258065\n",
      "train: step: 635, loss: 0.14141160249710083, acc: 0.96875, recall: 0.9411764705882353, precision: 1.0, f_beta: 0.9696969696969697\n",
      "train: step: 636, loss: 0.1713167279958725, acc: 0.9609375, recall: 0.9253731343283582, precision: 1.0, f_beta: 0.9612403100775194\n",
      "train: step: 637, loss: 0.1677466183900833, acc: 0.9375, recall: 0.9354838709677419, precision: 0.9354838709677419, f_beta: 0.9354838709677419\n",
      "train: step: 638, loss: 0.1515609622001648, acc: 0.953125, recall: 0.9855072463768116, precision: 0.9315068493150684, f_beta: 0.9577464788732394\n",
      "train: step: 639, loss: 0.1977422684431076, acc: 0.8984375, recall: 0.9298245614035088, precision: 0.8548387096774194, f_beta: 0.8907563025210085\n",
      "train: step: 640, loss: 0.11705964803695679, acc: 0.96875, recall: 0.9833333333333333, precision: 0.9516129032258065, f_beta: 0.9672131147540983\n",
      "train: step: 641, loss: 0.13055431842803955, acc: 0.9609375, recall: 0.9508196721311475, precision: 0.9666666666666667, f_beta: 0.9586776859504132\n",
      "train: step: 642, loss: 0.1511293202638626, acc: 0.9453125, recall: 0.9818181818181818, precision: 0.9, f_beta: 0.9391304347826087\n",
      "train: step: 643, loss: 0.21132415533065796, acc: 0.9140625, recall: 0.95, precision: 0.8769230769230769, f_beta: 0.912\n",
      "train: step: 644, loss: 0.14760306477546692, acc: 0.953125, recall: 0.9264705882352942, precision: 0.984375, f_beta: 0.9545454545454545\n",
      "train: step: 645, loss: 0.15094642341136932, acc: 0.9375, recall: 0.9384615384615385, precision: 0.9384615384615385, f_beta: 0.9384615384615385\n",
      "train: step: 646, loss: 0.15947256982326508, acc: 0.953125, recall: 0.9193548387096774, precision: 0.9827586206896551, f_beta: 0.95\n",
      "train: step: 647, loss: 0.18405117094516754, acc: 0.9296875, recall: 0.9365079365079365, precision: 0.921875, f_beta: 0.9291338582677166\n",
      "train: step: 648, loss: 0.1220298781991005, acc: 0.9609375, recall: 0.9830508474576272, precision: 0.9354838709677419, f_beta: 0.9586776859504132\n",
      "train: step: 649, loss: 0.16081933677196503, acc: 0.9453125, recall: 0.9365079365079365, precision: 0.9516129032258065, f_beta: 0.944\n",
      "train: step: 650, loss: 0.09385769069194794, acc: 0.984375, recall: 0.9846153846153847, precision: 0.9846153846153847, f_beta: 0.9846153846153847\n",
      "train: step: 651, loss: 0.10966290533542633, acc: 0.9765625, recall: 0.9833333333333333, precision: 0.9672131147540983, f_beta: 0.9752066115702478\n",
      "train: step: 652, loss: 0.24432964622974396, acc: 0.8984375, recall: 0.8620689655172413, precision: 0.9090909090909091, f_beta: 0.8849557522123893\n",
      "train: step: 653, loss: 0.17004378139972687, acc: 0.9296875, recall: 0.9411764705882353, precision: 0.927536231884058, f_beta: 0.9343065693430658\n",
      "train: step: 654, loss: 0.10435257852077484, acc: 0.9765625, recall: 0.967741935483871, precision: 0.9836065573770492, f_beta: 0.975609756097561\n",
      "train: step: 655, loss: 0.1905570924282074, acc: 0.8984375, recall: 0.9661016949152542, precision: 0.8382352941176471, f_beta: 0.8976377952755905\n",
      "train: step: 656, loss: 0.09197375923395157, acc: 0.9921875, recall: 0.9846153846153847, precision: 1.0, f_beta: 0.9922480620155039\n",
      "train: step: 657, loss: 0.15907275676727295, acc: 0.953125, recall: 0.9333333333333333, precision: 0.9655172413793104, f_beta: 0.9491525423728815\n",
      "train: step: 658, loss: 0.13081716001033783, acc: 0.9609375, recall: 0.967741935483871, precision: 0.9523809523809523, f_beta: 0.96\n",
      "train: step: 659, loss: 0.15899792313575745, acc: 0.9765625, recall: 0.9666666666666667, precision: 0.9830508474576272, f_beta: 0.9747899159663865\n",
      "train: step: 660, loss: 0.1016426682472229, acc: 0.9765625, recall: 0.95, precision: 1.0, f_beta: 0.9743589743589743\n",
      "train: step: 661, loss: 0.18179666996002197, acc: 0.9453125, recall: 0.9014084507042254, precision: 1.0, f_beta: 0.9481481481481481\n",
      "train: step: 662, loss: 0.17790180444717407, acc: 0.9296875, recall: 0.953125, precision: 0.9104477611940298, f_beta: 0.931297709923664\n",
      "train: step: 663, loss: 0.20127451419830322, acc: 0.9453125, recall: 0.9545454545454546, precision: 0.9402985074626866, f_beta: 0.9473684210526316\n",
      "train: step: 664, loss: 0.1286299079656601, acc: 0.96875, recall: 0.9814814814814815, precision: 0.9464285714285714, f_beta: 0.9636363636363636\n",
      "train: step: 665, loss: 0.13479670882225037, acc: 0.96875, recall: 0.9705882352941176, precision: 0.9705882352941176, f_beta: 0.9705882352941176\n",
      "train: step: 666, loss: 0.110307976603508, acc: 0.9765625, recall: 0.9649122807017544, precision: 0.9821428571428571, f_beta: 0.9734513274336283\n",
      "train: step: 667, loss: 0.14829018712043762, acc: 0.96875, recall: 0.953125, precision: 0.9838709677419355, f_beta: 0.9682539682539683\n",
      "train: step: 668, loss: 0.1527022123336792, acc: 0.96875, recall: 0.9516129032258065, precision: 0.9833333333333333, f_beta: 0.9672131147540983\n",
      "train: step: 669, loss: 0.13401579856872559, acc: 0.96875, recall: 0.9838709677419355, precision: 0.953125, f_beta: 0.9682539682539683\n",
      "train: step: 670, loss: 0.15686681866645813, acc: 0.9296875, recall: 0.918918918918919, precision: 0.9577464788732394, f_beta: 0.9379310344827587\n",
      "train: step: 671, loss: 0.2413395196199417, acc: 0.890625, recall: 0.8548387096774194, precision: 0.9137931034482759, f_beta: 0.8833333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 672, loss: 0.16179975867271423, acc: 0.9453125, recall: 0.9545454545454546, precision: 0.9402985074626866, f_beta: 0.9473684210526316\n",
      "train: step: 673, loss: 0.18445606529712677, acc: 0.921875, recall: 0.9344262295081968, precision: 0.9047619047619048, f_beta: 0.9193548387096775\n",
      "train: step: 674, loss: 0.14694620668888092, acc: 0.9453125, recall: 0.9130434782608695, precision: 0.984375, f_beta: 0.9473684210526315\n",
      "train: step: 675, loss: 0.17102336883544922, acc: 0.953125, recall: 0.9594594594594594, precision: 0.9594594594594594, f_beta: 0.9594594594594594\n",
      "train: step: 676, loss: 0.16592754423618317, acc: 0.9296875, recall: 0.9538461538461539, precision: 0.9117647058823529, f_beta: 0.9323308270676691\n",
      "train: step: 677, loss: 0.1753295212984085, acc: 0.9140625, recall: 0.9538461538461539, precision: 0.8857142857142857, f_beta: 0.9185185185185185\n",
      "train: step: 678, loss: 0.2076454907655716, acc: 0.8984375, recall: 0.9152542372881356, precision: 0.8709677419354839, f_beta: 0.8925619834710744\n",
      "train: step: 679, loss: 0.18722043931484222, acc: 0.921875, recall: 0.875, precision: 0.9423076923076923, f_beta: 0.9074074074074073\n",
      "train: step: 680, loss: 0.15448740124702454, acc: 0.9609375, recall: 0.9436619718309859, precision: 0.9852941176470589, f_beta: 0.9640287769784172\n",
      "train: step: 681, loss: 0.10552607476711273, acc: 0.96875, recall: 0.9841269841269841, precision: 0.9538461538461539, f_beta: 0.96875\n",
      "train: step: 682, loss: 0.16086120903491974, acc: 0.9375, recall: 0.9508196721311475, precision: 0.9206349206349206, f_beta: 0.9354838709677418\n",
      "train: step: 683, loss: 0.07389748841524124, acc: 0.9921875, recall: 1.0, precision: 0.9852941176470589, f_beta: 0.9925925925925926\n",
      "train: step: 684, loss: 0.14305098354816437, acc: 0.9453125, recall: 0.927536231884058, precision: 0.9696969696969697, f_beta: 0.9481481481481481\n",
      "train: step: 685, loss: 0.1394670307636261, acc: 0.9609375, recall: 0.9365079365079365, precision: 0.9833333333333333, f_beta: 0.9593495934959351\n",
      "train: step: 686, loss: 0.16089148819446564, acc: 0.96875, recall: 0.9508196721311475, precision: 0.9830508474576272, f_beta: 0.9666666666666667\n",
      "train: step: 687, loss: 0.1324584186077118, acc: 0.96875, recall: 0.9682539682539683, precision: 0.9682539682539683, f_beta: 0.9682539682539683\n",
      "train: step: 688, loss: 0.1743433177471161, acc: 0.9375, recall: 0.9852941176470589, precision: 0.9054054054054054, f_beta: 0.943661971830986\n",
      "train: step: 689, loss: 0.13926509022712708, acc: 0.9453125, recall: 0.9827586206896551, precision: 0.9047619047619048, f_beta: 0.9421487603305785\n",
      "train: step: 690, loss: 0.11319969594478607, acc: 0.9453125, recall: 0.9682539682539683, precision: 0.9242424242424242, f_beta: 0.9457364341085271\n",
      "train: step: 691, loss: 0.10322880744934082, acc: 0.9765625, recall: 0.9850746268656716, precision: 0.9705882352941176, f_beta: 0.9777777777777777\n",
      "train: step: 692, loss: 0.17050334811210632, acc: 0.953125, recall: 0.9454545454545454, precision: 0.9454545454545454, f_beta: 0.9454545454545454\n",
      "train: step: 693, loss: 0.13031765818595886, acc: 0.9375, recall: 0.9264705882352942, precision: 0.9545454545454546, f_beta: 0.9402985074626866\n",
      "train: step: 694, loss: 0.13412481546401978, acc: 0.9609375, recall: 0.9666666666666667, precision: 0.9508196721311475, f_beta: 0.9586776859504132\n",
      "train: step: 695, loss: 0.24785608053207397, acc: 0.90625, recall: 0.875, precision: 0.9722222222222222, f_beta: 0.9210526315789473\n",
      "train: step: 696, loss: 0.1509777307510376, acc: 0.9453125, recall: 0.9322033898305084, precision: 0.9482758620689655, f_beta: 0.94017094017094\n",
      "train: step: 697, loss: 0.12058377265930176, acc: 0.96875, recall: 0.9714285714285714, precision: 0.9714285714285714, f_beta: 0.9714285714285714\n",
      "train: step: 698, loss: 0.17144173383712769, acc: 0.953125, recall: 0.967741935483871, precision: 0.9375, f_beta: 0.9523809523809523\n",
      "train: step: 699, loss: 0.17865273356437683, acc: 0.921875, recall: 0.9538461538461539, precision: 0.8985507246376812, f_beta: 0.9253731343283582\n",
      "train: step: 700, loss: 0.1868944764137268, acc: 0.9296875, recall: 0.9538461538461539, precision: 0.9117647058823529, f_beta: 0.9323308270676691\n",
      "\n",
      "Evaluation:\n",
      "2019-08-14T10:54:19.587618, step: 700, loss: 0.3163083016108244, acc: 0.8651842948717948,precision: 0.89209183422272, recall: 0.8485069785147906, f_beta: 0.8690835742506005\n",
      "Saved model checkpoint to model/textCNN/model/my-model-700\n",
      "\n",
      "train: step: 701, loss: 0.1333644688129425, acc: 0.9609375, recall: 0.9523809523809523, precision: 0.967741935483871, f_beta: 0.96\n",
      "train: step: 702, loss: 0.1190311536192894, acc: 0.984375, recall: 0.9861111111111112, precision: 0.9861111111111112, f_beta: 0.9861111111111112\n",
      "train: step: 703, loss: 0.1147293969988823, acc: 0.96875, recall: 0.9846153846153847, precision: 0.9552238805970149, f_beta: 0.9696969696969696\n",
      "train: step: 704, loss: 0.1292976289987564, acc: 0.953125, recall: 0.9696969696969697, precision: 0.9411764705882353, f_beta: 0.955223880597015\n",
      "train: step: 705, loss: 0.13150982558727264, acc: 0.9609375, recall: 0.9710144927536232, precision: 0.9571428571428572, f_beta: 0.9640287769784173\n",
      "train: step: 706, loss: 0.16756115853786469, acc: 0.9375, recall: 0.9436619718309859, precision: 0.9436619718309859, f_beta: 0.9436619718309859\n",
      "train: step: 707, loss: 0.14512209594249725, acc: 0.953125, recall: 0.9523809523809523, precision: 0.9523809523809523, f_beta: 0.9523809523809523\n",
      "train: step: 708, loss: 0.14172399044036865, acc: 0.9453125, recall: 0.9523809523809523, precision: 0.9375, f_beta: 0.9448818897637795\n",
      "train: step: 709, loss: 0.1326645016670227, acc: 0.953125, recall: 0.9482758620689655, precision: 0.9482758620689655, f_beta: 0.9482758620689655\n",
      "train: step: 710, loss: 0.12072812020778656, acc: 0.9765625, recall: 0.9857142857142858, precision: 0.971830985915493, f_beta: 0.9787234042553192\n",
      "train: step: 711, loss: 0.14601776003837585, acc: 0.953125, recall: 0.9696969696969697, precision: 0.9411764705882353, f_beta: 0.955223880597015\n",
      "train: step: 712, loss: 0.18002796173095703, acc: 0.9375, recall: 0.9552238805970149, precision: 0.927536231884058, f_beta: 0.9411764705882353\n",
      "train: step: 713, loss: 0.18547794222831726, acc: 0.90625, recall: 0.8717948717948718, precision: 0.9714285714285714, f_beta: 0.9189189189189189\n",
      "train: step: 714, loss: 0.07865689694881439, acc: 0.9765625, recall: 1.0, precision: 0.9516129032258065, f_beta: 0.9752066115702479\n",
      "train: step: 715, loss: 0.17653551697731018, acc: 0.9453125, recall: 0.9516129032258065, precision: 0.9365079365079365, f_beta: 0.944\n",
      "train: step: 716, loss: 0.14927390217781067, acc: 0.9609375, recall: 0.9836065573770492, precision: 0.9375, f_beta: 0.96\n",
      "train: step: 717, loss: 0.13036999106407166, acc: 0.96875, recall: 0.9666666666666667, precision: 0.9666666666666667, f_beta: 0.9666666666666667\n",
      "train: step: 718, loss: 0.13141459226608276, acc: 0.96875, recall: 0.96875, precision: 0.96875, f_beta: 0.96875\n",
      "train: step: 719, loss: 0.15557296574115753, acc: 0.9609375, recall: 0.9384615384615385, precision: 0.9838709677419355, f_beta: 0.9606299212598426\n",
      "train: step: 720, loss: 0.19654983282089233, acc: 0.9140625, recall: 0.9, precision: 0.9402985074626866, f_beta: 0.9197080291970803\n",
      "train: step: 721, loss: 0.11110131442546844, acc: 0.96875, recall: 0.9701492537313433, precision: 0.9701492537313433, f_beta: 0.9701492537313433\n",
      "train: step: 722, loss: 0.19097751379013062, acc: 0.9375, recall: 0.9411764705882353, precision: 0.9411764705882353, f_beta: 0.9411764705882353\n",
      "train: step: 723, loss: 0.16488496959209442, acc: 0.953125, recall: 1.0, precision: 0.9076923076923077, f_beta: 0.9516129032258065\n",
      "train: step: 724, loss: 0.1313740462064743, acc: 0.9609375, recall: 0.9852941176470589, precision: 0.9436619718309859, f_beta: 0.9640287769784172\n",
      "train: step: 725, loss: 0.15945032238960266, acc: 0.9375, recall: 0.95, precision: 0.9193548387096774, f_beta: 0.9344262295081968\n",
      "train: step: 726, loss: 0.1380462348461151, acc: 0.9765625, recall: 0.9649122807017544, precision: 0.9821428571428571, f_beta: 0.9734513274336283\n",
      "train: step: 727, loss: 0.1291971504688263, acc: 0.96875, recall: 0.967741935483871, precision: 0.967741935483871, f_beta: 0.967741935483871\n",
      "train: step: 728, loss: 0.16365262866020203, acc: 0.9453125, recall: 0.9210526315789473, precision: 0.9859154929577465, f_beta: 0.9523809523809524\n",
      "train: step: 729, loss: 0.1259765326976776, acc: 0.9609375, recall: 0.9411764705882353, precision: 0.9846153846153847, f_beta: 0.962406015037594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 730, loss: 0.1165216863155365, acc: 0.9609375, recall: 0.9411764705882353, precision: 0.9846153846153847, f_beta: 0.962406015037594\n",
      "train: step: 731, loss: 0.12993267178535461, acc: 0.9609375, recall: 0.9454545454545454, precision: 0.9629629629629629, f_beta: 0.9541284403669724\n",
      "train: step: 732, loss: 0.1472492218017578, acc: 0.9453125, recall: 0.9393939393939394, precision: 0.9538461538461539, f_beta: 0.9465648854961831\n",
      "train: step: 733, loss: 0.16610726714134216, acc: 0.953125, recall: 0.9454545454545454, precision: 0.9454545454545454, f_beta: 0.9454545454545454\n",
      "train: step: 734, loss: 0.17847919464111328, acc: 0.921875, recall: 0.9193548387096774, precision: 0.9193548387096774, f_beta: 0.9193548387096774\n",
      "train: step: 735, loss: 0.209879070520401, acc: 0.9296875, recall: 0.9636363636363636, precision: 0.8833333333333333, f_beta: 0.9217391304347826\n",
      "train: step: 736, loss: 0.11174877732992172, acc: 0.9765625, recall: 1.0, precision: 0.95, f_beta: 0.9743589743589743\n",
      "train: step: 737, loss: 0.16152527928352356, acc: 0.9296875, recall: 0.9152542372881356, precision: 0.9310344827586207, f_beta: 0.923076923076923\n",
      "train: step: 738, loss: 0.17014120519161224, acc: 0.9296875, recall: 0.8888888888888888, precision: 0.9655172413793104, f_beta: 0.9256198347107438\n",
      "train: step: 739, loss: 0.11596158146858215, acc: 0.9765625, recall: 0.9538461538461539, precision: 1.0, f_beta: 0.9763779527559054\n",
      "train: step: 740, loss: 0.12545287609100342, acc: 0.9765625, recall: 0.96875, precision: 0.9841269841269841, f_beta: 0.9763779527559054\n",
      "train: step: 741, loss: 0.12531623244285583, acc: 0.96875, recall: 0.9701492537313433, precision: 0.9701492537313433, f_beta: 0.9701492537313433\n",
      "train: step: 742, loss: 0.13210125267505646, acc: 0.9609375, recall: 0.9661016949152542, precision: 0.95, f_beta: 0.957983193277311\n",
      "train: step: 743, loss: 0.13984470069408417, acc: 0.953125, recall: 0.9230769230769231, precision: 0.96, f_beta: 0.9411764705882353\n",
      "train: step: 744, loss: 0.1092228889465332, acc: 0.984375, recall: 0.9830508474576272, precision: 0.9830508474576272, f_beta: 0.9830508474576272\n",
      "train: step: 745, loss: 0.1007482260465622, acc: 0.9453125, recall: 0.9655172413793104, precision: 0.9180327868852459, f_beta: 0.9411764705882353\n",
      "train: step: 746, loss: 0.09372467547655106, acc: 0.984375, recall: 1.0, precision: 0.9736842105263158, f_beta: 0.9866666666666666\n",
      "train: step: 747, loss: 0.18517060577869415, acc: 0.9453125, recall: 0.9705882352941176, precision: 0.9295774647887324, f_beta: 0.9496402877697842\n",
      "train: step: 748, loss: 0.13983850181102753, acc: 0.9375, recall: 0.9846153846153847, precision: 0.9014084507042254, f_beta: 0.9411764705882353\n",
      "train: step: 749, loss: 0.1440829336643219, acc: 0.9609375, recall: 0.9545454545454546, precision: 0.9692307692307692, f_beta: 0.9618320610687022\n",
      "train: step: 750, loss: 0.13299155235290527, acc: 0.9609375, recall: 0.9558823529411765, precision: 0.9701492537313433, f_beta: 0.962962962962963\n",
      "train: step: 751, loss: 0.1250336468219757, acc: 0.9765625, recall: 0.9538461538461539, precision: 1.0, f_beta: 0.9763779527559054\n",
      "train: step: 752, loss: 0.10910209268331528, acc: 0.9609375, recall: 0.9206349206349206, precision: 1.0, f_beta: 0.9586776859504132\n",
      "train: step: 753, loss: 0.2076900154352188, acc: 0.921875, recall: 0.92, precision: 0.9452054794520548, f_beta: 0.9324324324324323\n",
      "train: step: 754, loss: 0.12275451421737671, acc: 0.9921875, recall: 1.0, precision: 0.9848484848484849, f_beta: 0.9923664122137404\n",
      "train: step: 755, loss: 0.15579472482204437, acc: 0.9375, recall: 0.9411764705882353, precision: 0.9056603773584906, f_beta: 0.923076923076923\n",
      "train: step: 756, loss: 0.15904879570007324, acc: 0.9765625, recall: 0.9838709677419355, precision: 0.9682539682539683, f_beta: 0.976\n",
      "train: step: 757, loss: 0.2218317687511444, acc: 0.921875, recall: 0.9538461538461539, precision: 0.8985507246376812, f_beta: 0.9253731343283582\n",
      "train: step: 758, loss: 0.17376747727394104, acc: 0.9375, recall: 0.9857142857142858, precision: 0.9078947368421053, f_beta: 0.9452054794520548\n",
      "train: step: 759, loss: 0.08942702412605286, acc: 0.984375, recall: 1.0, precision: 0.9722222222222222, f_beta: 0.9859154929577464\n",
      "train: step: 760, loss: 0.19047029316425323, acc: 0.9140625, recall: 0.9384615384615385, precision: 0.8970588235294118, f_beta: 0.9172932330827067\n",
      "train: step: 761, loss: 0.12773063778877258, acc: 0.953125, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001\n",
      "train: step: 762, loss: 0.1200910210609436, acc: 0.9765625, recall: 0.9682539682539683, precision: 0.9838709677419355, f_beta: 0.976\n",
      "train: step: 763, loss: 0.11077597737312317, acc: 0.9765625, recall: 0.9696969696969697, precision: 0.9846153846153847, f_beta: 0.9770992366412214\n",
      "train: step: 764, loss: 0.13738204538822174, acc: 0.9453125, recall: 0.896551724137931, precision: 0.9811320754716981, f_beta: 0.9369369369369369\n",
      "train: step: 765, loss: 0.13638871908187866, acc: 0.9609375, recall: 0.9516129032258065, precision: 0.9672131147540983, f_beta: 0.959349593495935\n",
      "train: step: 766, loss: 0.14042812585830688, acc: 0.9375, recall: 0.9130434782608695, precision: 0.9692307692307692, f_beta: 0.9402985074626865\n",
      "train: step: 767, loss: 0.1549236923456192, acc: 0.9296875, recall: 0.9538461538461539, precision: 0.9117647058823529, f_beta: 0.9323308270676691\n",
      "train: step: 768, loss: 0.18126317858695984, acc: 0.9296875, recall: 0.9152542372881356, precision: 0.9310344827586207, f_beta: 0.923076923076923\n",
      "train: step: 769, loss: 0.1137416660785675, acc: 0.9609375, recall: 0.9508196721311475, precision: 0.9666666666666667, f_beta: 0.9586776859504132\n",
      "train: step: 770, loss: 0.14870473742485046, acc: 0.953125, recall: 0.9838709677419355, precision: 0.9242424242424242, f_beta: 0.9531249999999999\n",
      "train: step: 771, loss: 0.10611901432275772, acc: 0.9765625, recall: 0.9830508474576272, precision: 0.9666666666666667, f_beta: 0.9747899159663865\n",
      "train: step: 772, loss: 0.19655358791351318, acc: 0.921875, recall: 0.9154929577464789, precision: 0.9420289855072463, f_beta: 0.9285714285714286\n",
      "train: step: 773, loss: 0.1018638163805008, acc: 0.96875, recall: 0.9696969696969697, precision: 0.9696969696969697, f_beta: 0.9696969696969697\n",
      "train: step: 774, loss: 0.14002913236618042, acc: 0.9765625, recall: 0.9714285714285714, precision: 0.9855072463768116, f_beta: 0.9784172661870504\n",
      "train: step: 775, loss: 0.18875664472579956, acc: 0.9375, recall: 0.8955223880597015, precision: 0.9836065573770492, f_beta: 0.9375\n",
      "train: step: 776, loss: 0.1566341519355774, acc: 0.953125, recall: 0.9710144927536232, precision: 0.9436619718309859, f_beta: 0.9571428571428571\n",
      "train: step: 777, loss: 0.1401616930961609, acc: 0.9453125, recall: 0.984375, precision: 0.9130434782608695, f_beta: 0.9473684210526315\n",
      "train: step: 778, loss: 0.13143280148506165, acc: 0.9609375, recall: 0.9726027397260274, precision: 0.9594594594594594, f_beta: 0.9659863945578231\n",
      "train: step: 779, loss: 0.18993869423866272, acc: 0.953125, recall: 0.9696969696969697, precision: 0.9411764705882353, f_beta: 0.955223880597015\n",
      "train: step: 780, loss: 0.16897377371788025, acc: 0.9296875, recall: 0.9508196721311475, precision: 0.90625, f_beta: 0.9279999999999999\n",
      "WARNING:tensorflow:From <ipython-input-9-be9ea6b67d43>:149: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "WARNING:tensorflow:From <ipython-input-9-be9ea6b67d43>:158: calling SavedModelBuilder.add_meta_graph_and_variables (from tensorflow.python.saved_model.builder_impl) with legacy_init_op is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Pass your op to the equivalent parameter main_op instead.\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: model/textCNN/savedModel/saved_model.pb\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "\n",
    "# 生成训练集和验证集\n",
    "trainReviews = data.trainReviews\n",
    "trainLabels = data.trainLabels\n",
    "evalReviews = data.evalReviews\n",
    "evalLabels = data.evalLabels\n",
    "\n",
    "wordEmbedding = data.wordEmbedding\n",
    "labelList = data.labelList\n",
    "\n",
    "# 定义计算图\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "    session_conf.gpu_options.allow_growth=True\n",
    "    session_conf.gpu_options.per_process_gpu_memory_fraction = 0.9  # 配置gpu占用率  \n",
    "\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    \n",
    "    # 定义会话\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(config, wordEmbedding)\n",
    "        \n",
    "        globalStep = tf.Variable(0, name=\"globalStep\", trainable=False)\n",
    "        # 定义优化函数，传入学习速率参数\n",
    "        optimizer = tf.train.AdamOptimizer(config.training.learningRate)\n",
    "        # 计算梯度,得到梯度和变量\n",
    "        gradsAndVars = optimizer.compute_gradients(cnn.loss)\n",
    "        # 将梯度应用到变量下，生成训练器\n",
    "        trainOp = optimizer.apply_gradients(gradsAndVars, global_step=globalStep)\n",
    "        \n",
    "        # 用summary绘制tensorBoard\n",
    "        gradSummaries = []\n",
    "        for g, v in gradsAndVars:\n",
    "            if g is not None:\n",
    "                tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "        \n",
    "        outDir = os.path.abspath(os.path.join(os.path.curdir, \"summarys\"))\n",
    "        print(\"Writing to {}\\n\".format(outDir))\n",
    "        \n",
    "        lossSummary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        summaryOp = tf.summary.merge_all()\n",
    "        \n",
    "        trainSummaryDir = os.path.join(outDir, \"train\")\n",
    "        trainSummaryWriter = tf.summary.FileWriter(trainSummaryDir, sess.graph)\n",
    "        \n",
    "        evalSummaryDir = os.path.join(outDir, \"eval\")\n",
    "        evalSummaryWriter = tf.summary.FileWriter(evalSummaryDir, sess.graph)\n",
    "        \n",
    "        \n",
    "        # 初始化所有变量\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)\n",
    "        \n",
    "        # 保存模型的一种方式，保存为pb文件\n",
    "        savedModelPath = \"model/textCNN/savedModel\"\n",
    "        if os.path.exists(savedModelPath):\n",
    "            os.rmdir(savedModelPath)\n",
    "        builder = tf.saved_model.builder.SavedModelBuilder(savedModelPath)\n",
    "            \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def trainStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            训练函数\n",
    "            \"\"\"   \n",
    "            feed_dict = {\n",
    "              cnn.inputX: batchX,\n",
    "              cnn.inputY: batchY,\n",
    "              cnn.dropoutKeepProb: config.model.dropoutKeepProb\n",
    "            }\n",
    "            _, summary, step, loss, predictions = sess.run(\n",
    "                [trainOp, summaryOp, globalStep, cnn.loss, cnn.predictions],\n",
    "                feed_dict)\n",
    "            timeStr = datetime.datetime.now().isoformat()\n",
    "            \n",
    "            if config.numClasses == 1:\n",
    "                acc, recall, prec, f_beta = get_binary_metrics(pred_y=predictions, true_y=batchY)\n",
    "\n",
    "                \n",
    "            elif config.numClasses > 1:\n",
    "                acc, recall, prec, f_beta = get_multi_metrics(pred_y=predictions, true_y=batchY,\n",
    "                                                              labels=labelList)\n",
    "                \n",
    "            trainSummaryWriter.add_summary(summary, step)\n",
    "            \n",
    "            return loss, acc, prec, recall, f_beta\n",
    "\n",
    "        def devStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            验证函数\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.inputX: batchX,\n",
    "              cnn.inputY: batchY,\n",
    "              cnn.dropoutKeepProb: 1.0\n",
    "            }\n",
    "            summary, step, loss, predictions = sess.run(\n",
    "                [summaryOp, globalStep, cnn.loss, cnn.predictions],\n",
    "                feed_dict)\n",
    "            \n",
    "            if config.numClasses == 1:\n",
    "            \n",
    "                acc, precision, recall, f_beta = get_binary_metrics(pred_y=predictions, true_y=batchY)\n",
    "            elif config.numClasses > 1:\n",
    "                acc, precision, recall, f_beta = get_multi_metrics(pred_y=predictions, true_y=batchY, labels=labelList)\n",
    "            \n",
    "            evalSummaryWriter.add_summary(summary, step)\n",
    "            \n",
    "            return loss, acc, precision, recall, f_beta\n",
    "        \n",
    "        for i in range(config.training.epoches):\n",
    "            # 训练模型\n",
    "            print(\"start training model\")\n",
    "            for batchTrain in nextBatch(trainReviews, trainLabels, config.batchSize):\n",
    "                loss, acc, prec, recall, f_beta = trainStep(batchTrain[0], batchTrain[1])\n",
    "                \n",
    "                currentStep = tf.train.global_step(sess, globalStep) \n",
    "                print(\"train: step: {}, loss: {}, acc: {}, recall: {}, precision: {}, f_beta: {}\".format(\n",
    "                    currentStep, loss, acc, recall, prec, f_beta))\n",
    "                if currentStep % config.training.evaluateEvery == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    \n",
    "                    losses = []\n",
    "                    accs = []\n",
    "                    f_betas = []\n",
    "                    precisions = []\n",
    "                    recalls = []\n",
    "                    \n",
    "                    for batchEval in nextBatch(evalReviews, evalLabels, config.batchSize):\n",
    "                        loss, acc, precision, recall, f_beta = devStep(batchEval[0], batchEval[1])\n",
    "                        losses.append(loss)\n",
    "                        accs.append(acc)\n",
    "                        f_betas.append(f_beta)\n",
    "                        precisions.append(precision)\n",
    "                        recalls.append(recall)\n",
    "                        \n",
    "                    time_str = datetime.datetime.now().isoformat()\n",
    "                    print(\"{}, step: {}, loss: {}, acc: {},precision: {}, recall: {}, f_beta: {}\".format(time_str, currentStep, mean(losses), \n",
    "                                                                                                       mean(accs), mean(precisions),\n",
    "                                                                                                       mean(recalls), mean(f_betas)))\n",
    "                    \n",
    "                if currentStep % config.training.checkpointEvery == 0:\n",
    "                    # 保存模型的另一种方法，保存checkpoint文件\n",
    "                    path = saver.save(sess, \"model/textCNN/model/my-model\", global_step=currentStep)\n",
    "                    print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "                    \n",
    "        inputs = {\"inputX\": tf.saved_model.utils.build_tensor_info(cnn.inputX),\n",
    "                  \"keepProb\": tf.saved_model.utils.build_tensor_info(cnn.dropoutKeepProb)}\n",
    "\n",
    "        outputs = {\"predictions\": tf.saved_model.utils.build_tensor_info(cnn.predictions)}\n",
    "\n",
    "        prediction_signature = tf.saved_model.signature_def_utils.build_signature_def(inputs=inputs, outputs=outputs,\n",
    "                                                                                      method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)\n",
    "        legacy_init_op = tf.group(tf.tables_initializer(), name=\"legacy_init_op\")\n",
    "        builder.add_meta_graph_and_variables(sess, [tf.saved_model.tag_constants.SERVING],\n",
    "                                            signature_def_map={\"predict\": prediction_signature}, legacy_init_op=legacy_init_op)\n",
    "\n",
    "        builder.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/nicken/NLP_study/nlp_env/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from model/textCNN/model/my-model-700\n",
      "['1']\n"
     ]
    }
   ],
   "source": [
    "x = \"this movie is full of references like mad max ii the wild one and many others the ladybug´s face it´s a clear reference or tribute to peter lorre this movie is a masterpiece we´ll talk much more about in the future\"\n",
    "\n",
    "# 注：下面两个词典要保证和当前加载的模型对应的词典是一致的\n",
    "with open(\"data/wordJson/word2idx.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    word2idx = json.load(f)\n",
    "        \n",
    "with open(\"data/wordJson/label2idx.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    label2idx = json.load(f)\n",
    "idx2label = {value: key for key, value in label2idx.items()}\n",
    "    \n",
    "xIds = [word2idx.get(item, word2idx[\"UNK\"]) for item in x.split(\" \")]\n",
    "if len(xIds) >= config.sequenceLength:\n",
    "    xIds = xIds[:config.sequenceLength]\n",
    "else:\n",
    "    xIds = xIds + [word2idx[\"PAD\"]] * (config.sequenceLength - len(xIds))\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False, gpu_options=gpu_options)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "\n",
    "    with sess.as_default():\n",
    "        checkpoint_file = tf.train.latest_checkpoint(\"model/textCNN/model/\")\n",
    "        saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
    "        saver.restore(sess, checkpoint_file)\n",
    "\n",
    "        # 获得需要喂给模型的参数，输出的结果依赖的输入值\n",
    "        inputX = graph.get_operation_by_name(\"inputX\").outputs[0]\n",
    "        dropoutKeepProb = graph.get_operation_by_name(\"dropoutKeepProb\").outputs[0]\n",
    "\n",
    "        # 获得输出的结果\n",
    "        predictions = graph.get_tensor_by_name(\"output/predictions:0\")\n",
    "\n",
    "        pred = sess.run(predictions, feed_dict={inputX: [xIds], dropoutKeepProb: 1.0})[0]\n",
    "        \n",
    "pred = [idx2label[item] for item in pred]     \n",
    "print(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
